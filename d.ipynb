{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-07 11:42:04 - ERROR - Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "2024-07-07 11:42:04 - DEBUG - Starting new HTTPS connection (1): api.wandb.ai:443\n",
      "2024-07-07 11:42:05 - DEBUG - https://api.wandb.ai:443 \"POST /graphql HTTP/1.1\" 200 1918\n",
      "2024-07-07 11:42:05 - DEBUG - https://api.wandb.ai:443 \"POST /graphql HTTP/1.1\" 200 390\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maakyildiz\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "2024-07-07 11:42:05 - DEBUG - no default config file found in config-defaults.yaml\n",
      "2024-07-07 11:49:34 - INFO - num_numerical: 1\n",
      "2024-07-07 11:49:34 - INFO - num_categorical: 5\n",
      "2024-07-07 11:49:34 - INFO - num_columns: 7\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from src.datasets.util.mask import PretrainType\n",
    "from torch_frame.data import DataLoader\n",
    "from torch_frame import stype\n",
    "from torch_frame.nn import (\n",
    "    EmbeddingEncoder,\n",
    "    LinearEncoder,\n",
    "    TimestampEncoder,\n",
    ")\n",
    "from torch_frame.nn.encoder.stypewise_encoder import StypeWiseFeatureEncoder\n",
    "from torch_frame import TensorFrame\n",
    "from torch_frame.data.stats import StatType\n",
    "\n",
    "from torch_geometric.utils import degree\n",
    "\n",
    "from src.datasets import IBMTransactionsAML\n",
    "from src.nn.models import TABGNN\n",
    "from src.nn.decoder import MCMHead\n",
    "from src.nn.gnn.decoder import LinkPredHead\n",
    "from src.utils.loss import SSLoss\n",
    "from src.utils.metric import SSMetric\n",
    "from src.nn.weighting.MoCo import MoCoLoss\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import wandb\n",
    "\n",
    "import logging\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,  # Set the logging level\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',  # Specify the log message format\n",
    "    datefmt='%Y-%m-%d %H:%M:%S',  # Specify the date format\n",
    "    handlers=[\n",
    "        #logging.FileHandler('app.log'),  # Log messages to a file\n",
    "        logging.StreamHandler()  # Also output log messages to the console\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create a logger\n",
    "logger = logging.getLogger(__name__)\n",
    "import sys\n",
    "import time\n",
    "\n",
    "torch.set_float32_matmul_precision('high')\n",
    "torch.set_num_threads(4)\n",
    "\n",
    "seed = 42\n",
    "batch_size = 200\n",
    "lr = 2e-4\n",
    "eps = 1e-8\n",
    "weight_decay = 1e-3\n",
    "epochs = 50\n",
    "\n",
    "compile = False\n",
    "data_split = [0.6, 0.2, 0.2]\n",
    "split_type = 'temporal'\n",
    "\n",
    "khop_neighbors = [100, 100]\n",
    "pos_sample_prob = 1\n",
    "num_neg_samples = 64\n",
    "channels = 128\n",
    "num_layers = 3\n",
    "dropout = 0.5\n",
    "\n",
    "pretrain = {PretrainType.MASK, PretrainType.LINK_PRED}\n",
    "#pretrain = {PretrainType.LINK_PRED}\n",
    "\n",
    "testing = True\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "args = {\n",
    "    'testing': testing,\n",
    "    'batch_size': batch_size,\n",
    "    'seed': seed,\n",
    "    'device': device,\n",
    "    'lr': lr,\n",
    "    'eps': eps,\n",
    "    'epochs': epochs,\n",
    "    'compile': compile,\n",
    "    'data_split': data_split,\n",
    "    'pos_sample_prob': pos_sample_prob,\n",
    "    'channels': channels,\n",
    "    'split_type': split_type,\n",
    "    'num_neg_samples': num_neg_samples,\n",
    "    'pretrain': pretrain,\n",
    "    'khop_neighbors': khop_neighbors,\n",
    "    'num_layers': num_layers,\n",
    "    'dropout': dropout,\n",
    "    'weight_decay': weight_decay,\n",
    "}\n",
    "\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "# When running on the CuDNN backend, two further options must be set\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "# Set a fixed value for the hash seed\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "wandb.login()\n",
    "run = wandb.init(\n",
    "    dir=\"/mnt/data/\",\n",
    "    #dir=\"/scratch/takyildiz/\",\n",
    "    mode=\"disabled\" if args['testing'] else \"online\",\n",
    "    #mode=\"disabled\" if args['testing'] else \"disabled\",\n",
    "    project=f\"rel-mm-fix\", \n",
    "    name=f\"tabgnn,tab=3,noemb,lp\",\n",
    "    #group=f\"new,mcm\",\n",
    "    entity=\"cse3000\",\n",
    "    #name=f\"debug-fused\",\n",
    "    config=args\n",
    ")\n",
    "\n",
    "dataset = IBMTransactionsAML(\n",
    "    root='/mnt/data/ibm-transactions-for-anti-money-laundering-aml/HI-Small_Trans-c.csv',# if not testing else '/mnt/data/ibm-transactions-for-anti-money-laundering-aml/dummy-c.csv', \n",
    "    #root='/scratch/takyildiz/ibm-transactions-for-anti-money-laundering-aml/HI-Small_Trans-c.csv' if not testing else '/scratch/takyildiz/ibm-transactions-for-anti-money-laundering-aml/dummy-c.csv', \n",
    "    pretrain=pretrain,\n",
    "    mask_type=\"replace\",\n",
    "    split_type=split_type, \n",
    "    splits=data_split, \n",
    "    khop_neighbors=khop_neighbors\n",
    ")\n",
    "dataset.materialize()\n",
    "dataset.df.head(5)\n",
    "train_dataset, val_dataset, test_dataset = dataset.split()\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    \n",
    "g = torch.Generator()\n",
    "g.manual_seed(seed)\n",
    "\n",
    "tensor_frame = dataset.tensor_frame\n",
    "train_loader = DataLoader(train_dataset.tensor_frame, batch_size=batch_size, shuffle=True, worker_init_fn=seed_worker, generator=g, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset.tensor_frame, batch_size=batch_size, shuffle=False, worker_init_fn=seed_worker, generator=g, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset.tensor_frame, batch_size=batch_size, shuffle=False, worker_init_fn=seed_worker, generator=g, num_workers=4)\n",
    "num_numerical = len(dataset.tensor_frame.col_names_dict[stype.numerical])\n",
    "num_categorical = len(dataset.tensor_frame.col_names_dict[stype.categorical])\n",
    "\n",
    "num_columns = num_numerical + num_categorical + 1\n",
    "logger.info(f\"num_numerical: {num_numerical}\")\n",
    "logger.info(f\"num_categorical: {num_categorical}\")\n",
    "logger.info(f\"num_columns: {num_columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-07 11:49:51 - INFO - model_params: 7751424\n",
      "2024-07-07 11:49:51 - INFO - encoder_params: 5936384\n",
      "2024-07-07 11:49:51 - INFO - lp_decoder_params: 52531\n",
      "2024-07-07 11:49:51 - INFO - mcm_decoder_params: 17837423\n",
      "2024-07-07 11:49:51 - INFO - learnable_params: 31577762\n"
     ]
    }
   ],
   "source": [
    "def mcm_inputs(tf: TensorFrame, dataset):\n",
    "    batch_size = len(tf.y)\n",
    "    edges = tf.y[:,-3:]\n",
    "    khop_source, khop_destination, idx = dataset.sample_neighbors(edges, 'train')\n",
    "\n",
    "    edge_attr = dataset.tensor_frame.__getitem__(idx)\n",
    "\n",
    "    nodes = torch.unique(torch.cat([khop_source, khop_destination]))\n",
    "    num_nodes = nodes.shape[0]\n",
    "    node_feats = torch.ones(num_nodes).view(-1,num_nodes).t()\n",
    "\n",
    "    n_id_map = {value.item(): index for index, value in enumerate(nodes)}\n",
    "    local_khop_source = torch.tensor([n_id_map[node.item()] for node in khop_source], dtype=torch.long)\n",
    "    local_khop_destination = torch.tensor([n_id_map[node.item()] for node in khop_destination], dtype=torch.long)\n",
    "    edge_index = torch.cat((local_khop_source.unsqueeze(0), local_khop_destination.unsqueeze(0)))\n",
    "\n",
    "    drop_edge_ind = torch.tensor([x for x in range(batch_size)])\n",
    "    mask = torch.zeros((edge_index.shape[1],)).long() #[E, ]\n",
    "    mask = mask.index_fill_(dim=0, index=drop_edge_ind, value=1).bool() #[E, ]\n",
    "    # input_edge_index = edge_index[:, ~mask]\n",
    "    # input_edge_attr  = edge_attr[~mask]\n",
    "    input_edge_index = edge_index\n",
    "    input_edge_attr  = edge_attr\n",
    "    target_edge_index = edge_index[:, mask]\n",
    "    target_edge_attr  = edge_attr[mask]\n",
    "    return node_feats.to(device), input_edge_index.to(device), input_edge_attr.to(device), target_edge_index.to(device), target_edge_attr.to(device)  \n",
    "    \n",
    "\n",
    "\n",
    "def train_mcm(dataset, loader, epoc: int, encoder, model, mcm_decoder, optimizer, scheduler) -> float:\n",
    "    model.train()\n",
    "    loss_accum = total_count = 0\n",
    "    loss_accum = loss_lp_accum = loss_c_accum = loss_n_accum = total_count = t_c = t_n = 0\n",
    "\n",
    "    with tqdm(loader, desc=f'Epoch {epoc}') as t:\n",
    "        for tf in t:\n",
    "            node_feats, edge_index, edge_attr, target_edge_index, target_edge_attr = mcm_inputs(tf, dataset)\n",
    "            tf = tf.to(device)\n",
    "            edge_attr, _ = encoder(edge_attr)\n",
    "            target_edge_attr, _ = encoder(target_edge_attr)\n",
    "            x, edge_attr, target_edge_attr = model(node_feats, edge_index, edge_attr, target_edge_attr)\n",
    "            x_target = x[target_edge_index.T].reshape(-1, 2 * channels)#.relu()\n",
    "            x_target = torch.cat((x_target, target_edge_attr), 1)\n",
    "            num_pred, cat_pred = mcm_decoder(x_target)\n",
    "            num_pred = num_pred.cpu()\n",
    "            cat_pred = [x.cpu() for x in cat_pred]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            t_loss, loss_c, loss_n = ssloss.mcm_loss(cat_pred, num_pred, tf.y)\n",
    "            t_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_accum += (t_loss.item() * len(tf.y))\n",
    "            total_count += len(tf.y)\n",
    "            t_c += loss_c[1]\n",
    "            t_n += loss_n[1]\n",
    "            loss_c_accum += loss_c[0].item()\n",
    "            loss_n_accum += loss_n[0].item()\n",
    "            t.set_postfix(loss=f'{loss_accum/total_count:.4f}', loss_c=f'{loss_c_accum/t_c:.4f}', loss_n=f'{loss_n_accum/t_n:.4f}')\n",
    "            wandb.log({\"train_loss\": loss_accum/total_count, \"train_loss_c\": loss_c_accum/t_c, \"train_loss_n\": loss_n_accum/t_n})\n",
    "    return {'loss': loss_accum / total_count}\n",
    "\n",
    "def train_lp(dataset, loader, epoc: int, encoder, model, lp_decoder, optimizer, scheduler) -> float:\n",
    "    encoder.train()\n",
    "    model.train()\n",
    "    lp_decoder.train()\n",
    "    total_count = 0\n",
    "    loss_lp_accum = 0\n",
    "\n",
    "    with tqdm(loader, desc=f'Epoch {epoc}') as t:\n",
    "        for tf in t:\n",
    "            batch_size = len(tf.y)\n",
    "            node_feats, edge_index, edge_attr, target_edge_index, target_edge_attr = lp_inputs(tf, dataset)\n",
    "            tf = tf.to(device)\n",
    "            edge_attr, _ = encoder(edge_attr)\n",
    "            target_edge_attr, _ = encoder(target_edge_attr)\n",
    "            x_gnn, edge_attr, target_edge_attr = model(node_feats, edge_index, edge_attr, target_edge_attr)\n",
    "            pos_edge_index = target_edge_index[:, :batch_size]\n",
    "            neg_edge_index = target_edge_index[:, batch_size:]\n",
    "            pos_edge_attr = target_edge_attr[:batch_size,:]\n",
    "            neg_edge_attr = target_edge_attr[batch_size:,:]\n",
    "            pos_pred, neg_pred = lp_decoder(x_gnn, pos_edge_index, pos_edge_attr, neg_edge_index, neg_edge_attr)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            link_loss = ssloss.lp_loss(pos_pred, neg_pred)\n",
    "            link_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_count += len(tf.y)\n",
    "            loss_lp_accum += link_loss.item() * len(tf.y)\n",
    "            t.set_postfix(loss_lp=f'{loss_lp_accum/total_count:.4f}')\n",
    "            # wandb.log({\"train_loss_lp\": loss_lp_accum/total_count})\n",
    "            #wandb.log({\"lr\": scheduler.get_last_lr()[0]})\n",
    "    return {'loss': loss_lp_accum / total_count} \n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_mcm(dataset, loader: DataLoader, encoder, model, mcm_decoder, dataset_name) -> float:\n",
    "    encoder.eval()\n",
    "    model.eval()\n",
    "    mcm_decoder.eval()\n",
    "    total_count = 0\n",
    "    accum_acc = accum_l2 = 0\n",
    "    loss_c_accum = loss_n_accum = total_count = t_c = t_n = 0\n",
    "    t_n = t_c = 0\n",
    "    with tqdm(loader, desc=f'Evaluating') as t:\n",
    "        for tf in t:\n",
    "            node_feats, edge_index, edge_attr, target_edge_index, target_edge_attr = mcm_inputs(tf, dataset)\n",
    "            tf = tf.to(device)\n",
    "            edge_attr, _ = encoder(edge_attr)\n",
    "            target_edge_attr, _ = encoder(target_edge_attr)\n",
    "            x, edge_attr, target_edge_attr = model(node_feats, edge_index, edge_attr, target_edge_attr)\n",
    "            x_target = x[target_edge_index.T].reshape(-1, 2 * channels)#.relu()\n",
    "            x_target = torch.cat((x_target, target_edge_attr), 1)\n",
    "            num_pred, cat_pred = mcm_decoder(x_target)\n",
    "            num_pred = num_pred.cpu()\n",
    "            cat_pred = [x.cpu() for x in cat_pred]\n",
    "            _, loss_c, loss_n = ssloss.mcm_loss(cat_pred, num_pred, tf.y)\n",
    "            t_c += loss_c[1]\n",
    "            t_n += loss_n[1] \n",
    "            loss_c_accum += loss_c[0].item()\n",
    "            loss_n_accum += loss_n[0].item()\n",
    "            total_count += len(num_pred)\n",
    "            for i, ans in enumerate(tf.y):\n",
    "                if ans[1] > (num_numerical-1):\n",
    "                    accum_acc += (cat_pred[int(ans[1])-num_numerical][i].argmax() == int(ans[0]))\n",
    "                else:\n",
    "                    accum_l2 += torch.square(ans[0] - num_pred[i][int(ans[1])]) #rmse\n",
    "            t.set_postfix(\n",
    "                accuracy=f'{accum_acc / t_c:.4f}',\n",
    "                rmse=f'{torch.sqrt(accum_l2 / t_n):.4f}', \n",
    "                loss_mcm=f'{(loss_c_accum/t_c) + (loss_n_accum/t_n):.4f}',\n",
    "                loss_c = f'{loss_c_accum/t_c:.4f}', \n",
    "                loss_n = f'{loss_n_accum/t_n:.4f}',\n",
    "            )\n",
    "            wandb.log({\n",
    "                f\"{dataset_name}_loss_mcm\": (loss_c_accum/t_c) + (loss_n_accum/t_n),\n",
    "                f\"{dataset_name}_loss_c\": loss_c_accum/t_c,\n",
    "                f\"{dataset_name}_loss_n\": loss_n_accum/t_n,\n",
    "            })\n",
    "        accuracy = accum_acc / t_c\n",
    "        rmse = torch.sqrt(accum_l2 / t_n)\n",
    "        wandb.log({\n",
    "            f\"{dataset_name}_accuracy\": accuracy,\n",
    "            f\"{dataset_name}_rmse\": rmse,\n",
    "        })\n",
    "        return {\"accuracy\": accuracy, \"rmse\": rmse}\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_lp(dataset, loader: DataLoader, encoder, model, lp_decoder, dataset_name) -> float:\n",
    "    encoder.eval()\n",
    "    model.eval()\n",
    "    lp_decoder.eval()\n",
    "    mrrs = []\n",
    "    hits1 = []\n",
    "    hits2 = []\n",
    "    hits5 = []\n",
    "    hits10 = []\n",
    "    loss_accum = 0\n",
    "    total_count = 0\n",
    "    loss_accum = loss_lp_accum = total_count = 0\n",
    "    with tqdm(loader, desc=f'Evaluating') as t:\n",
    "        for tf in t:\n",
    "            batch_size = len(tf.y)\n",
    "            node_feats, input_edge_index, input_edge_attr, target_edge_index, target_edge_attr = lp_inputs(tf, dataset)\n",
    "            tf = tf.to(device)\n",
    "            input_edge_attr, _ = encoder(input_edge_attr)\n",
    "            target_edge_attr, _ = encoder(target_edge_attr)\n",
    "            x_gnn, edge_attr, target_edge_attr = model(node_feats, input_edge_index, input_edge_attr, target_edge_attr)\n",
    "            pos_edge_index = target_edge_index[:, :batch_size]\n",
    "            neg_edge_index = target_edge_index[:, batch_size:]\n",
    "            pos_edge_attr = target_edge_attr[:batch_size,:]\n",
    "            neg_edge_attr = target_edge_attr[batch_size:,:]\n",
    "\n",
    "            pos_pred, neg_pred = lp_decoder(x_gnn, pos_edge_index, pos_edge_attr, neg_edge_index, neg_edge_attr)\n",
    "            loss = ssloss.lp_loss(pos_pred, neg_pred)\n",
    "            \n",
    "            loss_lp_accum += loss * len(pos_pred)\n",
    "            loss_accum += float(loss) * len(pos_pred)\n",
    "            total_count += len(pos_pred)\n",
    "            mrr_score, hits = ssmetric.mrr(pos_pred, neg_pred, [1,2,5,10], num_neg_samples)\n",
    "            mrrs.append(mrr_score)\n",
    "            hits1.append(hits['hits@1'])\n",
    "            hits2.append(hits['hits@2'])\n",
    "            hits5.append(hits['hits@5'])\n",
    "            hits10.append(hits['hits@10'])\n",
    "            t.set_postfix(\n",
    "                mrr=f'{np.mean(mrrs):.4f}',\n",
    "                hits1=f'{np.mean(hits1):.4f}',\n",
    "                hits2=f'{np.mean(hits2):.4f}',\n",
    "                hits5=f'{np.mean(hits5):.4f}',\n",
    "                hits10=f'{np.mean(hits10):.4f}',\n",
    "                loss_lp = f'{loss_lp_accum/total_count:.4f}',\n",
    "            )\n",
    "        wandb.log({\n",
    "            f\"{dataset_name}_loss_lp\": loss_lp_accum/total_count,\n",
    "        })\n",
    "        mrr_score = np.mean(mrrs)\n",
    "        hits1 = np.mean(hits1)\n",
    "        hits2 = np.mean(hits2)\n",
    "        hits5 = np.mean(hits5)\n",
    "        hits10 = np.mean(hits10)\n",
    "        wandb.log({\n",
    "            f\"{dataset_name}_mrr\": mrr_score,\n",
    "            f\"{dataset_name}_hits@1\": hits1,\n",
    "            f\"{dataset_name}_hits@2\": hits2,\n",
    "            f\"{dataset_name}_hits@5\": hits5,\n",
    "            f\"{dataset_name}_hits@10\": hits10,\n",
    "        })\n",
    "        return {\"mrr\": mrr_score, \"hits@1\": hits1, \"hits@2\": hits2, \"hits@5\": hits5, \"hits@10\": hits10}\n",
    "\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval(dataset, loader, encoder, model, lp_decoder, mcm_decoder, dataset_name):\n",
    "    encoder.eval()\n",
    "    model.eval()\n",
    "    lp_decoder.eval()\n",
    "    mcm_decoder.eval()\n",
    "    mrrs = []\n",
    "    hits1 = []\n",
    "    hits2 = []\n",
    "    hits5 = []\n",
    "    hits10 = []\n",
    "    loss_accum = 0\n",
    "    total_count = 0\n",
    "    loss_accum = loss_lp_accum = total_count = 0\n",
    "    accum_acc = accum_l2 = 0\n",
    "    loss_c_accum = loss_n_accum = t_c = t_n = 0\n",
    "    with tqdm(loader, desc=f'Evaluating') as t:\n",
    "        for tf in t:\n",
    "            batch_size = len(tf.y)\n",
    "            node_feats, input_edge_index, input_edge_attr, target_edge_index, target_edge_attr = lp_inputs(tf, dataset)\n",
    "            input_edge_attr, _ = encoder(input_edge_attr)\n",
    "            target_edge_attr, _ = encoder(target_edge_attr)\n",
    "            x_gnn, edge_attr, target_edge_attr = model(node_feats, input_edge_index, input_edge_attr, target_edge_attr)\n",
    "            pos_edge_index = target_edge_index[:, :batch_size]\n",
    "            neg_edge_index = target_edge_index[:, batch_size:]\n",
    "            pos_edge_attr = target_edge_attr[:batch_size,:]\n",
    "            neg_edge_attr = target_edge_attr[batch_size:,:]\n",
    "            pos_pred, neg_pred = lp_decoder(x_gnn, pos_edge_index, pos_edge_attr, neg_edge_index, neg_edge_attr)\n",
    "\n",
    "            x_target = x_gnn[pos_edge_index.T].reshape(-1, 2 * channels)#.relu()\n",
    "            x_target = torch.cat((x_target, pos_edge_attr), 1)\n",
    "            num_pred, cat_pred = mcm_decoder(x_target)\n",
    "            num_pred = num_pred.cpu()\n",
    "            cat_pred = [x.cpu() for x in cat_pred] \n",
    "\n",
    "            link_loss = ssloss.lp_loss(pos_pred, neg_pred)\n",
    "            t_loss, loss_c, loss_n = ssloss.mcm_loss(cat_pred, num_pred, tf.y)\n",
    "            moco_loss = mocoloss.loss([link_loss, t_loss])\n",
    "\n",
    "            loss_accum += ((link_loss.item()*moco_loss[0]+(t_loss.item()*moco_loss[1])) * len(tf.y))\n",
    "            total_count += len(tf.y)\n",
    "            t_c += loss_c[1]\n",
    "            t_n += loss_n[1]\n",
    "            loss_c_accum += loss_c[0].item()\n",
    "            loss_n_accum += loss_n[0].item()\n",
    "            loss_lp_accum += link_loss.item() * len(tf.y)\n",
    "            mrr_score, hits = ssmetric.mrr(pos_pred, neg_pred, [1,2,5,10], num_neg_samples)\n",
    "            mrrs.append(mrr_score)\n",
    "            hits1.append(hits['hits@1'])\n",
    "            hits2.append(hits['hits@2'])\n",
    "            hits5.append(hits['hits@5'])\n",
    "            hits10.append(hits['hits@10'])\n",
    "\n",
    "            for i, ans in enumerate(tf.y):\n",
    "                if ans[1] > (num_numerical-1):\n",
    "                    accum_acc += (cat_pred[int(ans[1])-num_numerical][i].argmax() == int(ans[0]))\n",
    "                else:\n",
    "                    accum_l2 += torch.square(ans[0] - num_pred[i][int(ans[1])]) #rmse\n",
    "\n",
    "            t.set_postfix(\n",
    "                mrr=f'{np.mean(mrrs):.4f}',\n",
    "                hits1=f'{np.mean(hits1):.4f}',\n",
    "                hits2=f'{np.mean(hits2):.4f}',\n",
    "                hits5=f'{np.mean(hits5):.4f}',\n",
    "                hits10=f'{np.mean(hits10):.4f}',\n",
    "                loss_lp = f'{loss_lp_accum/total_count:.4f}',\n",
    "                accuracy=f'{accum_acc / t_c:.4f}',\n",
    "                rmse=f'{torch.sqrt(accum_l2 / t_n):.4f}', \n",
    "                loss_mcm=f'{(loss_c_accum/t_c) + (loss_n_accum/t_n):.4f}',\n",
    "                loss_c = f'{loss_c_accum/t_c:.4f}', \n",
    "                loss_n = f'{loss_n_accum/t_n:.4f}'\n",
    "            )\n",
    "        mrr_score = np.mean(mrrs)\n",
    "        hits1 = np.mean(hits1)\n",
    "        hits2 = np.mean(hits2)\n",
    "        hits5 = np.mean(hits5)\n",
    "        hits10 = np.mean(hits10)\n",
    "        accuracy = accum_acc / t_c\n",
    "        rmse = torch.sqrt(accum_l2 / t_n)\n",
    "        wandb.log({\n",
    "            f\"{dataset_name}_loss_mcm\": (loss_c_accum/t_c) + (loss_n_accum/t_n),\n",
    "            f\"{dataset_name}_loss_c\": loss_c_accum/t_c,\n",
    "            f\"{dataset_name}_loss_n\": loss_n_accum/t_n,\n",
    "            f\"{dataset_name}_loss_lp\": loss_lp_accum/total_count,\n",
    "            f\"{dataset_name}_mrr\": mrr_score,\n",
    "            f\"{dataset_name}_hits@1\": hits1,\n",
    "            f\"{dataset_name}_hits@2\": hits2,\n",
    "            f\"{dataset_name}_hits@5\": hits5,\n",
    "            f\"{dataset_name}_hits@10\": hits10,\n",
    "            f\"{dataset_name}_accuracy\": accuracy,\n",
    "            f\"{dataset_name}_rmse\": rmse,\n",
    "        })\n",
    "        return {\"mrr\": mrr_score, \"hits@1\": hits1, \"hits@2\": hits2, \"hits@5\": hits5, \"hits@10\": hits10}, {\"accuracy\": accuracy, \"rmse\": rmse}\n",
    "\n",
    "num_categorical = [len(dataset.col_stats[col][StatType.COUNT][0]) for col in dataset.tensor_frame.col_names_dict[stype.categorical]] if stype.categorical in dataset.tensor_frame.col_names_dict else 0\n",
    "mcm_decoder = MCMHead(channels, num_numerical, num_categorical, w=3).to(device)\n",
    "lp_decoder = LinkPredHead(n_classes=1, n_hidden=channels, dropout=dropout).to(device)\n",
    "\n",
    "stype_encoder_dict = {\n",
    "    stype.categorical: EmbeddingEncoder(),\n",
    "    stype.numerical: LinearEncoder(),\n",
    "    stype.timestamp: TimestampEncoder(),\n",
    "}\n",
    "encoder = StypeWiseFeatureEncoder(\n",
    "            out_channels=channels,\n",
    "            col_stats=dataset.col_stats,\n",
    "            col_names_dict=dataset.tensor_frame.col_names_dict,\n",
    "            stype_encoder_dict=stype_encoder_dict,\n",
    ").to(device)\n",
    "\n",
    "edge_index = dataset.train_graph.edge_index\n",
    "num_nodes = dataset.train_graph.num_nodes\n",
    "in_degrees = degree(edge_index[1], num_nodes=num_nodes, dtype=torch.long)\n",
    "max_in_degree = int(in_degrees.max())\n",
    "in_degree_histogram = torch.zeros(max_in_degree + 1, dtype=torch.long)\n",
    "in_degree_histogram += torch.bincount(in_degrees, minlength=in_degree_histogram.numel())\n",
    "model = TABGNN(\n",
    "    encoder=encoder,\n",
    "    channels=channels,\n",
    "    edge_dim=channels*dataset.tensor_frame.num_cols,\n",
    "    num_layers=num_layers, \n",
    "    dropout=dropout,\n",
    "    deg=in_degree_histogram\n",
    ")\n",
    "model = torch.compile(model, dynamic=True) if compile else model\n",
    "model.to(device)\n",
    "\n",
    "model_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "encoder_params = sum(p.numel() for p in encoder.parameters() if p.requires_grad)\n",
    "lp_decoder_params = sum(p.numel() for p in lp_decoder.parameters() if p.requires_grad)\n",
    "mcm_decoder_params = sum(p.numel() for p in mcm_decoder.parameters() if p.requires_grad)\n",
    "logger.info(f\"model_params: {model_params}\")\n",
    "logger.info(f\"encoder_params: {encoder_params}\")\n",
    "logger.info(f\"lp_decoder_params: {lp_decoder_params}\")\n",
    "logger.info(f\"mcm_decoder_params: {mcm_decoder_params}\")\n",
    "learnable_params = model_params + encoder_params + lp_decoder_params + mcm_decoder_params\n",
    "logger.info(f\"learnable_params: {learnable_params}\")\n",
    "# wandb.log({\"learnable_params\": learnable_params})\n",
    "\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    # {'params': [p for n, p in encoder.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': weight_decay},\n",
    "    # {'params': [p for n, p in encoder.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': weight_decay},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n",
    "    {'params': [p for n, p in mcm_decoder.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': weight_decay},\n",
    "    {'params': [p for n, p in mcm_decoder.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n",
    "    {'params': [p for n, p in lp_decoder.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': weight_decay},\n",
    "    {'params': [p for n, p in lp_decoder.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=lr, eps=eps)\n",
    "scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=lr/3, max_lr=3*lr, step_size_up=2000, cycle_momentum=False)\n",
    "ssloss = SSLoss(device, num_numerical)\n",
    "ssmetric = SSMetric(device)\n",
    "mocoloss = MoCoLoss(model, 2, device, beta=0.999, beta_sigma=0.1, gamma=0.999, gamma_sigma=0.1, rho=0.05)\n",
    "\n",
    "save_dir = '/mnt/data/.cache/saved_models'\n",
    "#save_dir = '/scratch/takyildiz/.cache/saved_models'\n",
    "# run_id = wandb.run.id\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "best_lp = 0\n",
    "best_acc = 0\n",
    "best_rmse = 2\n",
    "\n",
    "\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lp_inputs(tf: TensorFrame, dataset):\n",
    "    edges = tf.y[:,-3:]\n",
    "    batch_size = len(edges)\n",
    "    start = time.time()\n",
    "    khop_source, khop_destination, idx = dataset.sample_neighbors(edges, 'train')\n",
    "    \n",
    "    edge_attr = dataset.tensor_frame.__getitem__(idx)\n",
    "\n",
    "    nodes = torch.unique(torch.cat([khop_source, khop_destination]))\n",
    "    num_nodes = nodes.shape[0]\n",
    "    node_feats = torch.ones(num_nodes).view(-1,num_nodes).t()\n",
    "\n",
    "    n_id_map = {value.item(): index for index, value in enumerate(nodes)}\n",
    "    local_khop_source = torch.tensor([n_id_map[node.item()] for node in khop_source], dtype=torch.long)\n",
    "    local_khop_destination = torch.tensor([n_id_map[node.item()] for node in khop_destination], dtype=torch.long)\n",
    "    edge_index = torch.cat((local_khop_source.unsqueeze(0), local_khop_destination.unsqueeze(0)))\n",
    "\n",
    "    drop_edge_ind = torch.tensor([x for x in range(int(batch_size))])\n",
    "    mask = torch.zeros((edge_index.shape[1],)).long() #[E, ]\n",
    "    mask = mask.index_fill_(dim=0, index=drop_edge_ind, value=1).bool() #[E, ]\n",
    "    input_edge_index = edge_index[:, ~mask]\n",
    "    input_edge_attr  = edge_attr[~mask]\n",
    "\n",
    "    pos_edge_index = edge_index[:, mask]\n",
    "    pos_edge_attr  = edge_attr[mask]\n",
    "    #logger.info(f\"sample_neighbors time: {time.time()-start}\")\n",
    "\n",
    "    start = time.time()\n",
    "    # generate/sample negative edges\n",
    "    neg_edges = []\n",
    "    target_dict = pos_edge_attr.feat_dict\n",
    "    for key, value in pos_edge_attr.feat_dict.items():\n",
    "        attr = []\n",
    "        # duplicate each row of the tensor by num_neg_samples times repeated values must be contiguous\n",
    "        for r in value:\n",
    "            if key == stype.timestamp:\n",
    "                attr.append(r.repeat(num_neg_samples, 1, 1))\n",
    "            else:\n",
    "                attr.append(r.repeat(num_neg_samples, 1))\n",
    "        target_dict[key] = torch.cat([target_dict[key], torch.cat(attr, dim=0)], dim=0)\n",
    "    target_edge_attr = TensorFrame(target_dict, pos_edge_attr.col_names_dict)\n",
    "    #logger.info(f\"target_edge_attr time: {time.time()-start}\")\n",
    "\n",
    "    start = time.time()\n",
    "    nodeset = set(range(edge_index.max()+1))\n",
    "    for i, edge in enumerate(pos_edge_index.t()):\n",
    "        src, dst = edge[0], edge[1]\n",
    "\n",
    "        # # # Chose negative examples in a smart way\n",
    "        # unavail_mask = (edge_index == src).any(dim=0) | (edge_index == dst).any(dim=0)\n",
    "        # unavail_nodes = torch.unique(edge_index[:, unavail_mask])\n",
    "        # unavail_nodes = set(unavail_nodes.tolist())\n",
    "        # avail_nodes = nodeset - unavail_nodes\n",
    "        # avail_nodes = torch.tensor(list(avail_nodes))\n",
    "        # # Finally, emmulate np.random.choice() to chose randomly amongst available nodes\n",
    "        # indices = torch.randperm(len(avail_nodes))[:num_neg_samples]\n",
    "        # neg_nodes = avail_nodes[indices]\n",
    "        \n",
    "        # # Create a mask of unavailable nodes\n",
    "        # unavail_mask = torch.isin(edge_index.flatten(), torch.tensor([src, dst])).view(edge_index.shape).any(dim=0)\n",
    "        \n",
    "        # # Get unique unavailable nodes\n",
    "        # unavail_nodes = torch.unique(edge_index[:, unavail_mask])\n",
    "        \n",
    "        # # Convert to set for fast set operations\n",
    "        # unavail_nodes_set = set(unavail_nodes.tolist())\n",
    "        \n",
    "        # # Determine available nodes by set difference\n",
    "        # avail_nodes = list(nodeset - unavail_nodes_set)\n",
    "        \n",
    "        # # Convert available nodes back to tensor\n",
    "        # avail_nodes = torch.tensor(avail_nodes, dtype=torch.long)\n",
    "        \n",
    "        # # Randomly select negative samples from available nodes\n",
    "        # indices = torch.randperm(len(avail_nodes))[:num_neg_samples]\n",
    "        # neg_nodes = avail_nodes[indices]\n",
    "\n",
    "        # Create a mask of unavailable nodes\n",
    "        unavail_mask = torch.isin(edge_index.flatten(), torch.tensor([src, dst]))\n",
    "        unavail_nodes = edge_index.flatten()[unavail_mask].unique()\n",
    "\n",
    "        # Create a mask for all nodes\n",
    "        all_nodes = torch.arange(max(nodeset) + 1)\n",
    "        avail_mask = ~torch.isin(all_nodes, unavail_nodes)\n",
    "\n",
    "        # Get available nodes\n",
    "        avail_nodes = all_nodes[avail_mask]\n",
    "\n",
    "        # Randomly select negative samples from available nodes\n",
    "        neg_nodes = avail_nodes[torch.randint(high=len(avail_nodes), size=(num_neg_samples,))]\n",
    "\n",
    "        # Generate num_neg_samples/2 negative edges with the same source but different destinations\n",
    "        num_neg_samples_half = int(num_neg_samples/2)\n",
    "        neg_dsts = neg_nodes[:num_neg_samples_half]  # Selecting num_neg_samples/2 random destination nodes for the source\n",
    "        neg_edges_src = torch.stack([src.repeat(num_neg_samples_half), neg_dsts], dim=0)\n",
    "        \n",
    "        # Generate num_neg_samples/2 negative edges with the same destination but different sources\n",
    "        neg_srcs = neg_nodes[num_neg_samples_half:]  # Selecting num_neg_samples/2 random source nodes for the destination\n",
    "        neg_edges_dst = torch.stack([neg_srcs, dst.repeat(num_neg_samples_half)], dim=0)\n",
    "\n",
    "        # Add these negative edges to the list\n",
    "        neg_edges.append(neg_edges_src)\n",
    "        neg_edges.append(neg_edges_dst)\n",
    "    \n",
    "    input_edge_index = input_edge_index.to(device)\n",
    "    input_edge_attr = input_edge_attr.to(device)\n",
    "    #pos_edge_index = pos_edge_index.to(device)\n",
    "    #pos_edge_attr = pos_edge_attr.to(device)\n",
    "    node_feats = node_feats.to(device)\n",
    "    if len(neg_edges) > 0:\n",
    "        #neg_edge_index = torch.cat(neg_edges, dim=1).to(device)\n",
    "        neg_edge_index = torch.cat(neg_edges, dim=1)\n",
    "    target_edge_index = torch.cat([pos_edge_index, neg_edge_index], dim=1).to(device)\n",
    "    target_edge_attr = target_edge_attr.to(device)\n",
    "    #logger.info(f\"negative edges: {time.time()-start}\")\n",
    "    #sys.exit()\n",
    "    return node_feats, input_edge_index, input_edge_attr, target_edge_index, target_edge_attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, loader, epoc: int, encoder, model, lp_decoder, mcm_decoder, optimizer, scheduler):\n",
    "    encoder.train()\n",
    "    model.train()\n",
    "    lp_decoder.train()\n",
    "    mcm_decoder.train()\n",
    "    loss_accum = total_count = 0\n",
    "    loss_lp_accum = loss_c_accum = loss_n_accum = total_count = t_c = t_n = 0\n",
    "    with tqdm(loader, desc=f'Epoch {epoc}') as t:\n",
    "        for tf in t:\n",
    "            ss = time.time()\n",
    "            batch_size = len(tf.y)\n",
    "            start = time.time()\n",
    "            node_feats, input_edge_index, input_edge_attr, target_edge_index, target_edge_attr = lp_inputs(tf, dataset)\n",
    "            l_time = time.time()-start\n",
    "            logger.info(f\"lp_inputs time: {l_time}\")\n",
    "            start = time.time()\n",
    "            input_edge_attr, _ = encoder(input_edge_attr)\n",
    "            target_edge_attr, _ = encoder(target_edge_attr)\n",
    "            x_gnn, edge_attr, target_edge_attr = model(node_feats, input_edge_index, input_edge_attr, target_edge_attr)\n",
    "            pos_edge_index = target_edge_index[:, :batch_size]\n",
    "            neg_edge_index = target_edge_index[:, batch_size:]\n",
    "            pos_edge_attr = target_edge_attr[:batch_size,:]\n",
    "            neg_edge_attr = target_edge_attr[batch_size:,:]\n",
    "            pos_pred, neg_pred = lp_decoder(x_gnn, pos_edge_index, pos_edge_attr, neg_edge_index, neg_edge_attr)\n",
    "\n",
    "            x_target = x_gnn[pos_edge_index.T].reshape(-1, 2 * channels)#.relu()\n",
    "            x_target = torch.cat((x_target, pos_edge_attr), 1)\n",
    "            num_pred, cat_pred = mcm_decoder(x_target)\n",
    "            num_pred = num_pred.cpu()\n",
    "            cat_pred = [x.cpu() for x in cat_pred]\n",
    "            m_time = time.time()-start\n",
    "            logger.info(f\"model time: {m_time}\")\n",
    "            \n",
    "            start = time.time()\n",
    "            optimizer.zero_grad()\n",
    "            link_loss = ssloss.lp_loss(pos_pred, neg_pred)\n",
    "            t_loss, loss_c, loss_n = ssloss.mcm_loss(cat_pred, num_pred, tf.y)\n",
    "            loss = link_loss + t_loss\n",
    "            loss.backward()\n",
    "            #moco_loss = mocoloss.loss([link_loss, t_loss])\n",
    "            optimizer.step()\n",
    "            # scheduler.step()\n",
    "\n",
    "            #loss_accum += ((link_loss.item()*moco_loss[0]+(t_loss.item()*moco_loss[1])) * len(tf.y))\n",
    "            total_count += len(tf.y)\n",
    "            t_c += loss_c[1]\n",
    "            t_n += loss_n[1]\n",
    "            loss_c_accum += loss_c[0].item()\n",
    "            loss_n_accum += loss_n[0].item()\n",
    "            loss_lp_accum += link_loss.item() * len(tf.y)\n",
    "            b_time = time.time()-start\n",
    "            logger.info(f\"backward time: {b_time}\")\n",
    "            t.set_postfix(loss=f'{loss_accum/total_count:.4f}', loss_lp=f'{loss_lp_accum/total_count:.4f}', loss_c=f'{loss_c_accum/t_c:.4f}', loss_n=f'{loss_n_accum/t_n:.4f}')#, moco_loss=f'{moco_loss[0]:.4f},{moco_loss[1]:.4f}')\n",
    "            logger.info(f\"total time: {b_time+m_time+l_time}\")\n",
    "            logger.info('-----------------')\n",
    "    return {'loss': loss_accum / total_count} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c7c6fbbdccb46b8b62a89f2f53136de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 0:   0%|          | 0/16245 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-07 15:00:02 - INFO - lp_inputs time: 0.17715907096862793\n",
      "2024-07-07 15:00:02 - INFO - model time: 0.060776710510253906\n",
      "2024-07-07 15:00:02 - INFO - backward time: 0.1234438419342041\n",
      "2024-07-07 15:00:02 - INFO - total time: 0.36137962341308594\n",
      "2024-07-07 15:00:02 - INFO - -----------------\n",
      "2024-07-07 15:00:02 - INFO - lp_inputs time: 0.08751940727233887\n",
      "2024-07-07 15:00:02 - INFO - model time: 0.05451607704162598\n",
      "2024-07-07 15:00:03 - INFO - backward time: 0.13009238243103027\n",
      "2024-07-07 15:00:03 - INFO - total time: 0.2721278667449951\n",
      "2024-07-07 15:00:03 - INFO - -----------------\n",
      "2024-07-07 15:00:03 - INFO - lp_inputs time: 0.0982503890991211\n",
      "2024-07-07 15:00:03 - INFO - model time: 0.0487363338470459\n",
      "2024-07-07 15:00:03 - INFO - backward time: 0.13444948196411133\n",
      "2024-07-07 15:00:03 - INFO - total time: 0.2814362049102783\n",
      "2024-07-07 15:00:03 - INFO - -----------------\n",
      "2024-07-07 15:00:03 - INFO - lp_inputs time: 0.08996176719665527\n",
      "2024-07-07 15:00:03 - INFO - model time: 0.04641866683959961\n",
      "2024-07-07 15:00:03 - INFO - backward time: 0.12282347679138184\n",
      "2024-07-07 15:00:03 - INFO - total time: 0.2592039108276367\n",
      "2024-07-07 15:00:03 - INFO - -----------------\n",
      "2024-07-07 15:00:03 - INFO - lp_inputs time: 0.08789277076721191\n",
      "2024-07-07 15:00:03 - INFO - model time: 0.04573798179626465\n",
      "2024-07-07 15:00:03 - INFO - backward time: 0.08533525466918945\n",
      "2024-07-07 15:00:03 - INFO - total time: 0.21896600723266602\n",
      "2024-07-07 15:00:03 - INFO - -----------------\n",
      "2024-07-07 15:00:03 - INFO - lp_inputs time: 0.09118175506591797\n",
      "2024-07-07 15:00:03 - INFO - model time: 0.046865224838256836\n",
      "2024-07-07 15:00:04 - INFO - backward time: 0.08292174339294434\n",
      "2024-07-07 15:00:04 - INFO - total time: 0.22096872329711914\n",
      "2024-07-07 15:00:04 - INFO - -----------------\n",
      "2024-07-07 15:00:04 - INFO - lp_inputs time: 0.09257030487060547\n",
      "2024-07-07 15:00:04 - INFO - model time: 0.04732823371887207\n",
      "2024-07-07 15:00:04 - INFO - backward time: 0.08297944068908691\n",
      "2024-07-07 15:00:04 - INFO - total time: 0.22287797927856445\n",
      "2024-07-07 15:00:04 - INFO - -----------------\n",
      "2024-07-07 15:00:04 - INFO - lp_inputs time: 0.0963592529296875\n",
      "2024-07-07 15:00:04 - INFO - model time: 0.04842567443847656\n",
      "2024-07-07 15:00:04 - INFO - backward time: 0.08253169059753418\n",
      "2024-07-07 15:00:04 - INFO - total time: 0.22731661796569824\n",
      "2024-07-07 15:00:04 - INFO - -----------------\n",
      "2024-07-07 15:00:04 - INFO - lp_inputs time: 0.094451904296875\n",
      "2024-07-07 15:00:04 - INFO - model time: 0.06013774871826172\n",
      "2024-07-07 15:00:04 - INFO - backward time: 0.08040261268615723\n",
      "2024-07-07 15:00:04 - INFO - total time: 0.23499226570129395\n",
      "2024-07-07 15:00:04 - INFO - -----------------\n",
      "2024-07-07 15:00:04 - INFO - lp_inputs time: 0.09548115730285645\n",
      "2024-07-07 15:00:04 - INFO - model time: 0.052935123443603516\n",
      "2024-07-07 15:00:04 - INFO - backward time: 0.08063721656799316\n",
      "2024-07-07 15:00:04 - INFO - total time: 0.22905349731445312\n",
      "2024-07-07 15:00:04 - INFO - -----------------\n",
      "2024-07-07 15:00:05 - INFO - lp_inputs time: 0.09429287910461426\n",
      "2024-07-07 15:00:05 - INFO - model time: 0.06162619590759277\n",
      "2024-07-07 15:00:05 - INFO - backward time: 0.08030533790588379\n",
      "2024-07-07 15:00:05 - INFO - total time: 0.23622441291809082\n",
      "2024-07-07 15:00:05 - INFO - -----------------\n",
      "2024-07-07 15:00:05 - INFO - lp_inputs time: 0.09020185470581055\n",
      "2024-07-07 15:00:05 - INFO - model time: 0.07431769371032715\n",
      "2024-07-07 15:00:05 - INFO - backward time: 0.08144140243530273\n",
      "2024-07-07 15:00:05 - INFO - total time: 0.24596095085144043\n",
      "2024-07-07 15:00:05 - INFO - -----------------\n",
      "2024-07-07 15:00:05 - INFO - lp_inputs time: 0.09599113464355469\n",
      "2024-07-07 15:00:05 - INFO - model time: 0.0872957706451416\n",
      "2024-07-07 15:00:05 - INFO - backward time: 0.08075833320617676\n",
      "2024-07-07 15:00:05 - INFO - total time: 0.26404523849487305\n",
      "2024-07-07 15:00:05 - INFO - -----------------\n",
      "2024-07-07 15:00:05 - INFO - lp_inputs time: 0.09749054908752441\n",
      "2024-07-07 15:00:05 - INFO - model time: 0.10534954071044922\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlp_decoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmcm_decoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 40\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(dataset, loader, epoc, encoder, model, lp_decoder, mcm_decoder, optimizer, scheduler)\u001b[0m\n\u001b[1;32m     37\u001b[0m t_loss, loss_c, loss_n \u001b[38;5;241m=\u001b[39m ssloss\u001b[38;5;241m.\u001b[39mmcm_loss(cat_pred, num_pred, tf\u001b[38;5;241m.\u001b[39my)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m#loss = link_loss + t_loss\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m#loss.backward()\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m moco_loss \u001b[38;5;241m=\u001b[39m \u001b[43mmocoloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlink_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_loss\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# scheduler.step()\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m#loss_accum += ((link_loss.item()*moco_loss[0]+(t_loss.item()*moco_loss[1])) * len(tf.y))\u001b[39;00m\n",
      "File \u001b[0;32m~/dev/models-for-relational-multimodal-data/src/nn/weighting/MoCo.py:205\u001b[0m, in \u001b[0;36mMoCoLoss.loss\u001b[0;34m(self, losses)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloss\u001b[39m(\u001b[38;5;28mself\u001b[39m, losses):\n\u001b[0;32m--> 205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlosses\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m        \u001b[49m\u001b[43mMoCo_beta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbeta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[43m        \u001b[49m\u001b[43mMoCo_beta_sigma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbeta_sigma\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[43m        \u001b[49m\u001b[43mMoCo_gamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgamma\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[43m        \u001b[49m\u001b[43mMoCo_gamma_sigma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgamma_sigma\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[43m        \u001b[49m\u001b[43mMoCo_rho\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrho\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/models-for-relational-multimodal-data/src/nn/weighting/MoCo.py:166\u001b[0m, in \u001b[0;36mMoCo.backward\u001b[0;34m(self, losses, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_grad_dim()\n\u001b[0;32m--> 166\u001b[0m     grads \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlosses\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbackward\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m tn \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtask_num):\n",
      "File \u001b[0;32m~/dev/models-for-relational-multimodal-data/src/nn/weighting/MoCo.py:52\u001b[0m, in \u001b[0;36mAbsWeighting._compute_grad\u001b[0;34m(self, losses, mode, rep_grad)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNo support \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m mode for gradient computation\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 52\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad_share_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrep, \u001b[38;5;28mdict\u001b[39m):\n",
      "File \u001b[0;32m~/dev/models-for-relational-multimodal-data/src/nn/weighting/MoCo.py:197\u001b[0m, in \u001b[0;36mMoCoLoss.zero_grad_share_params\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe model has no shared parameters\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mget_shared_params()\n\u001b[0;32m--> 197\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mzero_grad_share_params\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    198\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Set gradients of the shared parameters to zero.\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzero_grad_shared_parameters\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss = train(dataset, train_loader, 0, encoder, model, lp_decoder, mcm_decoder, optimizer, scheduler)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rel-mm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
