{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-14T08:39:14.686778Z",
     "start_time": "2024-05-14T08:39:14.671455Z"
    }
   },
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-14T08:39:16.860633Z",
     "start_time": "2024-05-14T08:39:15.480645Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_frame import stype\n",
    "from torch_frame.data import DataLoader\n",
    "from torch_frame.nn import (\n",
    "    EmbeddingEncoder,\n",
    "    LinearEncoder,\n",
    "    TimestampEncoder,\n",
    ")\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import get_inverse_sqrt_schedule\n",
    "\n",
    "import sys\n",
    "from icecream import ic\n",
    "import wandb\n",
    "torch.set_float32_matmul_precision('high')"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-14T08:39:18.462273Z",
     "start_time": "2024-05-14T08:39:17.595811Z"
    }
   },
   "source": [
    "from datasets.util.mask import PretrainType\n",
    "\n",
    "seed = 42\n",
    "batch_size = 1024\n",
    "channels = 32\n",
    "num_layers = 4\n",
    "\n",
    "data_split = [0.6, 0.2, 0.2]\n",
    "split_type = \"temporal\"\n",
    "\n",
    "pretrain = {PretrainType.MASK_VECTOR}\n",
    "compile = False\n",
    "lr = 5e-4\n",
    "eps = 1e-8\n",
    "epochs = 10\n",
    "args = {\n",
    "    \"testing\": False,\n",
    "    \"seed\": seed,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"channels\": channels,\n",
    "    \"num_layers\": num_layers,\n",
    "    \"pretrain\": pretrain,\n",
    "    \"compile\": compile,\n",
    "    \"lr\": lr,\n",
    "    \"eps\": eps,\n",
    "    \"epochs\": epochs,\n",
    "    \"data_split\": data_split,\n",
    "    \"split_type\": split_type,\n",
    "}\n"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-14T08:39:19.340558Z",
     "start_time": "2024-05-14T08:39:19.308757Z"
    }
   },
   "source": [
    "# wandb.login()\n",
    "# run = wandb.init(\n",
    "#     mode=\"disabled\" if args['testing'] else \"online\",\n",
    "#     project=f\"rel-mm\", \n",
    "#     name=\"model=fttransformer,dataset=IBM-AML_Hi_Sm,objective=MCM,loss=weighted_loss\", \n",
    "#     config=args\n",
    "# )"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-14T08:40:31.565014Z",
     "start_time": "2024-05-14T08:39:19.625031Z"
    }
   },
   "source": [
    "from src.datasets import IBMTransactionsAML\n",
    "#dataset = IBMTransactionsAML(root='/mnt/data/ibm-transactions-for-anti-money-laundering-aml/dummy.csv', pretrain=pretrain)\n",
    "dataset = IBMTransactionsAML(root='../ibm_data/dummy-c.csv', pretrain=pretrain, split_type='temporal', splits=data_split)\n",
    "ic(dataset)\n",
    "dataset.materialize()\n",
    "num_numerical = len(dataset.tensor_frame.col_names_dict[stype.numerical])\n",
    "num_categorical = len(dataset.tensor_frame.col_names_dict[stype.categorical])\n",
    "dataset.df.head(5)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| dataset: IBMTransactionsAML()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "   Timestamp From Bank   From ID  To Bank     To ID  Amount Received  \\\n",
       "0     451320  B_122094  408507.0   B_5466  355105.0         0.291561   \n",
       "1      49080     B_119  233932.0    B_223  287078.0         0.160232   \n",
       "2     422700    B_2824   20268.0     B_12  407962.0         0.174158   \n",
       "3     723600     B_126  238411.0     B_15  327018.0         0.192625   \n",
       "4     519300    B_3420   54077.0  B_21258   18014.0         0.204691   \n",
       "\n",
       "  Receiving Currency  Amount Paid Payment Currency Payment Format  \\\n",
       "0    Canadian Dollar     0.432908        US Dollar         Cheque   \n",
       "1        Saudi Riyal     0.216612      Saudi Riyal         Cheque   \n",
       "2           UK Pound     0.216833        US Dollar            ACH   \n",
       "3        Brazil Real     0.299131        US Dollar           Cash   \n",
       "4        Swiss Franc     0.219461        US Dollar    Credit Card   \n",
       "\n",
       "  Is Laundering  split                                        mask_vector  \n",
       "0             0      0  [[Euro, Receiving Currency], [0.29156126641406...  \n",
       "1             0      0            [[0.2166117237537929, Amount Received]]  \n",
       "2             0      0  [[0.216833368511414, Amount Received], [US Dol...  \n",
       "3             0      2  [[Bitcoin, Receiving Currency], [0.19262492237...  \n",
       "4             0      1  [[US Dollar, Receiving Currency], [0.204690615...  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>From Bank</th>\n",
       "      <th>From ID</th>\n",
       "      <th>To Bank</th>\n",
       "      <th>To ID</th>\n",
       "      <th>Amount Received</th>\n",
       "      <th>Receiving Currency</th>\n",
       "      <th>Amount Paid</th>\n",
       "      <th>Payment Currency</th>\n",
       "      <th>Payment Format</th>\n",
       "      <th>Is Laundering</th>\n",
       "      <th>split</th>\n",
       "      <th>mask_vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>451320</td>\n",
       "      <td>B_122094</td>\n",
       "      <td>408507.0</td>\n",
       "      <td>B_5466</td>\n",
       "      <td>355105.0</td>\n",
       "      <td>0.291561</td>\n",
       "      <td>Canadian Dollar</td>\n",
       "      <td>0.432908</td>\n",
       "      <td>US Dollar</td>\n",
       "      <td>Cheque</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[[Euro, Receiving Currency], [0.29156126641406...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>49080</td>\n",
       "      <td>B_119</td>\n",
       "      <td>233932.0</td>\n",
       "      <td>B_223</td>\n",
       "      <td>287078.0</td>\n",
       "      <td>0.160232</td>\n",
       "      <td>Saudi Riyal</td>\n",
       "      <td>0.216612</td>\n",
       "      <td>Saudi Riyal</td>\n",
       "      <td>Cheque</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[[0.2166117237537929, Amount Received]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>422700</td>\n",
       "      <td>B_2824</td>\n",
       "      <td>20268.0</td>\n",
       "      <td>B_12</td>\n",
       "      <td>407962.0</td>\n",
       "      <td>0.174158</td>\n",
       "      <td>UK Pound</td>\n",
       "      <td>0.216833</td>\n",
       "      <td>US Dollar</td>\n",
       "      <td>ACH</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[[0.216833368511414, Amount Received], [US Dol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>723600</td>\n",
       "      <td>B_126</td>\n",
       "      <td>238411.0</td>\n",
       "      <td>B_15</td>\n",
       "      <td>327018.0</td>\n",
       "      <td>0.192625</td>\n",
       "      <td>Brazil Real</td>\n",
       "      <td>0.299131</td>\n",
       "      <td>US Dollar</td>\n",
       "      <td>Cash</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>[[Bitcoin, Receiving Currency], [0.19262492237...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>519300</td>\n",
       "      <td>B_3420</td>\n",
       "      <td>54077.0</td>\n",
       "      <td>B_21258</td>\n",
       "      <td>18014.0</td>\n",
       "      <td>0.204691</td>\n",
       "      <td>Swiss Franc</td>\n",
       "      <td>0.219461</td>\n",
       "      <td>US Dollar</td>\n",
       "      <td>Credit Card</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[[US Dollar, Receiving Currency], [0.204690615...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-14T08:40:31.630461Z",
     "start_time": "2024-05-14T08:40:31.565919Z"
    }
   },
   "source": [
    "num_columns = num_numerical + num_categorical\n",
    "ic(\n",
    "    num_numerical,\n",
    "    num_categorical,\n",
    "    num_columns,\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| num_numerical: 2, num_categorical: 5, num_columns: 7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2, 5, 7)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-14T08:40:31.652319Z",
     "start_time": "2024-05-14T08:40:31.630975Z"
    }
   },
   "source": [
    "torch.manual_seed(seed)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# wandb.log({\"device\": str(device)})"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-14T08:40:31.689616Z",
     "start_time": "2024-05-14T08:40:31.653389Z"
    }
   },
   "source": [
    "train_dataset, val_dataset, test_dataset = dataset.split()"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-14T08:40:31.727040Z",
     "start_time": "2024-05-14T08:40:31.690445Z"
    }
   },
   "source": [
    "train_tensor_frame = train_dataset.tensor_frame\n",
    "val_tensor_frame = val_dataset.tensor_frame\n",
    "test_tensor_frame = test_dataset.tensor_frame\n",
    "train_loader = DataLoader(train_tensor_frame, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_tensor_frame, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_tensor_frame, batch_size=batch_size, shuffle=False)\n",
    "ic(len(train_loader), len(val_loader), len(test_loader))\n",
    "# wandb.log({\n",
    "#     \"train_loader size\": len(train_loader), \n",
    "#     \"val_loader size\": len(val_loader), \n",
    "#     \"test_loader size\": len(test_loader)\n",
    "# })"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| len(train_loader): 32, len(val_loader): 10, len(test_loader): 9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(32, 10, 9)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-14T09:27:03.557833Z",
     "start_time": "2024-05-14T09:27:03.220085Z"
    }
   },
   "source": [
    "\n",
    "from utils import SSLoss, SSMetric\n",
    "\n",
    "stype_encoder_dict = {\n",
    "    stype.categorical: EmbeddingEncoder(),\n",
    "    stype.numerical: LinearEncoder(),\n",
    "    stype.timestamp: TimestampEncoder(),\n",
    "}\n",
    "\n",
    "from src.nn.models.ft_transformer import FTTransformer \n",
    "model = FTTransformer(\n",
    "    channels=channels,\n",
    "    out_channels=None,\n",
    "    num_layers=num_layers,\n",
    "    col_stats=dataset.col_stats,\n",
    "    col_names_dict=train_tensor_frame.col_names_dict,\n",
    "    stype_encoder_dict=stype_encoder_dict,\n",
    "    pretrain = pretrain\n",
    ").to(device)\n",
    "\n",
    "model = torch.compile(model, dynamic=True) if compile else model\n",
    "learnable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "ic(learnable_params)\n",
    "# wandb.log({\"learnable_params\": learnable_params})\n",
    "\n",
    "# Prepare optimizer and lr scheduler\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ]\n",
    "optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=lr, eps=eps)\n",
    "scheduler = get_inverse_sqrt_schedule(optimizer, num_warmup_steps=0, timescale=1000)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "def calc_loss(pred, y):\n",
    "    accum_n = accum_c = t_n = t_c = 0\n",
    "    for i, ans in enumerate(y):\n",
    "        # ans --> [val, idx]\n",
    "        # pred --> feature_type_num X type_num X batch_size\n",
    "        if ans[1] > (num_numerical-1):\n",
    "            t_c += 1\n",
    "            a = torch.tensor(int(ans[0])).to(device)\n",
    "            accum_c += F.cross_entropy(pred[1][int(ans[1])-num_numerical][i], a)\n",
    "            del a\n",
    "        else:\n",
    "            t_n += 1\n",
    "            accum_n += torch.square(pred[0][i][int(ans[1])] - ans[0]) #mse\n",
    "    return (accum_n / t_n) + torch.sqrt(accum_c / t_c), (accum_c, t_c), (accum_n, t_n)\n",
    "\n",
    "\n",
    "def train(epoc: int) -> float:\n",
    "    model.train()\n",
    "    loss_accum = loss_c_accum = loss_n_accum = total_count = t_c = t_n = 0\n",
    "    ssloss = SSLoss(device, num_numerical)\n",
    "    with tqdm(train_loader, desc=f'Epoch {epoc}') as t:\n",
    "        for tf in t:\n",
    "            tf = tf.to(device)\n",
    "            pred = model(tf)\n",
    "            # loss, loss_c, loss_n = calc_loss(pred, tf.y)\n",
    "            loss, vector_loss, loss_c, loss_n = ssloss.mv_loss(pred, tf.y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_accum += float(loss) * len(tf.y)\n",
    "            loss_c_accum += loss_c[0]\n",
    "            loss_n_accum += loss_n[0]\n",
    "            total_count += len(tf.y)\n",
    "            t_c += loss_c[1]\n",
    "            t_n += loss_n[1]\n",
    "            # t.set_postfix(loss=f'{loss_accum/total_count:.4f}', loss_c = f'{loss_c_accum/t_c:.4f}', loss_n = f'{loss_n_accum/t_n:.4f}')\n",
    "            t.set_postfix(loss=f'{loss_accum/total_count:.4f}', loss_c = f'{loss_c_accum/t_c:.4f}', loss_n = f'{loss_n_accum/t_n:.4f}')\n",
    "            del pred\n",
    "            del tf\n",
    "        # wandb.log({\"train_loss\": loss_accum/total_count, \"train_loss_c\": loss_c_accum/t_c, \"train_loss_n\": loss_n_accum/t_n})\n",
    "    return ((loss_c_accum/t_c) * (num_categorical/num_columns)) + ((loss_n_accum/t_n) * (num_numerical/num_columns))\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(loader: DataLoader, dataset_name) -> float:\n",
    "    model.eval()\n",
    "    accum_acc = accum_l2 = 0\n",
    "    loss_c_accum = loss_n_accum = 0\n",
    "    t_n = t_c = 0\n",
    "    ssloss = SSLoss(device, num_numerical)\n",
    "    ssmetric = SSMetric(num_numerical)\n",
    "    with tqdm(loader, desc=f'Evaluating') as t:\n",
    "        for tf in t:\n",
    "            tf = tf.to(device)\n",
    "            pred = model(tf)\n",
    "   \n",
    "            loss, vector_loss, loss_c, loss_n = ssloss.mv_loss(pred, tf.y)\n",
    "            loss_c_accum += loss_c[0]\n",
    "            loss_n_accum += loss_n[0]\n",
    "            t_c += loss_c[1]\n",
    "            t_n += loss_n[1]\n",
    "            mask_vector_acc = ssmetric.mv_accuracy(pred, tf.y)\n",
    "            t.set_postfix(accuracy=f'{mask_vector_acc:.4f}', loss=f'{loss:.4f}', loss_c = f'{loss_c_accum/t_c:.4f}', loss_n = f'{loss_n_accum/t_n:.4f}', loss_vector = f'{vector_loss:.4f}')\n",
    "        # wandb.log({f\"{dataset_name}_accuracy\": accum_acc/t_c, f\"{dataset_name}_rmse\": torch.sqrt(accum_l2/t_n), f\"{dataset_name}_loss\": ((loss_c_accum/t_c) * (num_categorical/num_columns)) + ((loss_n_accum/t_n) * (num_numerical/num_columns)), f\"{dataset_name}_loss_c\": loss_c_accum/t_c, f\"{dataset_name}_loss_n\": loss_n_accum/t_n})\n",
    "        del tf\n",
    "        del pred\n",
    "        return loss_c_accum"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| learnable_params: 395086\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-14T09:29:04.143883Z",
     "start_time": "2024-05-14T09:27:04.851233Z"
    }
   },
   "source": [
    "train_metric = test(train_loader, \"train\")\n",
    "val_metric = test(val_loader, \"val\")\n",
    "test_metric = test(test_loader, \"test\")\n",
    "ic( \n",
    "    train_metric, \n",
    "    val_metric, \n",
    "    test_metric\n",
    ")\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train_loss = train(epoch)\n",
    "    train_metric = test(train_loader, \"train\")\n",
    "    val_metric = test(val_loader, \"val\")\n",
    "    test_metric = test(test_loader, \"test\")\n",
    "    ic(\n",
    "        train_loss, \n",
    "        train_metric, \n",
    "        val_metric, \n",
    "        test_metric\n",
    "    )"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 32/32 [00:04<00:00,  7.31it/s, accuracy=0.3163, loss=4.9338, loss_c=2.4792, loss_n=0.0684, loss_vector=2.3342]\n",
      "Evaluating: 100%|██████████| 10/10 [00:01<00:00,  7.61it/s, accuracy=0.3294, loss=4.9188, loss_c=2.4792, loss_n=0.0685, loss_vector=2.3356]\n",
      "Evaluating: 100%|██████████| 9/9 [00:01<00:00,  7.75it/s, accuracy=0.3270, loss=4.8299, loss_c=2.5055, loss_n=0.0675, loss_vector=2.3309]\n",
      "ic| train_metric: tensor(94676.5391)\n",
      "    val_metric: tensor(28272.2715)\n",
      "    test_metric: tensor(25493.4590)\n",
      "Epoch 1: 100%|██████████| 32/32 [00:08<00:00,  3.72it/s, loss=4.6729, loss_c=2.3611, loss_n=0.0651]\n",
      "Evaluating: 100%|██████████| 32/32 [00:04<00:00,  7.14it/s, accuracy=0.6272, loss=4.4652, loss_c=2.2145, loss_n=0.0339, loss_vector=2.1644]\n",
      "Evaluating: 100%|██████████| 10/10 [00:01<00:00,  7.45it/s, accuracy=0.6198, loss=4.3978, loss_c=2.2159, loss_n=0.0318, loss_vector=2.1737]\n",
      "Evaluating: 100%|██████████| 9/9 [00:01<00:00,  7.45it/s, accuracy=0.6035, loss=4.4202, loss_c=2.2179, loss_n=0.0319, loss_vector=2.1822]\n",
      "ic| train_loss: tensor(1.7051, grad_fn=<AddBackward0>)\n",
      "    train_metric: tensor(84568.5000)\n",
      "    val_metric: tensor(25270.3398)\n",
      "    test_metric: tensor(22567.5625)\n",
      "Epoch 2: 100%|██████████| 32/32 [00:08<00:00,  3.75it/s, loss=4.3565, loss_c=2.1728, loss_n=0.0388]\n",
      "Evaluating: 100%|██████████| 32/32 [00:04<00:00,  7.36it/s, accuracy=0.7066, loss=4.2481, loss_c=2.0857, loss_n=0.0194, loss_vector=2.0955]\n",
      "Evaluating: 100%|██████████| 10/10 [00:01<00:00,  7.57it/s, accuracy=0.7205, loss=4.1211, loss_c=2.0768, loss_n=0.0167, loss_vector=2.0897]\n",
      "Evaluating: 100%|██████████| 9/9 [00:01<00:00,  7.77it/s, accuracy=0.6983, loss=4.2069, loss_c=2.0745, loss_n=0.0170, loss_vector=2.1022]\n",
      "ic| train_loss: tensor(1.5631, grad_fn=<AddBackward0>)\n",
      "    train_metric: tensor(79649.9297)\n",
      "    val_metric: tensor(23684.1387)\n",
      "    test_metric: tensor(21107.9531)\n",
      "Epoch 3: 100%|██████████| 32/32 [00:08<00:00,  3.77it/s, loss=4.1853, loss_c=2.0769, loss_n=0.0306]\n",
      "Evaluating: 100%|██████████| 32/32 [00:04<00:00,  7.32it/s, accuracy=0.7120, loss=3.9866, loss_c=2.0152, loss_n=0.0167, loss_vector=2.0367]\n",
      "Evaluating: 100%|██████████| 10/10 [00:01<00:00,  7.56it/s, accuracy=0.7205, loss=3.9602, loss_c=1.9966, loss_n=0.0140, loss_vector=2.0299]\n",
      "Evaluating: 100%|██████████| 9/9 [00:01<00:00,  7.69it/s, accuracy=0.6992, loss=4.0847, loss_c=1.9961, loss_n=0.0142, loss_vector=2.0476]\n",
      "ic| train_loss: tensor(1.4923, grad_fn=<AddBackward0>)\n",
      "    train_metric: tensor(76958.1094)\n",
      "    val_metric: tensor(22769.3223)\n",
      "    test_metric: tensor(20310.7695)\n",
      "Epoch 4: 100%|██████████| 32/32 [00:08<00:00,  3.93it/s, loss=4.0669, loss_c=2.0169, loss_n=0.0267]\n",
      "Evaluating: 100%|██████████| 32/32 [00:04<00:00,  7.31it/s, accuracy=0.7112, loss=4.0009, loss_c=1.9792, loss_n=0.0164, loss_vector=1.9891]\n",
      "Evaluating: 100%|██████████| 10/10 [00:01<00:00,  7.67it/s, accuracy=0.7205, loss=3.8626, loss_c=1.9579, loss_n=0.0137, loss_vector=1.9818]\n",
      "Evaluating: 100%|██████████| 9/9 [00:01<00:00,  7.67it/s, accuracy=0.6992, loss=4.0114, loss_c=1.9596, loss_n=0.0139, loss_vector=2.0007]\n",
      "ic| train_loss: tensor(1.4483, grad_fn=<AddBackward0>)\n",
      "    train_metric: tensor(75584.7266)\n",
      "    val_metric: tensor(22327.3828)\n",
      "    test_metric: tensor(19938.6250)\n",
      "Epoch 5: 100%|██████████| 32/32 [00:08<00:00,  3.89it/s, loss=3.9866, loss_c=1.9830, loss_n=0.0236]\n",
      "Evaluating: 100%|██████████| 32/32 [00:04<00:00,  7.32it/s, accuracy=0.7273, loss=3.8986, loss_c=1.9612, loss_n=0.0162, loss_vector=1.9404]\n",
      "Evaluating: 100%|██████████| 10/10 [00:01<00:00,  7.56it/s, accuracy=0.7205, loss=3.8044, loss_c=1.9381, loss_n=0.0135, loss_vector=1.9450]\n",
      "Evaluating: 100%|██████████| 9/9 [00:01<00:00,  7.67it/s, accuracy=0.6992, loss=3.9654, loss_c=1.9416, loss_n=0.0137, loss_vector=1.9636]\n",
      "ic| train_loss: tensor(1.4232, grad_fn=<AddBackward0>)\n",
      "    train_metric: tensor(74896.4453)\n",
      "    val_metric: tensor(22102.2207)\n",
      "    test_metric: tensor(19755.9922)\n",
      "Epoch 6: 100%|██████████| 32/32 [00:08<00:00,  3.93it/s, loss=3.9354, loss_c=1.9682, loss_n=0.0212]\n",
      "Evaluating: 100%|██████████| 32/32 [00:04<00:00,  7.31it/s, accuracy=0.7296, loss=3.9516, loss_c=1.9538, loss_n=0.0161, loss_vector=1.9094]\n",
      "Evaluating: 100%|██████████| 10/10 [00:01<00:00,  7.62it/s, accuracy=0.7205, loss=3.7623, loss_c=1.9269, loss_n=0.0135, loss_vector=1.9165]\n",
      "Evaluating: 100%|██████████| 9/9 [00:01<00:00,  7.71it/s, accuracy=0.6992, loss=3.9346, loss_c=1.9317, loss_n=0.0137, loss_vector=1.9351]\n",
      "ic| train_loss: tensor(1.4119, grad_fn=<AddBackward0>)\n",
      "    train_metric: tensor(74612.9453)\n",
      "    val_metric: tensor(21974.7461)\n",
      "    test_metric: tensor(19654.8477)\n",
      "Epoch 7: 100%|██████████| 32/32 [00:08<00:00,  3.85it/s, loss=3.8992, loss_c=1.9589, loss_n=0.0201]\n",
      "Evaluating: 100%|██████████| 32/32 [00:04<00:00,  7.26it/s, accuracy=0.7166, loss=3.8507, loss_c=1.9504, loss_n=0.0159, loss_vector=1.8983]\n",
      "Evaluating: 100%|██████████| 10/10 [00:01<00:00,  7.53it/s, accuracy=0.7205, loss=3.7410, loss_c=1.9264, loss_n=0.0132, loss_vector=1.8955]\n",
      "Evaluating: 100%|██████████| 9/9 [00:01<00:00,  7.66it/s, accuracy=0.6992, loss=3.9182, loss_c=1.9317, loss_n=0.0134, loss_vector=1.9135]\n",
      "ic| train_loss: tensor(1.4050, grad_fn=<AddBackward0>)\n",
      "    train_metric: tensor(74485.5391)\n",
      "    val_metric: tensor(21969.1914)\n",
      "    test_metric: tensor(19655.3145)\n",
      "Epoch 8:  53%|█████▎    | 17/32 [00:04<00:04,  3.57it/s, loss=3.8857, loss_c=1.9619, loss_n=0.0195]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[31], line 10\u001B[0m\n\u001B[1;32m      4\u001B[0m ic( \n\u001B[1;32m      5\u001B[0m     train_metric, \n\u001B[1;32m      6\u001B[0m     val_metric, \n\u001B[1;32m      7\u001B[0m     test_metric\n\u001B[1;32m      8\u001B[0m )\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m1\u001B[39m, epochs \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m):\n\u001B[0;32m---> 10\u001B[0m     train_loss \u001B[38;5;241m=\u001B[39m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mepoch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     11\u001B[0m     train_metric \u001B[38;5;241m=\u001B[39m test(train_loader, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     12\u001B[0m     val_metric \u001B[38;5;241m=\u001B[39m test(val_loader, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mval\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[0;32mIn[30], line 62\u001B[0m, in \u001B[0;36mtrain\u001B[0;34m(epoc)\u001B[0m\n\u001B[1;32m     60\u001B[0m loss, vector_loss, loss_c, loss_n \u001B[38;5;241m=\u001B[39m ssloss\u001B[38;5;241m.\u001B[39mmv_loss(pred, tf\u001B[38;5;241m.\u001B[39my)\n\u001B[1;32m     61\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m---> 62\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     63\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m     65\u001B[0m loss_accum \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28mfloat\u001B[39m(loss) \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mlen\u001B[39m(tf\u001B[38;5;241m.\u001B[39my)\n",
      "File \u001B[0;32m~/miniconda3/envs/rel-mm/lib/python3.10/site-packages/torch/_tensor.py:525\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    515\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    517\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    518\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    523\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[1;32m    524\u001B[0m     )\n\u001B[0;32m--> 525\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    526\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[1;32m    527\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/rel-mm/lib/python3.10/site-packages/torch/autograd/__init__.py:267\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    262\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    264\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[1;32m    265\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    266\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 267\u001B[0m \u001B[43m_engine_run_backward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    268\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    269\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    270\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    271\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    272\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    273\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    274\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    275\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/rel-mm/lib/python3.10/site-packages/torch/autograd/graph.py:744\u001B[0m, in \u001B[0;36m_engine_run_backward\u001B[0;34m(t_outputs, *args, **kwargs)\u001B[0m\n\u001B[1;32m    742\u001B[0m     unregister_hooks \u001B[38;5;241m=\u001B[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[1;32m    743\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 744\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[1;32m    745\u001B[0m \u001B[43m        \u001B[49m\u001B[43mt_outputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[1;32m    746\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[1;32m    747\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    748\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-14T08:07:14.781602Z",
     "start_time": "2024-05-14T08:07:14.715641Z"
    }
   },
   "source": "# wandb.finish()",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rel-mm",
   "language": "python",
   "name": "rel-mm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
