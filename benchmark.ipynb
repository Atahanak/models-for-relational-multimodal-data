{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-19 10:30:29 - DEBUG - Popen(['git', 'version'], cwd=/home/takyildiz/dev/models-for-relational-multimodal-data, stdin=None, shell=False, universal_newlines=False)\n",
      "2024-07-19 10:30:29 - DEBUG - Popen(['git', 'version'], cwd=/home/takyildiz/dev/models-for-relational-multimodal-data, stdin=None, shell=False, universal_newlines=False)\n",
      "2024-07-19 10:30:29 - DEBUG - Trying paths: ['/home/takyildiz/.docker/config.json', '/home/takyildiz/.dockercfg']\n",
      "2024-07-19 10:30:29 - DEBUG - No config file found\n",
      "2024-07-19 10:30:29 - DEBUG - [Tracing] Create new propagation context: {'trace_id': '448843671af545ffb1c7a87eb36cd01d', 'span_id': 'b931657646c07090', 'parent_span_id': None, 'dynamic_sampling_context': None}\n",
      "2024-07-19 10:30:29 - ERROR - Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "2024-07-19 10:30:30 - DEBUG - Starting new HTTPS connection (1): api.wandb.ai:443\n",
      "2024-07-19 10:30:30 - DEBUG - https://api.wandb.ai:443 \"POST /graphql HTTP/1.1\" 200 1918\n",
      "2024-07-19 10:30:30 - DEBUG - https://api.wandb.ai:443 \"POST /graphql HTTP/1.1\" 200 390\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maakyildiz\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "2024-07-19 10:30:30 - DEBUG - no default config file found in config-defaults.yaml\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from src.datasets.util.mask import PretrainType\n",
    "from torch_frame.data import DataLoader\n",
    "from torch_frame import stype\n",
    "from torch_frame.nn import (\n",
    "    EmbeddingEncoder,\n",
    "    LinearEncoder,\n",
    "    TimestampEncoder,\n",
    ")\n",
    "from torch_frame.nn.encoder.stypewise_encoder import StypeWiseFeatureEncoder\n",
    "from torch_frame import TensorFrame\n",
    "from torch_frame.data.stats import StatType\n",
    "\n",
    "from torch_geometric.utils import degree\n",
    "\n",
    "from src.datasets import IBMTransactionsAML\n",
    "from src.nn.models import TABGNN\n",
    "from src.nn.decoder import MCMHead\n",
    "from src.nn.gnn.decoder import LinkPredHead\n",
    "from src.utils.loss import SSLoss\n",
    "from src.utils.metric import SSMetric\n",
    "from src.nn.weighting.MoCo import MoCoLoss\n",
    "from src.primitives import negative_sampling\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import wandb\n",
    "\n",
    "import logging\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,  # Set the logging level\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',  # Specify the log message format\n",
    "    datefmt='%Y-%m-%d %H:%M:%S',  # Specify the date format\n",
    "    handlers=[\n",
    "        #logging.FileHandler('app.log'),  # Log messages to a file\n",
    "        logging.StreamHandler()  # Also output log messages to the console\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create a logger\n",
    "logger = logging.getLogger(__name__)\n",
    "import sys\n",
    "import time\n",
    "\n",
    "torch.set_num_threads(4)\n",
    "\n",
    "seed = 42\n",
    "batch_size = 200\n",
    "lr = 2e-4\n",
    "eps = 1e-8\n",
    "weight_decay = 1e-3\n",
    "epochs = 50\n",
    "\n",
    "compile = False\n",
    "data_split = [0.6, 0.2, 0.2]\n",
    "split_type = 'temporal'\n",
    "\n",
    "khop_neighbors = [100, 100]\n",
    "pos_sample_prob = 1\n",
    "num_neg_samples = 64\n",
    "channels = 128\n",
    "num_layers = 3\n",
    "dropout = 0.5\n",
    "\n",
    "pretrain = {PretrainType.MASK, PretrainType.LINK_PRED}\n",
    "#pretrain = {PretrainType.LINK_PRED}\n",
    "\n",
    "testing = True\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "args = {\n",
    "    'testing': testing,\n",
    "    'batch_size': batch_size,\n",
    "    'seed': seed,\n",
    "    'device': device,\n",
    "    'lr': lr,\n",
    "    'eps': eps,\n",
    "    'epochs': epochs,\n",
    "    'compile': compile,\n",
    "    'data_split': data_split,\n",
    "    'pos_sample_prob': pos_sample_prob,\n",
    "    'channels': channels,\n",
    "    'split_type': split_type,\n",
    "    'num_neg_samples': num_neg_samples,\n",
    "    'pretrain': pretrain,\n",
    "    'khop_neighbors': khop_neighbors,\n",
    "    'num_layers': num_layers,\n",
    "    'dropout': dropout,\n",
    "    'weight_decay': weight_decay,\n",
    "}\n",
    "\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "# When running on the CuDNN backend, two further options must be set\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "# Set a fixed value for the hash seed\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "wandb.login()\n",
    "run = wandb.init(\n",
    "    dir=\"/mnt/data/\",\n",
    "    #dir=\"/scratch/takyildiz/\",\n",
    "    mode=\"disabled\" if args['testing'] else \"online\",\n",
    "    #mode=\"disabled\" if args['testing'] else \"disabled\",\n",
    "    project=f\"rel-mm-fix\", \n",
    "    name=f\"tabgnn,tab=3,noemb,lp\",\n",
    "    #group=f\"new,mcm\",\n",
    "    entity=\"cse3000\",\n",
    "    #name=f\"debug-fused\",\n",
    "    config=args\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-19 10:30:33 - INFO - Creating graph...\n",
      "2024-07-19 10:30:47 - INFO - Graph created in 13.41337513923645 seconds.\n",
      "2024-07-19 10:30:47 - INFO - Applying mask...\n",
      "2024-07-19 10:30:47 - INFO - Loading masked columns from /mnt/data/ibm-transactions-for-anti-money-laundering-aml/HI-Small_Trans-c.csv.npy\n",
      "2024-07-19 10:31:40 - INFO - Mask applied in 53.353737115859985 seconds.\n"
     ]
    }
   ],
   "source": [
    "dataset = IBMTransactionsAML(\n",
    "    root='/mnt/data/ibm-transactions-for-anti-money-laundering-aml/HI-Small_Trans-c.csv',# if not testing else '/mnt/data/ibm-transactions-for-anti-money-laundering-aml/dummy-c.csv', \n",
    "    #root='/mnt/data/ibm-transactions-for-anti-money-laundering-aml/HI-Medium_Trans-c.csv',# if not testing else '/mnt/data/ibm-transactions-for-anti-money-laundering-aml/dummy-c.csv', \n",
    "    #root='/scratch/takyildiz/ibm-transactions-for-anti-money-laundering-aml/HI-Small_Trans-c.csv' if not testing else '/scratch/takyildiz/ibm-transactions-for-anti-money-laundering-aml/dummy-c.csv', \n",
    "    pretrain=pretrain,\n",
    "    mask_type=\"replace\",\n",
    "    split_type=split_type, \n",
    "    splits=data_split, \n",
    "    khop_neighbors=khop_neighbors\n",
    ")\n",
    "dataset.materialize()\n",
    "train_dataset, val_dataset, test_dataset = dataset.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-19 10:32:21 - INFO - num_numerical: 1\n",
      "2024-07-19 10:32:21 - INFO - num_categorical: 5\n",
      "2024-07-19 10:32:21 - INFO - num_columns: 7\n"
     ]
    }
   ],
   "source": [
    "batch_size = 200\n",
    "tensor_frame = dataset.tensor_frame\n",
    "train_loader = DataLoader(train_dataset.tensor_frame, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset.tensor_frame, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset.tensor_frame, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "num_numerical = len(dataset.tensor_frame.col_names_dict[stype.numerical])\n",
    "num_categorical = len(dataset.tensor_frame.col_names_dict[stype.categorical])\n",
    "\n",
    "num_columns = num_numerical + num_categorical + 1\n",
    "logger.info(f\"num_numerical: {num_numerical}\")\n",
    "logger.info(f\"num_categorical: {num_categorical}\")\n",
    "logger.info(f\"num_columns: {num_columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-19 10:32:22 - INFO - model_params: 7451904\n",
      "2024-07-19 10:32:22 - INFO - encoder_params: 5936384\n",
      "2024-07-19 10:32:22 - INFO - lp_decoder_params: 52531\n",
      "2024-07-19 10:32:22 - INFO - mcm_decoder_params: 17837423\n",
      "2024-07-19 10:32:22 - INFO - learnable_params: 31278242\n"
     ]
    }
   ],
   "source": [
    "def train_mcm(dataset, loader, epoc: int, encoder, model, mcm_decoder, optimizer, scheduler) -> float:\n",
    "    model.train()\n",
    "    loss_accum = total_count = 0\n",
    "    loss_accum = loss_lp_accum = loss_c_accum = loss_n_accum = total_count = t_c = t_n = 0\n",
    "\n",
    "    with tqdm(loader, desc=f'Epoch {epoc}') as t:\n",
    "        for tf in t:\n",
    "            node_feats, edge_index, edge_attr, target_edge_index, target_edge_attr = mcm_inputs(tf, dataset)\n",
    "            tf = tf.to(device)\n",
    "            edge_attr, _ = encoder(edge_attr)\n",
    "            target_edge_attr, _ = encoder(target_edge_attr)\n",
    "            x, edge_attr, target_edge_attr = model(node_feats, edge_index, edge_attr, target_edge_attr)\n",
    "            x_target = x[target_edge_index.T].reshape(-1, 2 * channels)#.relu()\n",
    "            x_target = torch.cat((x_target, target_edge_attr), 1)\n",
    "            num_pred, cat_pred = mcm_decoder(x_target)\n",
    "            num_pred = num_pred.cpu()\n",
    "            cat_pred = [x.cpu() for x in cat_pred]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            t_loss, loss_c, loss_n = ssloss.mcm_loss(cat_pred, num_pred, tf.y)\n",
    "            t_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_accum += (t_loss.item() * len(tf.y))\n",
    "            total_count += len(tf.y)\n",
    "            t_c += loss_c[1]\n",
    "            t_n += loss_n[1]\n",
    "            loss_c_accum += loss_c[0].item()\n",
    "            loss_n_accum += loss_n[0].item()\n",
    "            t.set_postfix(loss=f'{loss_accum/total_count:.4f}', loss_c=f'{loss_c_accum/t_c:.4f}', loss_n=f'{loss_n_accum/t_n:.4f}')\n",
    "            wandb.log({\"train_loss\": loss_accum/total_count, \"train_loss_c\": loss_c_accum/t_c, \"train_loss_n\": loss_n_accum/t_n})\n",
    "    return {'loss': loss_accum / total_count}\n",
    "\n",
    "def train_lp(dataset, loader, epoc: int, encoder, model, lp_decoder, optimizer, scheduler) -> float:\n",
    "    encoder.train()\n",
    "    model.train()\n",
    "    lp_decoder.train()\n",
    "    total_count = 0\n",
    "    loss_lp_accum = 0\n",
    "\n",
    "    with tqdm(loader, desc=f'Epoch {epoc}') as t:\n",
    "        for tf in t:\n",
    "            batch_size = len(tf.y)\n",
    "            node_feats, edge_index, edge_attr, target_edge_index, target_edge_attr = lp_inputs(tf, dataset)\n",
    "            tf = tf.to(device)\n",
    "            edge_attr, _ = encoder(edge_attr)\n",
    "            target_edge_attr, _ = encoder(target_edge_attr)\n",
    "            x_gnn, edge_attr, target_edge_attr = model(node_feats, edge_index, edge_attr, target_edge_attr)\n",
    "            pos_edge_index = target_edge_index[:, :batch_size]\n",
    "            neg_edge_index = target_edge_index[:, batch_size:]\n",
    "            pos_edge_attr = target_edge_attr[:batch_size,:]\n",
    "            neg_edge_attr = target_edge_attr[batch_size:,:]\n",
    "            pos_pred, neg_pred = lp_decoder(x_gnn, pos_edge_index, pos_edge_attr, neg_edge_index, neg_edge_attr)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            link_loss = ssloss.lp_loss(pos_pred, neg_pred)\n",
    "            link_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_count += len(tf.y)\n",
    "            loss_lp_accum += link_loss.item() * len(tf.y)\n",
    "            t.set_postfix(loss_lp=f'{loss_lp_accum/total_count:.4f}')\n",
    "            # wandb.log({\"train_loss_lp\": loss_lp_accum/total_count})\n",
    "            #wandb.log({\"lr\": scheduler.get_last_lr()[0]})\n",
    "    return {'loss': loss_lp_accum / total_count} \n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_mcm(dataset, loader: DataLoader, encoder, model, mcm_decoder, dataset_name) -> float:\n",
    "    encoder.eval()\n",
    "    model.eval()\n",
    "    mcm_decoder.eval()\n",
    "    total_count = 0\n",
    "    accum_acc = accum_l2 = 0\n",
    "    loss_c_accum = loss_n_accum = total_count = t_c = t_n = 0\n",
    "    t_n = t_c = 0\n",
    "    with tqdm(loader, desc=f'Evaluating') as t:\n",
    "        for tf in t:\n",
    "            node_feats, edge_index, edge_attr, target_edge_index, target_edge_attr = mcm_inputs(tf, dataset)\n",
    "            tf = tf.to(device)\n",
    "            edge_attr, _ = encoder(edge_attr)\n",
    "            target_edge_attr, _ = encoder(target_edge_attr)\n",
    "            x, edge_attr, target_edge_attr = model(node_feats, edge_index, edge_attr, target_edge_attr)\n",
    "            x_target = x[target_edge_index.T].reshape(-1, 2 * channels)#.relu()\n",
    "            x_target = torch.cat((x_target, target_edge_attr), 1)\n",
    "            num_pred, cat_pred = mcm_decoder(x_target)\n",
    "            num_pred = num_pred.cpu()\n",
    "            cat_pred = [x.cpu() for x in cat_pred]\n",
    "            _, loss_c, loss_n = ssloss.mcm_loss(cat_pred, num_pred, tf.y)\n",
    "            t_c += loss_c[1]\n",
    "            t_n += loss_n[1] \n",
    "            loss_c_accum += loss_c[0].item()\n",
    "            loss_n_accum += loss_n[0].item()\n",
    "            total_count += len(num_pred)\n",
    "            for i, ans in enumerate(tf.y):\n",
    "                if ans[1] > (num_numerical-1):\n",
    "                    accum_acc += (cat_pred[int(ans[1])-num_numerical][i].argmax() == int(ans[0]))\n",
    "                else:\n",
    "                    accum_l2 += torch.square(ans[0] - num_pred[i][int(ans[1])]) #rmse\n",
    "            t.set_postfix(\n",
    "                accuracy=f'{accum_acc / t_c:.4f}',\n",
    "                rmse=f'{torch.sqrt(accum_l2 / t_n):.4f}', \n",
    "                loss_mcm=f'{(loss_c_accum/t_c) + (loss_n_accum/t_n):.4f}',\n",
    "                loss_c = f'{loss_c_accum/t_c:.4f}', \n",
    "                loss_n = f'{loss_n_accum/t_n:.4f}',\n",
    "            )\n",
    "            wandb.log({\n",
    "                f\"{dataset_name}_loss_mcm\": (loss_c_accum/t_c) + (loss_n_accum/t_n),\n",
    "                f\"{dataset_name}_loss_c\": loss_c_accum/t_c,\n",
    "                f\"{dataset_name}_loss_n\": loss_n_accum/t_n,\n",
    "            })\n",
    "        accuracy = accum_acc / t_c\n",
    "        rmse = torch.sqrt(accum_l2 / t_n)\n",
    "        wandb.log({\n",
    "            f\"{dataset_name}_accuracy\": accuracy,\n",
    "            f\"{dataset_name}_rmse\": rmse,\n",
    "        })\n",
    "        return {\"accuracy\": accuracy, \"rmse\": rmse}\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_lp(dataset, loader: DataLoader, encoder, model, lp_decoder, dataset_name) -> float:\n",
    "    encoder.eval()\n",
    "    model.eval()\n",
    "    lp_decoder.eval()\n",
    "    mrrs = []\n",
    "    hits1 = []\n",
    "    hits2 = []\n",
    "    hits5 = []\n",
    "    hits10 = []\n",
    "    loss_accum = 0\n",
    "    total_count = 0\n",
    "    loss_accum = loss_lp_accum = total_count = 0\n",
    "    with tqdm(loader, desc=f'Evaluating') as t:\n",
    "        for tf in t:\n",
    "            batch_size = len(tf.y)\n",
    "            node_feats, input_edge_index, input_edge_attr, target_edge_index, target_edge_attr = lp_inputs(tf, dataset)\n",
    "            tf = tf.to(device)\n",
    "            input_edge_attr, _ = encoder(input_edge_attr)\n",
    "            target_edge_attr, _ = encoder(target_edge_attr)\n",
    "            x_gnn, edge_attr, target_edge_attr = model(node_feats, input_edge_index, input_edge_attr, target_edge_attr)\n",
    "            pos_edge_index = target_edge_index[:, :batch_size]\n",
    "            neg_edge_index = target_edge_index[:, batch_size:]\n",
    "            pos_edge_attr = target_edge_attr[:batch_size,:]\n",
    "            neg_edge_attr = target_edge_attr[batch_size:,:]\n",
    "\n",
    "            pos_pred, neg_pred = lp_decoder(x_gnn, pos_edge_index, pos_edge_attr, neg_edge_index, neg_edge_attr)\n",
    "            loss = ssloss.lp_loss(pos_pred, neg_pred)\n",
    "            \n",
    "            loss_lp_accum += loss * len(pos_pred)\n",
    "            loss_accum += float(loss) * len(pos_pred)\n",
    "            total_count += len(pos_pred)\n",
    "            mrr_score, hits = ssmetric.mrr(pos_pred, neg_pred, [1,2,5,10], num_neg_samples)\n",
    "            mrrs.append(mrr_score)\n",
    "            hits1.append(hits['hits@1'])\n",
    "            hits2.append(hits['hits@2'])\n",
    "            hits5.append(hits['hits@5'])\n",
    "            hits10.append(hits['hits@10'])\n",
    "            t.set_postfix(\n",
    "                mrr=f'{np.mean(mrrs):.4f}',\n",
    "                hits1=f'{np.mean(hits1):.4f}',\n",
    "                hits2=f'{np.mean(hits2):.4f}',\n",
    "                hits5=f'{np.mean(hits5):.4f}',\n",
    "                hits10=f'{np.mean(hits10):.4f}',\n",
    "                loss_lp = f'{loss_lp_accum/total_count:.4f}',\n",
    "            )\n",
    "        wandb.log({\n",
    "            f\"{dataset_name}_loss_lp\": loss_lp_accum/total_count,\n",
    "        })\n",
    "        mrr_score = np.mean(mrrs)\n",
    "        hits1 = np.mean(hits1)\n",
    "        hits2 = np.mean(hits2)\n",
    "        hits5 = np.mean(hits5)\n",
    "        hits10 = np.mean(hits10)\n",
    "        wandb.log({\n",
    "            f\"{dataset_name}_mrr\": mrr_score,\n",
    "            f\"{dataset_name}_hits@1\": hits1,\n",
    "            f\"{dataset_name}_hits@2\": hits2,\n",
    "            f\"{dataset_name}_hits@5\": hits5,\n",
    "            f\"{dataset_name}_hits@10\": hits10,\n",
    "        })\n",
    "        return {\"mrr\": mrr_score, \"hits@1\": hits1, \"hits@2\": hits2, \"hits@5\": hits5, \"hits@10\": hits10}\n",
    "\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval(dataset, loader, encoder, model, lp_decoder, mcm_decoder, dataset_name):\n",
    "    encoder.eval()\n",
    "    model.eval()\n",
    "    lp_decoder.eval()\n",
    "    mcm_decoder.eval()\n",
    "    mrrs = []\n",
    "    hits1 = []\n",
    "    hits2 = []\n",
    "    hits5 = []\n",
    "    hits10 = []\n",
    "    loss_accum = 0\n",
    "    total_count = 0\n",
    "    loss_accum = loss_lp_accum = total_count = 0\n",
    "    accum_acc = accum_l2 = 0\n",
    "    loss_c_accum = loss_n_accum = t_c = t_n = 0\n",
    "    with tqdm(loader, desc=f'Evaluating') as t:\n",
    "        for tf in t:\n",
    "            batch_size = len(tf.y)\n",
    "            node_feats, input_edge_index, input_edge_attr, target_edge_index, target_edge_attr = lp_inputs(tf, dataset)\n",
    "            input_edge_attr, _ = encoder(input_edge_attr)\n",
    "            target_edge_attr, _ = encoder(target_edge_attr)\n",
    "            x_gnn, edge_attr, target_edge_attr = model(node_feats, input_edge_index, input_edge_attr, target_edge_attr)\n",
    "            pos_edge_index = target_edge_index[:, :batch_size]\n",
    "            neg_edge_index = target_edge_index[:, batch_size:]\n",
    "            pos_edge_attr = target_edge_attr[:batch_size,:]\n",
    "            neg_edge_attr = target_edge_attr[batch_size:,:]\n",
    "            pos_pred, neg_pred = lp_decoder(x_gnn, pos_edge_index, pos_edge_attr, neg_edge_index, neg_edge_attr)\n",
    "\n",
    "            x_target = x_gnn[pos_edge_index.T].reshape(-1, 2 * channels)#.relu()\n",
    "            x_target = torch.cat((x_target, pos_edge_attr), 1)\n",
    "            num_pred, cat_pred = mcm_decoder(x_target)\n",
    "            num_pred = num_pred.cpu()\n",
    "            cat_pred = [x.cpu() for x in cat_pred] \n",
    "\n",
    "            link_loss = ssloss.lp_loss(pos_pred, neg_pred)\n",
    "            t_loss, loss_c, loss_n = ssloss.mcm_loss(cat_pred, num_pred, tf.y)\n",
    "            moco_loss = mocoloss.loss([link_loss, t_loss])\n",
    "\n",
    "            loss_accum += ((link_loss.item()*moco_loss[0]+(t_loss.item()*moco_loss[1])) * len(tf.y))\n",
    "            total_count += len(tf.y)\n",
    "            t_c += loss_c[1]\n",
    "            t_n += loss_n[1]\n",
    "            loss_c_accum += loss_c[0].item()\n",
    "            loss_n_accum += loss_n[0].item()\n",
    "            loss_lp_accum += link_loss.item() * len(tf.y)\n",
    "            mrr_score, hits = ssmetric.mrr(pos_pred, neg_pred, [1,2,5,10], num_neg_samples)\n",
    "            mrrs.append(mrr_score)\n",
    "            hits1.append(hits['hits@1'])\n",
    "            hits2.append(hits['hits@2'])\n",
    "            hits5.append(hits['hits@5'])\n",
    "            hits10.append(hits['hits@10'])\n",
    "\n",
    "            for i, ans in enumerate(tf.y):\n",
    "                if ans[1] > (num_numerical-1):\n",
    "                    accum_acc += (cat_pred[int(ans[1])-num_numerical][i].argmax() == int(ans[0]))\n",
    "                else:\n",
    "                    accum_l2 += torch.square(ans[0] - num_pred[i][int(ans[1])]) #rmse\n",
    "\n",
    "            t.set_postfix(\n",
    "                mrr=f'{np.mean(mrrs):.4f}',\n",
    "                hits1=f'{np.mean(hits1):.4f}',\n",
    "                hits2=f'{np.mean(hits2):.4f}',\n",
    "                hits5=f'{np.mean(hits5):.4f}',\n",
    "                hits10=f'{np.mean(hits10):.4f}',\n",
    "                loss_lp = f'{loss_lp_accum/total_count:.4f}',\n",
    "                accuracy=f'{accum_acc / t_c:.4f}',\n",
    "                rmse=f'{torch.sqrt(accum_l2 / t_n):.4f}', \n",
    "                loss_mcm=f'{(loss_c_accum/t_c) + (loss_n_accum/t_n):.4f}',\n",
    "                loss_c = f'{loss_c_accum/t_c:.4f}', \n",
    "                loss_n = f'{loss_n_accum/t_n:.4f}'\n",
    "            )\n",
    "        mrr_score = np.mean(mrrs)\n",
    "        hits1 = np.mean(hits1)\n",
    "        hits2 = np.mean(hits2)\n",
    "        hits5 = np.mean(hits5)\n",
    "        hits10 = np.mean(hits10)\n",
    "        accuracy = accum_acc / t_c\n",
    "        rmse = torch.sqrt(accum_l2 / t_n)\n",
    "        wandb.log({\n",
    "            f\"{dataset_name}_loss_mcm\": (loss_c_accum/t_c) + (loss_n_accum/t_n),\n",
    "            f\"{dataset_name}_loss_c\": loss_c_accum/t_c,\n",
    "            f\"{dataset_name}_loss_n\": loss_n_accum/t_n,\n",
    "            f\"{dataset_name}_loss_lp\": loss_lp_accum/total_count,\n",
    "            f\"{dataset_name}_mrr\": mrr_score,\n",
    "            f\"{dataset_name}_hits@1\": hits1,\n",
    "            f\"{dataset_name}_hits@2\": hits2,\n",
    "            f\"{dataset_name}_hits@5\": hits5,\n",
    "            f\"{dataset_name}_hits@10\": hits10,\n",
    "            f\"{dataset_name}_accuracy\": accuracy,\n",
    "            f\"{dataset_name}_rmse\": rmse,\n",
    "        })\n",
    "        return {\"mrr\": mrr_score, \"hits@1\": hits1, \"hits@2\": hits2, \"hits@5\": hits5, \"hits@10\": hits10}, {\"accuracy\": accuracy, \"rmse\": rmse}\n",
    "\n",
    "num_categorical = [len(dataset.col_stats[col][StatType.COUNT][0]) for col in dataset.tensor_frame.col_names_dict[stype.categorical]] if stype.categorical in dataset.tensor_frame.col_names_dict else 0\n",
    "mcm_decoder = MCMHead(channels, num_numerical, num_categorical, w=3).to(device)\n",
    "lp_decoder = LinkPredHead(n_classes=1, n_hidden=channels, dropout=dropout).to(device)\n",
    "\n",
    "stype_encoder_dict = {\n",
    "    stype.categorical: EmbeddingEncoder(),\n",
    "    stype.numerical: LinearEncoder(),\n",
    "    stype.timestamp: TimestampEncoder(),\n",
    "}\n",
    "encoder = StypeWiseFeatureEncoder(\n",
    "            out_channels=channels,\n",
    "            col_stats=dataset.col_stats,\n",
    "            col_names_dict=dataset.tensor_frame.col_names_dict,\n",
    "            stype_encoder_dict=stype_encoder_dict,\n",
    ").to(device)\n",
    "\n",
    "edge_index = dataset.train_graph.edge_index\n",
    "num_nodes = dataset.train_graph.num_nodes\n",
    "in_degrees = degree(edge_index[1], num_nodes=num_nodes, dtype=torch.long)\n",
    "max_in_degree = int(in_degrees.max())\n",
    "in_degree_histogram = torch.zeros(max_in_degree + 1, dtype=torch.long)\n",
    "in_degree_histogram += torch.bincount(in_degrees, minlength=in_degree_histogram.numel())\n",
    "model = TABGNN(\n",
    "    encoder=encoder,\n",
    "    channels=channels,\n",
    "    edge_dim=channels*dataset.tensor_frame.num_cols,\n",
    "    num_layers=num_layers, \n",
    "    dropout=dropout,\n",
    "    deg=in_degree_histogram\n",
    ")\n",
    "model = torch.compile(model, dynamic=True) if compile else model\n",
    "model.to(device)\n",
    "\n",
    "model_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "encoder_params = sum(p.numel() for p in encoder.parameters() if p.requires_grad)\n",
    "lp_decoder_params = sum(p.numel() for p in lp_decoder.parameters() if p.requires_grad)\n",
    "mcm_decoder_params = sum(p.numel() for p in mcm_decoder.parameters() if p.requires_grad)\n",
    "logger.info(f\"model_params: {model_params}\")\n",
    "logger.info(f\"encoder_params: {encoder_params}\")\n",
    "logger.info(f\"lp_decoder_params: {lp_decoder_params}\")\n",
    "logger.info(f\"mcm_decoder_params: {mcm_decoder_params}\")\n",
    "learnable_params = model_params + encoder_params + lp_decoder_params + mcm_decoder_params\n",
    "logger.info(f\"learnable_params: {learnable_params}\")\n",
    "# wandb.log({\"learnable_params\": learnable_params})\n",
    "\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    # {'params': [p for n, p in encoder.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': weight_decay},\n",
    "    # {'params': [p for n, p in encoder.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': weight_decay},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n",
    "    {'params': [p for n, p in mcm_decoder.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': weight_decay},\n",
    "    {'params': [p for n, p in mcm_decoder.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n",
    "    {'params': [p for n, p in lp_decoder.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': weight_decay},\n",
    "    {'params': [p for n, p in lp_decoder.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=lr, eps=eps)\n",
    "scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=lr/3, max_lr=3*lr, step_size_up=2000, cycle_momentum=False)\n",
    "ssloss = SSLoss(device, num_numerical)\n",
    "ssmetric = SSMetric(device)\n",
    "mocoloss = MoCoLoss(model, 2, device, beta=0.999, beta_sigma=0.1, gamma=0.999, gamma_sigma=0.1, rho=0.05)\n",
    "\n",
    "save_dir = '/mnt/data/.cache/saved_models'\n",
    "#save_dir = '/scratch/takyildiz/.cache/saved_models'\n",
    "# run_id = wandb.run.id\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "best_lp = 0\n",
    "best_acc = 0\n",
    "best_rmse = 2\n",
    "\n",
    "\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def mcm_inputs(tf: TensorFrame, dataset):\n",
    "#     batch_size = len(tf.y)\n",
    "#     edges = tf.y[:,-3:]\n",
    "#     khop_source, khop_destination, idx = dataset.sample_neighbors(edges, 'train')\n",
    "\n",
    "#     edge_attr = dataset.tensor_frame.__getitem__(idx)\n",
    "\n",
    "#     nodes = torch.unique(torch.cat([khop_source, khop_destination]))\n",
    "#     num_nodes = nodes.shape[0]\n",
    "#     node_feats = torch.ones(num_nodes).view(-1,num_nodes).t()\n",
    "\n",
    "#     n_id_map = {value.item(): index for index, value in enumerate(nodes)}\n",
    "#     local_khop_source = torch.tensor([n_id_map[node.item()] for node in khop_source], dtype=torch.long)\n",
    "#     local_khop_destination = torch.tensor([n_id_map[node.item()] for node in khop_destination], dtype=torch.long)\n",
    "#     edge_index = torch.cat((local_khop_source.unsqueeze(0), local_khop_destination.unsqueeze(0)))\n",
    "\n",
    "#     drop_edge_ind = torch.tensor([x for x in range(batch_size)])\n",
    "#     mask = torch.zeros((edge_index.shape[1],)).long() #[E, ]\n",
    "#     mask = mask.index_fill_(dim=0, index=drop_edge_ind, value=1).bool() #[E, ]\n",
    "#     # input_edge_index = edge_index[:, ~mask]\n",
    "#     # input_edge_attr  = edge_attr[~mask]\n",
    "#     input_edge_index = edge_index\n",
    "#     input_edge_attr  = edge_attr\n",
    "#     target_edge_index = edge_index[:, mask]\n",
    "#     target_edge_attr  = edge_attr[mask]\n",
    "#     return node_feats.to(device), input_edge_index.to(device), input_edge_attr.to(device), target_edge_index.to(device), target_edge_attr.to(device)  \n",
    "\n",
    "# def lp_inputs(tf: TensorFrame, dataset):\n",
    "#     edges = tf.y[:,-3:]\n",
    "#     batch_size = len(edges)\n",
    "#     start = time.time()\n",
    "#     khop_source, khop_destination, idx = dataset.sample_neighbors(edges, 'train')\n",
    "#     # logger.info(f\"sample_neighbors time: {time.time()-start}\")\n",
    "    \n",
    "#     edge_attr = dataset.tensor_frame.__getitem__(idx)\n",
    "\n",
    "#     nodes = torch.unique(torch.cat([khop_source, khop_destination]))\n",
    "#     num_nodes = nodes.shape[0]\n",
    "#     node_feats = torch.ones(num_nodes).view(-1,num_nodes).t()\n",
    "\n",
    "#     start = time.time()\n",
    "#     import numpy as np\n",
    "\n",
    "#     n_id_map = {value.item(): index for index, value in enumerate(nodes)}\n",
    "#     vectorized_map = np.vectorize(lambda x: n_id_map[x])\n",
    "\n",
    "#     khop_combined = torch.cat((khop_source, khop_destination))\n",
    "#     local_khop_combined = torch.LongTensor(vectorized_map(khop_combined.numpy()))\n",
    "\n",
    "#     local_khop_source, local_khop_destination = local_khop_combined.split(khop_source.size(0))\n",
    "#     edge_index = torch.stack((local_khop_source, local_khop_destination))\n",
    "#     # n_id_map = {value.item(): index for index, value in enumerate(nodes)}\n",
    "#     # local_khop_source = torch.tensor([n_id_map[node.item()] for node in khop_source], dtype=torch.long)\n",
    "#     # local_khop_destination = torch.tensor([n_id_map[node.item()] for node in khop_destination], dtype=torch.long)\n",
    "#     # edge_index = torch.cat((local_khop_source.unsqueeze(0), local_khop_destination.unsqueeze(0)))\n",
    "#     #edge_index = torch.stack((local_khop_source, local_khop_destination))\n",
    "#     # logger.info(f\"new_edge_index time: {time.time()-start}\")\n",
    "#     start = time.time()\n",
    "\n",
    "#     # drop_edge_ind = torch.tensor([x for x in range(int(batch_size))])\n",
    "#     # mask = torch.zeros((edge_index.shape[1],)).long() #[E, ]\n",
    "#     # mask = mask.index_fill_(dim=0, index=drop_edge_ind, value=1).bool() #[E, ]\n",
    "#     input_edge_index = edge_index[:, batch_size:]\n",
    "#     input_edge_attr  = edge_attr[batch_size:]\n",
    "\n",
    "#     pos_edge_index = edge_index[:, :batch_size]\n",
    "#     pos_edge_attr  = edge_attr[:batch_size]\n",
    "#     #logger.info(f\"sample_neighbors time: {time.time()-start}\")\n",
    "\n",
    "#     #start = time.time()\n",
    "#     # generate/sample negative edges\n",
    "#     # neg_edges = []\n",
    "#     target_dict = pos_edge_attr.feat_dict\n",
    "#     for key, value in pos_edge_attr.feat_dict.items():\n",
    "#         attr = []\n",
    "#         # duplicate each row of the tensor by num_neg_samples times repeated values must be contiguous\n",
    "#         for r in value:\n",
    "#             if key == stype.timestamp:\n",
    "#                 attr.append(r.repeat(num_neg_samples, 1, 1))\n",
    "#             else:\n",
    "#                 attr.append(r.repeat(num_neg_samples, 1))\n",
    "#         target_dict[key] = torch.cat([target_dict[key], torch.cat(attr, dim=0)], dim=0)\n",
    "#     target_edge_attr = TensorFrame(target_dict, pos_edge_attr.col_names_dict)\n",
    "#     # logger.info(f\"target_edge_attr time: {time.time()-start}\")\n",
    "\n",
    "#     start = time.time()\n",
    "#     # nodeset = set(range(edge_index.max()+1))\n",
    "#     # # logger.info(f\"edge_index: {edge_index}\")\n",
    "#     # # logger.info(f\"pos_edge_index: {pos_edge_index}\")\n",
    "#     # # # Write to a file\n",
    "#     # # import json\n",
    "#     # # with open('edge_index.json', 'w') as f:\n",
    "#     # #     json.dump(edge_index.tolist(), f)\n",
    "#     # # with open('pos_edge_index.json', 'w') as f:\n",
    "#     # #     json.dump(pos_edge_index.tolist(), f)\n",
    "#     # # sys.exit()\n",
    "#     # for i, edge in enumerate(pos_edge_index.t()):\n",
    "#     #     src, dst = edge[0], edge[1]\n",
    "\n",
    "#     #     # # # Chose negative examples in a smart way\n",
    "#     #     # unavail_mask = (edge_index == src).any(dim=0) | (edge_index == dst).any(dim=0)\n",
    "#     #     # unavail_nodes = torch.unique(edge_index[:, unavail_mask])\n",
    "#     #     # unavail_nodes = set(unavail_nodes.tolist())\n",
    "#     #     # avail_nodes = nodeset - unavail_nodes\n",
    "#     #     # avail_nodes = torch.tensor(list(avail_nodes))\n",
    "#     #     # # Finally, emmulate np.random.choice() to chose randomly amongst available nodes\n",
    "#     #     # indices = torch.randperm(len(avail_nodes))[:num_neg_samples]\n",
    "#     #     # neg_nodes = avail_nodes[indices]\n",
    "        \n",
    "#     #     # # Create a mask of unavailable nodes\n",
    "#     #     # unavail_mask = torch.isin(edge_index.flatten(), torch.tensor([src, dst])).view(edge_index.shape).any(dim=0)\n",
    "        \n",
    "#     #     # # Get unique unavailable nodes\n",
    "#     #     # unavail_nodes = torch.unique(edge_index[:, unavail_mask])\n",
    "        \n",
    "#     #     # # Convert to set for fast set operations\n",
    "#     #     # unavail_nodes_set = set(unavail_nodes.tolist())\n",
    "        \n",
    "#     #     # # Determine available nodes by set difference\n",
    "#     #     # avail_nodes = list(nodeset - unavail_nodes_set)\n",
    "        \n",
    "#     #     # # Convert available nodes back to tensor\n",
    "#     #     # avail_nodes = torch.tensor(avail_nodes, dtype=torch.long)\n",
    "        \n",
    "#     #     # # Randomly select negative samples from available nodes\n",
    "#     #     # indices = torch.randperm(len(avail_nodes))[:num_neg_samples]\n",
    "#     #     # neg_nodes = avail_nodes[indices]\n",
    "\n",
    "#     #     # Create a mask of unavailable nodes\n",
    "#     #     unavail_mask = torch.isin(edge_index.flatten(), torch.tensor([src, dst]))\n",
    "#     #     unavail_nodes = edge_index.flatten()[unavail_mask].unique()\n",
    "\n",
    "#     #     # Create a mask for all nodes\n",
    "#     #     all_nodes = torch.arange(max(nodeset) + 1)\n",
    "#     #     avail_mask = ~torch.isin(all_nodes, unavail_nodes)\n",
    "\n",
    "#     #     # Get available nodes\n",
    "#     #     avail_nodes = all_nodes[avail_mask]\n",
    "\n",
    "#     #     # Randomly select negative samples from available nodes\n",
    "#     #     neg_nodes = avail_nodes[torch.randint(high=len(avail_nodes), size=(num_neg_samples,))]\n",
    "\n",
    "#     #     # Generate num_neg_samples/2 negative edges with the same source but different destinations\n",
    "#     #     num_neg_samples_half = int(num_neg_samples/2)\n",
    "#     #     neg_dsts = neg_nodes[:num_neg_samples_half]  # Selecting num_neg_samples/2 random destination nodes for the source\n",
    "#     #     neg_edges_src = torch.stack([src.repeat(num_neg_samples_half), neg_dsts], dim=0)\n",
    "        \n",
    "#     #     # Generate num_neg_samples/2 negative edges with the same destination but different sources\n",
    "#     #     neg_srcs = neg_nodes[num_neg_samples_half:]  # Selecting num_neg_samples/2 random source nodes for the destination\n",
    "#     #     neg_edges_dst = torch.stack([neg_srcs, dst.repeat(num_neg_samples_half)], dim=0)\n",
    "\n",
    "#     #     # Add these negative edges to the list\n",
    "#     #     neg_edges.append(neg_edges_src)\n",
    "#     #     neg_edges.append(neg_edges_dst)\n",
    "    \n",
    "#     input_edge_index = input_edge_index.to(device)\n",
    "#     input_edge_attr = input_edge_attr.to(device)\n",
    "#     #pos_edge_index = pos_edge_index.to(device)\n",
    "#     #pos_edge_attr = pos_edge_attr.to(device)\n",
    "#     node_feats = node_feats.to(device)\n",
    "    \n",
    "#     # if len(neg_edges) > 0:\n",
    "#     #     #neg_edge_index = torch.cat(neg_edges, dim=1).to(device)\n",
    "#     #     neg_edge_index = torch.cat(neg_edges, dim=1)\n",
    "\n",
    "#     neg_edge_index = negative_sampling.generate_negative_samples(edge_index.tolist(), pos_edge_index.tolist(), num_neg_samples)\n",
    "#     neg_edge_index = torch.tensor(neg_edge_index, dtype=torch.long)\n",
    "\n",
    "#     target_edge_index = torch.cat([pos_edge_index, neg_edge_index], dim=1).to(device)\n",
    "#     target_edge_attr = target_edge_attr.to(device)\n",
    "#     #logger.info(f\"negative edges: {time.time()-start}\")\n",
    "#     #sys.exit()\n",
    "#     return node_feats, input_edge_index, input_edge_attr, target_edge_index, target_edge_attr\n",
    "\n",
    "from src.utils.batch_processing import mcm_inputs, lp_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, loader, epoc: int, encoder, model, lp_decoder, mcm_decoder, optimizer, scheduler):\n",
    "    encoder.train()\n",
    "    model.train()\n",
    "    lp_decoder.train()\n",
    "    mcm_decoder.train()\n",
    "    loss_accum = total_count = 0\n",
    "    loss_lp_accum = loss_c_accum = loss_n_accum = total_count = t_c = t_n = 0\n",
    "    count = 0\n",
    "    ave = 0\n",
    "    with tqdm(loader, desc=f'Epoch {epoc}') as t:\n",
    "        for tf in t:\n",
    "            ss = time.time()\n",
    "            batch_size = len(tf.y)\n",
    "            start = time.time()\n",
    "            node_feats, edge_index, edge_attr, input_edge_index, input_edge_attr, target_edge_index, target_edge_attr = lp_inputs(tf, dataset)\n",
    "            node_feats = node_feats.to(device)\n",
    "            edge_index = edge_index.to(device)\n",
    "            edge_attr = edge_attr.to(device)\n",
    "            input_edge_index = input_edge_index.to(device)\n",
    "            #input_edge_attr = input_edge_attr.to(device)\n",
    "            target_edge_index = target_edge_index.to(device)\n",
    "            target_edge_attr = target_edge_attr.to(device)\n",
    "\n",
    "            l_time = time.time()-start\n",
    "            logger.info(f\"lp_inputs time: {l_time}\")\n",
    "            start = time.time()\n",
    "            edge_attr, _ = encoder(edge_attr)\n",
    "            input_edge_attr = edge_attr[batch_size:,:,:]\n",
    "            target_edge_attr, _ = encoder(target_edge_attr)\n",
    "            x_gnn, _, target_edge_attr_lp = model(node_feats, input_edge_index, input_edge_attr, target_edge_attr)\n",
    "            pos_edge_index = target_edge_index[:, :batch_size]\n",
    "            neg_edge_index = target_edge_index[:, batch_size:]\n",
    "            pos_edge_attr = target_edge_attr_lp[:batch_size,:]\n",
    "            neg_edge_attr = target_edge_attr_lp[batch_size:,:]\n",
    "            pos_pred, neg_pred = lp_decoder(x_gnn, pos_edge_index, pos_edge_attr, neg_edge_index, neg_edge_attr)\n",
    "\n",
    "            x_gnn, _, target_edge_attr_mcm = model(node_feats, edge_index, edge_attr, target_edge_attr)\n",
    "            x_target = x_gnn[pos_edge_index.T].reshape(-1, 2 * channels)#.relu()\n",
    "            pos_edge_attr = target_edge_attr_mcm[:batch_size,:]\n",
    "            x_target = torch.cat((x_target, pos_edge_attr), 1)\n",
    "            num_pred, cat_pred = mcm_decoder(x_target)\n",
    "            num_pred = num_pred.cpu()\n",
    "            cat_pred = [x.cpu() for x in cat_pred]\n",
    "            m_time = time.time()-start\n",
    "            logger.info(f\"forward time: {m_time}\")\n",
    "            \n",
    "            start = time.time()\n",
    "            optimizer.zero_grad()\n",
    "            link_loss = ssloss.lp_loss(pos_pred, neg_pred)\n",
    "            t_loss, loss_c, loss_n = ssloss.mcm_loss(cat_pred, num_pred, tf.y)\n",
    "            loss = link_loss + t_loss\n",
    "            loss.backward()\n",
    "            #moco_loss = mocoloss.loss([link_loss, t_loss])\n",
    "            optimizer.step()\n",
    "            # scheduler.step()\n",
    "\n",
    "            #loss_accum += ((link_loss.item()*moco_loss[0]+(t_loss.item()*moco_loss[1])) * len(tf.y))\n",
    "            total_count += len(tf.y)\n",
    "            t_c += loss_c[1]\n",
    "            t_n += loss_n[1]\n",
    "            loss_c_accum += loss_c[0].item()\n",
    "            loss_n_accum += loss_n[0].item()\n",
    "            loss_lp_accum += link_loss.item() * len(tf.y)\n",
    "            b_time = time.time()-start\n",
    "            logger.info(f\"backward time: {b_time}\")\n",
    "            t.set_postfix(loss=f'{loss_accum/total_count:.4f}', loss_lp=f'{loss_lp_accum/total_count:.4f}', loss_c=f'{loss_c_accum/t_c:.4f}', loss_n=f'{loss_n_accum/t_n:.4f}')#, moco_loss=f'{moco_loss[0]:.4f},{moco_loss[1]:.4f}')\n",
    "            logger.info(f\"total time: {b_time+m_time+l_time}\")\n",
    "            ave += b_time+m_time+l_time\n",
    "            # logger.info('-----------------')\n",
    "            if count == 100:\n",
    "                logger.info(f\"ave time: {ave/count}\")\n",
    "                break\n",
    "            count += 1\n",
    "    return {'loss': loss_accum / total_count} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec07b2868e284a69b0e3284b966d1626",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 0:   0%|          | 0/16245 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-19 09:07:06 - INFO - lp_inputs time: 0.0726618766784668\n",
      "2024-07-19 09:07:06 - INFO - forward time: 0.061501264572143555\n",
      "2024-07-19 09:07:06 - INFO - backward time: 0.05964374542236328\n",
      "2024-07-19 09:07:06 - INFO - total time: 0.19380688667297363\n",
      "2024-07-19 09:07:06 - INFO - lp_inputs time: 0.018404245376586914\n",
      "2024-07-19 09:07:06 - INFO - forward time: 0.05689525604248047\n",
      "2024-07-19 09:07:06 - INFO - backward time: 0.05898165702819824\n",
      "2024-07-19 09:07:06 - INFO - total time: 0.13428115844726562\n",
      "2024-07-19 09:07:07 - INFO - lp_inputs time: 0.018146038055419922\n",
      "2024-07-19 09:07:07 - INFO - forward time: 0.05805802345275879\n",
      "2024-07-19 09:07:07 - INFO - backward time: 0.05774259567260742\n",
      "2024-07-19 09:07:07 - INFO - total time: 0.13394665718078613\n",
      "2024-07-19 09:07:07 - INFO - lp_inputs time: 0.018685579299926758\n",
      "2024-07-19 09:07:07 - INFO - forward time: 0.05870556831359863\n",
      "2024-07-19 09:07:07 - INFO - backward time: 0.05827927589416504\n",
      "2024-07-19 09:07:07 - INFO - total time: 0.13567042350769043\n",
      "2024-07-19 09:07:07 - INFO - lp_inputs time: 0.019391775131225586\n",
      "2024-07-19 09:07:07 - INFO - forward time: 0.06462740898132324\n",
      "2024-07-19 09:07:07 - INFO - backward time: 0.06120491027832031\n",
      "2024-07-19 09:07:07 - INFO - total time: 0.14522409439086914\n",
      "2024-07-19 09:07:07 - INFO - lp_inputs time: 0.018042564392089844\n",
      "2024-07-19 09:07:07 - INFO - forward time: 0.06130647659301758\n",
      "2024-07-19 09:07:07 - INFO - backward time: 0.057248830795288086\n",
      "2024-07-19 09:07:07 - INFO - total time: 0.1365978717803955\n",
      "2024-07-19 09:07:07 - INFO - lp_inputs time: 0.01985025405883789\n",
      "2024-07-19 09:07:07 - INFO - forward time: 0.06176185607910156\n",
      "2024-07-19 09:07:07 - INFO - backward time: 0.061188697814941406\n",
      "2024-07-19 09:07:07 - INFO - total time: 0.14280080795288086\n",
      "2024-07-19 09:07:07 - INFO - lp_inputs time: 0.0182647705078125\n",
      "2024-07-19 09:07:07 - INFO - forward time: 0.059203147888183594\n",
      "2024-07-19 09:07:07 - INFO - backward time: 0.058638811111450195\n",
      "2024-07-19 09:07:07 - INFO - total time: 0.1361067295074463\n",
      "2024-07-19 09:07:07 - INFO - lp_inputs time: 0.019267559051513672\n",
      "2024-07-19 09:07:07 - INFO - forward time: 0.05744504928588867\n",
      "2024-07-19 09:07:07 - INFO - backward time: 0.06032824516296387\n",
      "2024-07-19 09:07:07 - INFO - total time: 0.1370408535003662\n",
      "2024-07-19 09:07:08 - INFO - lp_inputs time: 0.01906871795654297\n",
      "2024-07-19 09:07:08 - INFO - forward time: 0.056581735610961914\n",
      "2024-07-19 09:07:08 - INFO - backward time: 0.05914783477783203\n",
      "2024-07-19 09:07:08 - INFO - total time: 0.13479828834533691\n",
      "2024-07-19 09:07:08 - INFO - lp_inputs time: 0.019312381744384766\n",
      "2024-07-19 09:07:08 - INFO - forward time: 0.05774283409118652\n",
      "2024-07-19 09:07:08 - INFO - backward time: 0.06052875518798828\n",
      "2024-07-19 09:07:08 - INFO - total time: 0.13758397102355957\n",
      "2024-07-19 09:07:08 - INFO - lp_inputs time: 0.018650293350219727\n",
      "2024-07-19 09:07:08 - INFO - forward time: 0.056958675384521484\n",
      "2024-07-19 09:07:08 - INFO - backward time: 0.059747934341430664\n",
      "2024-07-19 09:07:08 - INFO - total time: 0.13535690307617188\n",
      "2024-07-19 09:07:08 - INFO - lp_inputs time: 0.018952608108520508\n",
      "2024-07-19 09:07:08 - INFO - forward time: 0.057256460189819336\n",
      "2024-07-19 09:07:08 - INFO - backward time: 0.06007194519042969\n",
      "2024-07-19 09:07:08 - INFO - total time: 0.13628101348876953\n",
      "2024-07-19 09:07:08 - INFO - lp_inputs time: 0.01970052719116211\n",
      "2024-07-19 09:07:08 - INFO - forward time: 0.05853891372680664\n",
      "2024-07-19 09:07:08 - INFO - backward time: 0.0615537166595459\n",
      "2024-07-19 09:07:08 - INFO - total time: 0.13979315757751465\n",
      "2024-07-19 09:07:08 - INFO - lp_inputs time: 0.018822193145751953\n",
      "2024-07-19 09:07:08 - INFO - forward time: 0.0572047233581543\n",
      "2024-07-19 09:07:08 - INFO - backward time: 0.06000471115112305\n",
      "2024-07-19 09:07:08 - INFO - total time: 0.1360316276550293\n",
      "2024-07-19 09:07:08 - INFO - lp_inputs time: 0.017864465713500977\n",
      "2024-07-19 09:07:08 - INFO - forward time: 0.0546262264251709\n",
      "2024-07-19 09:07:08 - INFO - backward time: 0.05729532241821289\n",
      "2024-07-19 09:07:08 - INFO - total time: 0.12978601455688477\n",
      "2024-07-19 09:07:08 - INFO - lp_inputs time: 0.019075632095336914\n",
      "2024-07-19 09:07:09 - INFO - forward time: 0.05715608596801758\n",
      "2024-07-19 09:07:09 - INFO - backward time: 0.059935569763183594\n",
      "2024-07-19 09:07:09 - INFO - total time: 0.13616728782653809\n",
      "2024-07-19 09:07:09 - INFO - lp_inputs time: 0.018204212188720703\n",
      "2024-07-19 09:07:09 - INFO - forward time: 0.05508303642272949\n",
      "2024-07-19 09:07:09 - INFO - backward time: 0.057662248611450195\n",
      "2024-07-19 09:07:09 - INFO - total time: 0.1309494972229004\n",
      "2024-07-19 09:07:09 - INFO - lp_inputs time: 0.018662452697753906\n",
      "2024-07-19 09:07:09 - INFO - forward time: 0.05614447593688965\n",
      "2024-07-19 09:07:09 - INFO - backward time: 0.05890154838562012\n",
      "2024-07-19 09:07:09 - INFO - total time: 0.13370847702026367\n",
      "2024-07-19 09:07:09 - INFO - lp_inputs time: 0.019063472747802734\n",
      "2024-07-19 09:07:09 - INFO - forward time: 0.05745697021484375\n",
      "2024-07-19 09:07:09 - INFO - backward time: 0.060273170471191406\n",
      "2024-07-19 09:07:09 - INFO - total time: 0.1367936134338379\n",
      "2024-07-19 09:07:09 - INFO - lp_inputs time: 0.018838882446289062\n",
      "2024-07-19 09:07:09 - INFO - forward time: 0.05669903755187988\n",
      "2024-07-19 09:07:09 - INFO - backward time: 0.05923867225646973\n",
      "2024-07-19 09:07:09 - INFO - total time: 0.13477659225463867\n",
      "2024-07-19 09:07:09 - INFO - lp_inputs time: 0.01890087127685547\n",
      "2024-07-19 09:07:09 - INFO - forward time: 0.05672907829284668\n",
      "2024-07-19 09:07:09 - INFO - backward time: 0.05955314636230469\n",
      "2024-07-19 09:07:09 - INFO - total time: 0.13518309593200684\n",
      "2024-07-19 09:07:09 - INFO - lp_inputs time: 0.018728971481323242\n",
      "2024-07-19 09:07:09 - INFO - forward time: 0.05706000328063965\n",
      "2024-07-19 09:07:09 - INFO - backward time: 0.05964231491088867\n",
      "2024-07-19 09:07:09 - INFO - total time: 0.13543128967285156\n",
      "2024-07-19 09:07:09 - INFO - lp_inputs time: 0.01921558380126953\n",
      "2024-07-19 09:07:09 - INFO - forward time: 0.05758237838745117\n",
      "2024-07-19 09:07:10 - INFO - backward time: 0.06009984016418457\n",
      "2024-07-19 09:07:10 - INFO - total time: 0.13689780235290527\n",
      "2024-07-19 09:07:10 - INFO - lp_inputs time: 0.018353939056396484\n",
      "2024-07-19 09:07:10 - INFO - forward time: 0.056104183197021484\n",
      "2024-07-19 09:07:10 - INFO - backward time: 0.05907297134399414\n",
      "2024-07-19 09:07:10 - INFO - total time: 0.1335310935974121\n",
      "2024-07-19 09:07:10 - INFO - lp_inputs time: 0.01905679702758789\n",
      "2024-07-19 09:07:10 - INFO - forward time: 0.05704092979431152\n",
      "2024-07-19 09:07:10 - INFO - backward time: 0.05948352813720703\n",
      "2024-07-19 09:07:10 - INFO - total time: 0.13558125495910645\n",
      "2024-07-19 09:07:10 - INFO - lp_inputs time: 0.018622159957885742\n",
      "2024-07-19 09:07:10 - INFO - forward time: 0.056124210357666016\n",
      "2024-07-19 09:07:10 - INFO - backward time: 0.058762311935424805\n",
      "2024-07-19 09:07:10 - INFO - total time: 0.13350868225097656\n",
      "2024-07-19 09:07:10 - INFO - lp_inputs time: 0.018912076950073242\n",
      "2024-07-19 09:07:10 - INFO - forward time: 0.05717015266418457\n",
      "2024-07-19 09:07:10 - INFO - backward time: 0.06006932258605957\n",
      "2024-07-19 09:07:10 - INFO - total time: 0.13615155220031738\n",
      "2024-07-19 09:07:10 - INFO - lp_inputs time: 0.018537282943725586\n",
      "2024-07-19 09:07:10 - INFO - forward time: 0.05594992637634277\n",
      "2024-07-19 09:07:10 - INFO - backward time: 0.05875277519226074\n",
      "2024-07-19 09:07:10 - INFO - total time: 0.1332399845123291\n",
      "2024-07-19 09:07:10 - INFO - lp_inputs time: 0.019330263137817383\n",
      "2024-07-19 09:07:10 - INFO - forward time: 0.061751604080200195\n",
      "2024-07-19 09:07:10 - INFO - backward time: 0.06160593032836914\n",
      "2024-07-19 09:07:10 - INFO - total time: 0.14268779754638672\n",
      "2024-07-19 09:07:10 - INFO - lp_inputs time: 0.01817631721496582\n",
      "2024-07-19 09:07:10 - INFO - forward time: 0.056130409240722656\n",
      "2024-07-19 09:07:11 - INFO - backward time: 0.05870413780212402\n",
      "2024-07-19 09:07:11 - INFO - total time: 0.1330108642578125\n",
      "2024-07-19 09:07:11 - INFO - lp_inputs time: 0.017525911331176758\n",
      "2024-07-19 09:07:11 - INFO - forward time: 0.054433345794677734\n",
      "2024-07-19 09:07:11 - INFO - backward time: 0.057242631912231445\n",
      "2024-07-19 09:07:11 - INFO - total time: 0.12920188903808594\n",
      "2024-07-19 09:07:11 - INFO - lp_inputs time: 0.019321203231811523\n",
      "2024-07-19 09:07:11 - INFO - forward time: 0.05850720405578613\n",
      "2024-07-19 09:07:11 - INFO - backward time: 0.061003684997558594\n",
      "2024-07-19 09:07:11 - INFO - total time: 0.13883209228515625\n",
      "2024-07-19 09:07:11 - INFO - lp_inputs time: 0.018858909606933594\n",
      "2024-07-19 09:07:11 - INFO - forward time: 0.0568852424621582\n",
      "2024-07-19 09:07:11 - INFO - backward time: 0.05966353416442871\n",
      "2024-07-19 09:07:11 - INFO - total time: 0.1354076862335205\n",
      "2024-07-19 09:07:11 - INFO - lp_inputs time: 0.01853036880493164\n",
      "2024-07-19 09:07:11 - INFO - forward time: 0.056616783142089844\n",
      "2024-07-19 09:07:11 - INFO - backward time: 0.05935096740722656\n",
      "2024-07-19 09:07:11 - INFO - total time: 0.13449811935424805\n",
      "2024-07-19 09:07:11 - INFO - lp_inputs time: 0.018706798553466797\n",
      "2024-07-19 09:07:11 - INFO - forward time: 0.057463645935058594\n",
      "2024-07-19 09:07:11 - INFO - backward time: 0.06011223793029785\n",
      "2024-07-19 09:07:11 - INFO - total time: 0.13628268241882324\n",
      "2024-07-19 09:07:11 - INFO - lp_inputs time: 0.019858837127685547\n",
      "2024-07-19 09:07:11 - INFO - forward time: 0.05891227722167969\n",
      "2024-07-19 09:07:11 - INFO - backward time: 0.061829566955566406\n",
      "2024-07-19 09:07:11 - INFO - total time: 0.14060068130493164\n",
      "2024-07-19 09:07:11 - INFO - lp_inputs time: 0.019238710403442383\n",
      "2024-07-19 09:07:11 - INFO - forward time: 0.057965755462646484\n",
      "2024-07-19 09:07:11 - INFO - backward time: 0.06076836585998535\n",
      "2024-07-19 09:07:11 - INFO - total time: 0.13797283172607422\n",
      "2024-07-19 09:07:12 - INFO - lp_inputs time: 0.01797342300415039\n",
      "2024-07-19 09:07:12 - INFO - forward time: 0.055377960205078125\n",
      "2024-07-19 09:07:12 - INFO - backward time: 0.058097124099731445\n",
      "2024-07-19 09:07:12 - INFO - total time: 0.13144850730895996\n",
      "2024-07-19 09:07:12 - INFO - lp_inputs time: 0.01836109161376953\n",
      "2024-07-19 09:07:12 - INFO - forward time: 0.056572914123535156\n",
      "2024-07-19 09:07:12 - INFO - backward time: 0.059545278549194336\n",
      "2024-07-19 09:07:12 - INFO - total time: 0.13447928428649902\n",
      "2024-07-19 09:07:12 - INFO - lp_inputs time: 0.01809859275817871\n",
      "2024-07-19 09:07:12 - INFO - forward time: 0.05554056167602539\n",
      "2024-07-19 09:07:12 - INFO - backward time: 0.05821418762207031\n",
      "2024-07-19 09:07:12 - INFO - total time: 0.13185334205627441\n",
      "2024-07-19 09:07:12 - INFO - lp_inputs time: 0.01883697509765625\n",
      "2024-07-19 09:07:12 - INFO - forward time: 0.05739426612854004\n",
      "2024-07-19 09:07:12 - INFO - backward time: 0.060187339782714844\n",
      "2024-07-19 09:07:12 - INFO - total time: 0.13641858100891113\n",
      "2024-07-19 09:07:12 - INFO - lp_inputs time: 0.01927328109741211\n",
      "2024-07-19 09:07:12 - INFO - forward time: 0.0582585334777832\n",
      "2024-07-19 09:07:12 - INFO - backward time: 0.06114816665649414\n",
      "2024-07-19 09:07:12 - INFO - total time: 0.13867998123168945\n",
      "2024-07-19 09:07:12 - INFO - lp_inputs time: 0.019376039505004883\n",
      "2024-07-19 09:07:12 - INFO - forward time: 0.058269500732421875\n",
      "2024-07-19 09:07:12 - INFO - backward time: 0.06115078926086426\n",
      "2024-07-19 09:07:12 - INFO - total time: 0.13879632949829102\n",
      "2024-07-19 09:07:12 - INFO - lp_inputs time: 0.01957249641418457\n",
      "2024-07-19 09:07:12 - INFO - forward time: 0.05841660499572754\n",
      "2024-07-19 09:07:12 - INFO - backward time: 0.06138014793395996\n",
      "2024-07-19 09:07:12 - INFO - total time: 0.13936924934387207\n",
      "2024-07-19 09:07:12 - INFO - lp_inputs time: 0.019388437271118164\n",
      "2024-07-19 09:07:13 - INFO - forward time: 0.05753302574157715\n",
      "2024-07-19 09:07:13 - INFO - backward time: 0.06016421318054199\n",
      "2024-07-19 09:07:13 - INFO - total time: 0.1370856761932373\n",
      "2024-07-19 09:07:13 - INFO - lp_inputs time: 0.019815444946289062\n",
      "2024-07-19 09:07:13 - INFO - forward time: 0.058828115463256836\n",
      "2024-07-19 09:07:13 - INFO - backward time: 0.06185626983642578\n",
      "2024-07-19 09:07:13 - INFO - total time: 0.14049983024597168\n",
      "2024-07-19 09:07:13 - INFO - lp_inputs time: 0.018970012664794922\n",
      "2024-07-19 09:07:13 - INFO - forward time: 0.05762124061584473\n",
      "2024-07-19 09:07:13 - INFO - backward time: 0.06069779396057129\n",
      "2024-07-19 09:07:13 - INFO - total time: 0.13728904724121094\n",
      "2024-07-19 09:07:13 - INFO - lp_inputs time: 0.01867365837097168\n",
      "2024-07-19 09:07:13 - INFO - forward time: 0.05720162391662598\n",
      "2024-07-19 09:07:13 - INFO - backward time: 0.05996823310852051\n",
      "2024-07-19 09:07:13 - INFO - total time: 0.13584351539611816\n",
      "2024-07-19 09:07:13 - INFO - lp_inputs time: 0.018506765365600586\n",
      "2024-07-19 09:07:13 - INFO - forward time: 0.05645489692687988\n",
      "2024-07-19 09:07:13 - INFO - backward time: 0.05938601493835449\n",
      "2024-07-19 09:07:13 - INFO - total time: 0.13434767723083496\n",
      "2024-07-19 09:07:13 - INFO - lp_inputs time: 0.018715620040893555\n",
      "2024-07-19 09:07:13 - INFO - forward time: 0.05693984031677246\n",
      "2024-07-19 09:07:13 - INFO - backward time: 0.059882164001464844\n",
      "2024-07-19 09:07:13 - INFO - total time: 0.13553762435913086\n",
      "2024-07-19 09:07:13 - INFO - lp_inputs time: 0.018854379653930664\n",
      "2024-07-19 09:07:13 - INFO - forward time: 0.05766463279724121\n",
      "2024-07-19 09:07:13 - INFO - backward time: 0.06039881706237793\n",
      "2024-07-19 09:07:13 - INFO - total time: 0.1369178295135498\n",
      "2024-07-19 09:07:13 - INFO - lp_inputs time: 0.018333911895751953\n",
      "2024-07-19 09:07:14 - INFO - forward time: 0.05539202690124512\n",
      "2024-07-19 09:07:14 - INFO - backward time: 0.05838656425476074\n",
      "2024-07-19 09:07:14 - INFO - total time: 0.1321125030517578\n",
      "2024-07-19 09:07:14 - INFO - lp_inputs time: 0.018929481506347656\n",
      "2024-07-19 09:07:14 - INFO - forward time: 0.05765247344970703\n",
      "2024-07-19 09:07:14 - INFO - backward time: 0.06027793884277344\n",
      "2024-07-19 09:07:14 - INFO - total time: 0.13685989379882812\n",
      "2024-07-19 09:07:14 - INFO - lp_inputs time: 0.018204689025878906\n",
      "2024-07-19 09:07:14 - INFO - forward time: 0.05587959289550781\n",
      "2024-07-19 09:07:14 - INFO - backward time: 0.05835604667663574\n",
      "2024-07-19 09:07:14 - INFO - total time: 0.13244032859802246\n",
      "2024-07-19 09:07:14 - INFO - lp_inputs time: 0.017384767532348633\n",
      "2024-07-19 09:07:14 - INFO - forward time: 0.05399966239929199\n",
      "2024-07-19 09:07:14 - INFO - backward time: 0.05625319480895996\n",
      "2024-07-19 09:07:14 - INFO - total time: 0.12763762474060059\n",
      "2024-07-19 09:07:14 - INFO - lp_inputs time: 0.018774032592773438\n",
      "2024-07-19 09:07:14 - INFO - forward time: 0.056890249252319336\n",
      "2024-07-19 09:07:14 - INFO - backward time: 0.05935502052307129\n",
      "2024-07-19 09:07:14 - INFO - total time: 0.13501930236816406\n",
      "2024-07-19 09:07:14 - INFO - lp_inputs time: 0.018347501754760742\n",
      "2024-07-19 09:07:14 - INFO - forward time: 0.05651092529296875\n",
      "2024-07-19 09:07:14 - INFO - backward time: 0.05903506278991699\n",
      "2024-07-19 09:07:14 - INFO - total time: 0.13389348983764648\n",
      "2024-07-19 09:07:14 - INFO - lp_inputs time: 0.01979851722717285\n",
      "2024-07-19 09:07:14 - INFO - forward time: 0.05965471267700195\n",
      "2024-07-19 09:07:14 - INFO - backward time: 0.06270527839660645\n",
      "2024-07-19 09:07:14 - INFO - total time: 0.14215850830078125\n",
      "2024-07-19 09:07:14 - INFO - lp_inputs time: 0.019138574600219727\n",
      "2024-07-19 09:07:14 - INFO - forward time: 0.057967185974121094\n",
      "2024-07-19 09:07:15 - INFO - backward time: 0.06070089340209961\n",
      "2024-07-19 09:07:15 - INFO - total time: 0.13780665397644043\n",
      "2024-07-19 09:07:15 - INFO - lp_inputs time: 0.01824784278869629\n",
      "2024-07-19 09:07:15 - INFO - forward time: 0.05584979057312012\n",
      "2024-07-19 09:07:15 - INFO - backward time: 0.05831646919250488\n",
      "2024-07-19 09:07:15 - INFO - total time: 0.1324141025543213\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x78f8f5750430>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/miniconda3/envs/rel-mm/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/miniconda3/envs/rel-mm/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1443, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/opt/miniconda3/envs/rel-mm/lib/python3.10/multiprocessing/process.py\", line 149, in join\n",
      "    res = self._popen.wait(timeout)\n",
      "  File \"/opt/miniconda3/envs/rel-mm/lib/python3.10/multiprocessing/popen_fork.py\", line 40, in wait\n",
      "    if not wait([self.sentinel], timeout):\n",
      "  File \"/opt/miniconda3/envs/rel-mm/lib/python3.10/multiprocessing/connection.py\", line 931, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/opt/miniconda3/envs/rel-mm/lib/python3.10/selectors.py\", line 416, in select\n",
      "    fd_event_list = self._selector.poll(timeout)\n",
      "KeyboardInterrupt: \n",
      "2024-07-19 09:07:51 - INFO - lp_inputs time: 35.900059938430786\n",
      "2024-07-19 09:07:51 - INFO - forward time: 0.07610678672790527\n",
      "2024-07-19 09:07:51 - INFO - backward time: 0.06362175941467285\n",
      "2024-07-19 09:07:51 - INFO - total time: 36.039788484573364\n",
      "2024-07-19 09:07:51 - INFO - lp_inputs time: 0.022232770919799805\n",
      "2024-07-19 09:07:51 - INFO - forward time: 0.0637350082397461\n",
      "2024-07-19 09:07:51 - INFO - backward time: 0.06049752235412598\n",
      "2024-07-19 09:07:51 - INFO - total time: 0.14646530151367188\n",
      "2024-07-19 09:07:51 - INFO - lp_inputs time: 0.02176189422607422\n",
      "2024-07-19 09:07:51 - INFO - forward time: 0.05743741989135742\n",
      "2024-07-19 09:07:51 - INFO - backward time: 0.06033039093017578\n",
      "2024-07-19 09:07:51 - INFO - total time: 0.13952970504760742\n",
      "2024-07-19 09:07:51 - INFO - lp_inputs time: 0.0233304500579834\n",
      "2024-07-19 09:07:51 - INFO - forward time: 0.058127403259277344\n",
      "2024-07-19 09:07:51 - INFO - backward time: 0.06137657165527344\n",
      "2024-07-19 09:07:51 - INFO - total time: 0.14283442497253418\n",
      "2024-07-19 09:07:51 - INFO - lp_inputs time: 0.021240949630737305\n",
      "2024-07-19 09:07:51 - INFO - forward time: 0.05643796920776367\n",
      "2024-07-19 09:07:51 - INFO - backward time: 0.05953335762023926\n",
      "2024-07-19 09:07:51 - INFO - total time: 0.13721227645874023\n",
      "2024-07-19 09:07:51 - INFO - lp_inputs time: 0.018809080123901367\n",
      "2024-07-19 09:07:51 - INFO - forward time: 0.054930925369262695\n",
      "2024-07-19 09:07:51 - INFO - backward time: 0.05779838562011719\n",
      "2024-07-19 09:07:51 - INFO - total time: 0.13153839111328125\n",
      "2024-07-19 09:07:51 - INFO - lp_inputs time: 0.02002739906311035\n",
      "2024-07-19 09:07:52 - INFO - forward time: 0.05677008628845215\n",
      "2024-07-19 09:07:52 - INFO - backward time: 0.059638023376464844\n",
      "2024-07-19 09:07:52 - INFO - total time: 0.13643550872802734\n",
      "2024-07-19 09:07:52 - INFO - lp_inputs time: 0.019265174865722656\n",
      "2024-07-19 09:07:52 - INFO - forward time: 0.05615568161010742\n",
      "2024-07-19 09:07:52 - INFO - backward time: 0.05895185470581055\n",
      "2024-07-19 09:07:52 - INFO - total time: 0.13437271118164062\n",
      "2024-07-19 09:07:52 - INFO - lp_inputs time: 0.018964529037475586\n",
      "2024-07-19 09:07:52 - INFO - forward time: 0.055023193359375\n",
      "2024-07-19 09:07:52 - INFO - backward time: 0.057727813720703125\n",
      "2024-07-19 09:07:52 - INFO - total time: 0.1317155361175537\n",
      "2024-07-19 09:07:52 - INFO - lp_inputs time: 0.01953601837158203\n",
      "2024-07-19 09:07:52 - INFO - forward time: 0.05672407150268555\n",
      "2024-07-19 09:07:52 - INFO - backward time: 0.059308528900146484\n",
      "2024-07-19 09:07:52 - INFO - total time: 0.13556861877441406\n",
      "2024-07-19 09:07:52 - INFO - lp_inputs time: 0.019835948944091797\n",
      "2024-07-19 09:07:52 - INFO - forward time: 0.056799888610839844\n",
      "2024-07-19 09:07:52 - INFO - backward time: 0.059828758239746094\n",
      "2024-07-19 09:07:52 - INFO - total time: 0.13646459579467773\n",
      "2024-07-19 09:07:52 - INFO - lp_inputs time: 0.0201263427734375\n",
      "2024-07-19 09:07:52 - INFO - forward time: 0.057378530502319336\n",
      "2024-07-19 09:07:52 - INFO - backward time: 0.06010556221008301\n",
      "2024-07-19 09:07:52 - INFO - total time: 0.13761043548583984\n",
      "2024-07-19 09:07:52 - INFO - lp_inputs time: 0.018680095672607422\n",
      "2024-07-19 09:07:52 - INFO - forward time: 0.0545804500579834\n",
      "2024-07-19 09:07:52 - INFO - backward time: 0.05729961395263672\n",
      "2024-07-19 09:07:52 - INFO - total time: 0.13056015968322754\n",
      "2024-07-19 09:07:52 - INFO - lp_inputs time: 0.019073963165283203\n",
      "2024-07-19 09:07:52 - INFO - forward time: 0.055466651916503906\n",
      "2024-07-19 09:07:53 - INFO - backward time: 0.05815315246582031\n",
      "2024-07-19 09:07:53 - INFO - total time: 0.13269376754760742\n",
      "2024-07-19 09:07:53 - INFO - lp_inputs time: 0.02054286003112793\n",
      "2024-07-19 09:07:53 - INFO - forward time: 0.057508230209350586\n",
      "2024-07-19 09:07:53 - INFO - backward time: 0.06045341491699219\n",
      "2024-07-19 09:07:53 - INFO - total time: 0.1385045051574707\n",
      "2024-07-19 09:07:53 - INFO - lp_inputs time: 0.020176410675048828\n",
      "2024-07-19 09:07:53 - INFO - forward time: 0.05739235877990723\n",
      "2024-07-19 09:07:53 - INFO - backward time: 0.06006646156311035\n",
      "2024-07-19 09:07:53 - INFO - total time: 0.1376352310180664\n",
      "2024-07-19 09:07:53 - INFO - lp_inputs time: 0.019506454467773438\n",
      "2024-07-19 09:07:53 - INFO - forward time: 0.056737661361694336\n",
      "2024-07-19 09:07:53 - INFO - backward time: 0.05949759483337402\n",
      "2024-07-19 09:07:53 - INFO - total time: 0.1357417106628418\n",
      "2024-07-19 09:07:53 - INFO - lp_inputs time: 0.019993066787719727\n",
      "2024-07-19 09:07:53 - INFO - forward time: 0.05732917785644531\n",
      "2024-07-19 09:07:53 - INFO - backward time: 0.060106754302978516\n",
      "2024-07-19 09:07:53 - INFO - total time: 0.13742899894714355\n",
      "2024-07-19 09:07:53 - INFO - lp_inputs time: 0.018916845321655273\n",
      "2024-07-19 09:07:53 - INFO - forward time: 0.054918766021728516\n",
      "2024-07-19 09:07:53 - INFO - backward time: 0.057402610778808594\n",
      "2024-07-19 09:07:53 - INFO - total time: 0.13123822212219238\n",
      "2024-07-19 09:07:53 - INFO - lp_inputs time: 0.01945018768310547\n",
      "2024-07-19 09:07:53 - INFO - forward time: 0.0558779239654541\n",
      "2024-07-19 09:07:53 - INFO - backward time: 0.058623313903808594\n",
      "2024-07-19 09:07:53 - INFO - total time: 0.13395142555236816\n",
      "2024-07-19 09:07:53 - INFO - lp_inputs time: 0.020510196685791016\n",
      "2024-07-19 09:07:53 - INFO - forward time: 0.05827593803405762\n",
      "2024-07-19 09:07:54 - INFO - backward time: 0.06091451644897461\n",
      "2024-07-19 09:07:54 - INFO - total time: 0.13970065116882324\n",
      "2024-07-19 09:07:54 - INFO - lp_inputs time: 0.020359516143798828\n",
      "2024-07-19 09:07:54 - INFO - forward time: 0.0580904483795166\n",
      "2024-07-19 09:07:54 - INFO - backward time: 0.06043124198913574\n",
      "2024-07-19 09:07:54 - INFO - total time: 0.13888120651245117\n",
      "2024-07-19 09:07:54 - INFO - lp_inputs time: 0.01892852783203125\n",
      "2024-07-19 09:07:54 - INFO - forward time: 0.05532717704772949\n",
      "2024-07-19 09:07:54 - INFO - backward time: 0.05754733085632324\n",
      "2024-07-19 09:07:54 - INFO - total time: 0.13180303573608398\n",
      "2024-07-19 09:07:54 - INFO - lp_inputs time: 0.020777463912963867\n",
      "2024-07-19 09:07:54 - INFO - forward time: 0.05875515937805176\n",
      "2024-07-19 09:07:54 - INFO - backward time: 0.06214022636413574\n",
      "2024-07-19 09:07:54 - INFO - total time: 0.14167284965515137\n",
      "2024-07-19 09:07:54 - INFO - lp_inputs time: 0.02042365074157715\n",
      "2024-07-19 09:07:54 - INFO - forward time: 0.05803322792053223\n",
      "2024-07-19 09:07:54 - INFO - backward time: 0.0603032112121582\n",
      "2024-07-19 09:07:54 - INFO - total time: 0.13876008987426758\n",
      "2024-07-19 09:07:54 - INFO - lp_inputs time: 0.019871234893798828\n",
      "2024-07-19 09:07:54 - INFO - forward time: 0.05675363540649414\n",
      "2024-07-19 09:07:54 - INFO - backward time: 0.059171438217163086\n",
      "2024-07-19 09:07:54 - INFO - total time: 0.13579630851745605\n",
      "2024-07-19 09:07:54 - INFO - lp_inputs time: 0.019947052001953125\n",
      "2024-07-19 09:07:54 - INFO - forward time: 0.05771613121032715\n",
      "2024-07-19 09:07:54 - INFO - backward time: 0.06015443801879883\n",
      "2024-07-19 09:07:54 - INFO - total time: 0.1378176212310791\n",
      "2024-07-19 09:07:54 - INFO - lp_inputs time: 0.020735502243041992\n",
      "2024-07-19 09:07:54 - INFO - forward time: 0.0585331916809082\n",
      "2024-07-19 09:07:54 - INFO - backward time: 0.06107068061828613\n",
      "2024-07-19 09:07:54 - INFO - total time: 0.14033937454223633\n",
      "2024-07-19 09:07:55 - INFO - lp_inputs time: 0.01922011375427246\n",
      "2024-07-19 09:07:55 - INFO - forward time: 0.05575275421142578\n",
      "2024-07-19 09:07:55 - INFO - backward time: 0.05849051475524902\n",
      "2024-07-19 09:07:55 - INFO - total time: 0.13346338272094727\n",
      "2024-07-19 09:07:55 - INFO - lp_inputs time: 0.02131795883178711\n",
      "2024-07-19 09:07:55 - INFO - forward time: 0.05915069580078125\n",
      "2024-07-19 09:07:55 - INFO - backward time: 0.061640262603759766\n",
      "2024-07-19 09:07:55 - INFO - total time: 0.14210891723632812\n",
      "2024-07-19 09:07:55 - INFO - lp_inputs time: 0.020974159240722656\n",
      "2024-07-19 09:07:55 - INFO - forward time: 0.05920147895812988\n",
      "2024-07-19 09:07:55 - INFO - backward time: 0.062125444412231445\n",
      "2024-07-19 09:07:55 - INFO - total time: 0.14230108261108398\n",
      "2024-07-19 09:07:55 - INFO - lp_inputs time: 0.019230365753173828\n",
      "2024-07-19 09:07:55 - INFO - forward time: 0.0554046630859375\n",
      "2024-07-19 09:07:55 - INFO - backward time: 0.05793595314025879\n",
      "2024-07-19 09:07:55 - INFO - total time: 0.13257098197937012\n",
      "2024-07-19 09:07:55 - INFO - lp_inputs time: 0.019945621490478516\n",
      "2024-07-19 09:07:55 - INFO - forward time: 0.05718731880187988\n",
      "2024-07-19 09:07:55 - INFO - backward time: 0.05942177772521973\n",
      "2024-07-19 09:07:55 - INFO - total time: 0.13655471801757812\n",
      "2024-07-19 09:07:55 - INFO - lp_inputs time: 0.02025151252746582\n",
      "2024-07-19 09:07:55 - INFO - forward time: 0.057512760162353516\n",
      "2024-07-19 09:07:55 - INFO - backward time: 0.059682369232177734\n",
      "2024-07-19 09:07:55 - INFO - total time: 0.13744664192199707\n",
      "2024-07-19 09:07:55 - INFO - lp_inputs time: 0.019594907760620117\n",
      "2024-07-19 09:07:55 - INFO - forward time: 0.055906057357788086\n",
      "2024-07-19 09:07:55 - INFO - backward time: 0.05851554870605469\n",
      "2024-07-19 09:07:55 - INFO - total time: 0.1340165138244629\n",
      "2024-07-19 09:07:55 - INFO - lp_inputs time: 0.019494295120239258\n",
      "2024-07-19 09:07:56 - INFO - forward time: 0.05696249008178711\n",
      "2024-07-19 09:07:56 - INFO - backward time: 0.05956864356994629\n",
      "2024-07-19 09:07:56 - INFO - total time: 0.13602542877197266\n",
      "2024-07-19 09:07:56 - INFO - lp_inputs time: 0.021086692810058594\n",
      "2024-07-19 09:07:56 - INFO - forward time: 0.05872607231140137\n",
      "2024-07-19 09:07:56 - INFO - backward time: 0.06161308288574219\n",
      "2024-07-19 09:07:56 - INFO - total time: 0.14142584800720215\n",
      "2024-07-19 09:07:56 - INFO - lp_inputs time: 0.021248340606689453\n",
      "2024-07-19 09:07:56 - INFO - forward time: 0.0594937801361084\n",
      "2024-07-19 09:07:56 - INFO - backward time: 0.06198406219482422\n",
      "2024-07-19 09:07:56 - INFO - total time: 0.14272618293762207\n",
      "2024-07-19 09:07:56 - INFO - lp_inputs time: 0.019147634506225586\n",
      "2024-07-19 09:07:56 - INFO - forward time: 0.0553288459777832\n",
      "2024-07-19 09:07:56 - INFO - backward time: 0.0575251579284668\n",
      "2024-07-19 09:07:56 - INFO - total time: 0.13200163841247559\n",
      "2024-07-19 09:07:56 - INFO - lp_inputs time: 0.019783496856689453\n",
      "2024-07-19 09:07:56 - INFO - forward time: 0.05677175521850586\n",
      "2024-07-19 09:07:56 - INFO - backward time: 0.05994105339050293\n",
      "2024-07-19 09:07:56 - INFO - total time: 0.13649630546569824\n",
      "2024-07-19 09:07:56 - INFO - ave time: 0.4972362899780273\n"
     ]
    }
   ],
   "source": [
    "loss = train(dataset, train_loader, 0, encoder, model, lp_decoder, mcm_decoder, optimizer, scheduler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.nn.models import TABGNNFused\n",
    "model = TABGNNFused(\n",
    "    encoder=encoder,\n",
    "    channels=channels,\n",
    "    edge_dim=channels*dataset.tensor_frame.num_cols,\n",
    "    num_layers=num_layers, \n",
    "    dropout=dropout,\n",
    "    deg=in_degree_histogram\n",
    ")\n",
    "model.to(device)\n",
    "def train_mcm(dataset, loader, epoc: int, encoder, model, mcm_decoder, optimizer, scheduler) -> float:\n",
    "    model.train()\n",
    "    loss_accum = total_count = 0\n",
    "    loss_accum = loss_lp_accum = loss_c_accum = loss_n_accum = total_count = t_c = t_n = 0\n",
    "\n",
    "    with tqdm(loader, desc=f'Epoch {epoc}') as t:\n",
    "        for tf in t:\n",
    "            node_feats, edge_index, edge_attr, target_edge_index, target_edge_attr = mcm_inputs(tf, dataset, 'train')\n",
    "            node_feats = node_feats.to(device)\n",
    "            edge_index = edge_index.to(device)\n",
    "            edge_attr = edge_attr.to(device)\n",
    "            target_edge_index = target_edge_index.to(device)\n",
    "            target_edge_attr = target_edge_attr.to(device)\n",
    "\n",
    "            edge_attr, _ = encoder(edge_attr)\n",
    "            target_edge_attr, _ = encoder(target_edge_attr)\n",
    "            x, edge_attr, target_edge_attr = model(node_feats, edge_index, edge_attr, target_edge_index, target_edge_attr)\n",
    "            x_target = x[target_edge_index.T].reshape(-1, 2 * args[\"channels\"])#.relu()\n",
    "            x_target = torch.cat((x_target, target_edge_attr), 1)\n",
    "            num_pred, cat_pred = mcm_decoder(x_target)\n",
    "            num_pred = num_pred.cpu()\n",
    "            cat_pred = [x.cpu() for x in cat_pred]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            t_loss, loss_c, loss_n = ssloss.mcm_loss(cat_pred, num_pred, tf.y)\n",
    "            t_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_accum += (t_loss.item() * len(tf.y))\n",
    "            total_count += len(tf.y)\n",
    "            t_c += loss_c[1]\n",
    "            t_n += loss_n[1]\n",
    "            loss_c_accum += loss_c[0].item()\n",
    "            loss_n_accum += loss_n[0].item()\n",
    "            t.set_postfix(loss=f'{loss_accum/total_count:.4f}', loss_c=f'{loss_c_accum/t_c:.4f}', loss_n=f'{loss_n_accum/t_n:.4f}')\n",
    "            # wandb.log({\"train_loss\": loss_accum/total_count, \"train_loss_c\": loss_c_accum/t_c, \"train_loss_n\": loss_n_accum/t_n})\n",
    "    return {'loss': loss_accum / total_count}\n",
    "\n",
    "def train_lp(dataset, loader, epoc: int, encoder, model, lp_decoder, optimizer, scheduler) -> float:\n",
    "    encoder.train()\n",
    "    model.train()\n",
    "    lp_decoder.train()\n",
    "    total_count = 0\n",
    "    loss_lp_accum = 0\n",
    "\n",
    "    with tqdm(loader, desc=f'Epoch {epoc}') as t:\n",
    "        for tf in t:\n",
    "            batch_size = len(tf.y)\n",
    "            node_feats, _, _, neigh_edge_index, neigh_edge_attr, target_edge_index, target_edge_attr = lp_inputs(tf, dataset, args[\"num_neg_samples\"], 'train')\n",
    "            node_feats = node_feats.to(device)\n",
    "            neigh_edge_index = neigh_edge_index.to(device)\n",
    "            neigh_edge_attr = neigh_edge_attr.to(device)\n",
    "            target_edge_index = target_edge_index.to(device)\n",
    "            target_edge_attr = target_edge_attr.to(device)\n",
    "\n",
    "            neigh_edge_attr, _ = encoder(neigh_edge_attr)\n",
    "            target_edge_attr, _ = encoder(target_edge_attr)\n",
    "            x_gnn, _, target_edge_attr = model(node_feats, neigh_edge_index, neigh_edge_attr, target_edge_index, target_edge_attr, True)\n",
    "            pos_edge_index = target_edge_index[:, :batch_size]\n",
    "            neg_edge_index = target_edge_index[:, batch_size:]\n",
    "            pos_edge_attr = target_edge_attr[:batch_size,:]\n",
    "            neg_edge_attr = target_edge_attr[batch_size:,:]\n",
    "            pos_pred, neg_pred = lp_decoder(x_gnn, pos_edge_index, pos_edge_attr, neg_edge_index, neg_edge_attr)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            link_loss = ssloss.lp_loss(pos_pred, neg_pred)\n",
    "            link_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_count += len(tf.y)\n",
    "            loss_lp_accum += link_loss.item() * len(tf.y)\n",
    "            t.set_postfix(loss_lp=f'{loss_lp_accum/total_count:.4f}')\n",
    "            # wandb.log({\"train_loss_lp\": loss_lp_accum/total_count})\n",
    "            #wandb.log({\"lr\": scheduler.get_last_lr()[0]})\n",
    "    return {'loss': loss_lp_accum / total_count} \n",
    "def trainf(dataset, loader, epoc: int, encoder, model, lp_decoder, mcm_decoder, optimizer, scheduler, moo):\n",
    "    encoder.train()\n",
    "    model.train()\n",
    "    lp_decoder.train()\n",
    "    mcm_decoder.train()\n",
    "    count = 0\n",
    "    if moo == \"moco\":\n",
    "        mocoloss = MoCoLoss(model, 2, device, beta=0.999, beta_sigma=0.1, gamma=0.999, gamma_sigma=0.1, rho=0.05)\n",
    "    loss_accum = total_count = 0\n",
    "    loss_lp_accum = loss_c_accum = loss_n_accum = total_count = t_c = t_n = 0\n",
    "    with tqdm(loader, desc=f'Epoch {epoc}') as t:\n",
    "        for tf in t:\n",
    "            start = time.time()\n",
    "            batch_size = len(tf.y)\n",
    "            node_feats, edge_index, edge_attr, neigh_edge_index, neigh_edge_attr, target_edge_index, target_edge_attr = lp_inputs(tf, dataset, args[\"num_neg_samples\"], 'train')\n",
    "            node_feats = node_feats.to(device)\n",
    "            neigh_edge_index = neigh_edge_index.to(device)\n",
    "            neigh_edge_attr = neigh_edge_attr.to(device)\n",
    "            target_edge_index = target_edge_index.to(device)\n",
    "            target_edge_attr = target_edge_attr.to(device)\n",
    "            edge_index = edge_index.to(device)\n",
    "            edge_attr = edge_attr.to(device)\n",
    "            print(f\"lp_inputs time: {time.time()-start}\")\n",
    "            \n",
    "            start = time.time()\n",
    "            edge_attr, _ = encoder(edge_attr)\n",
    "            neigh_edge_attr = edge_attr[batch_size:,:,:]\n",
    "            target_edge_attr, _ = encoder(target_edge_attr)\n",
    "            x_gnn, _, target_edge_attr_lp = model(node_feats, neigh_edge_index, neigh_edge_attr, target_edge_index, target_edge_attr, True)\n",
    "            pos_edge_index = target_edge_index[:, :batch_size]\n",
    "            neg_edge_index = target_edge_index[:, batch_size:]\n",
    "            pos_edge_attr = target_edge_attr_lp[:batch_size,:]\n",
    "            neg_edge_attr = target_edge_attr_lp[batch_size:,:]\n",
    "            pos_pred, neg_pred = lp_decoder(x_gnn, pos_edge_index, pos_edge_attr, neg_edge_index, neg_edge_attr)\n",
    "\n",
    "            x_gnn, _, target_edge_attr_mcm = model(node_feats, edge_index, edge_attr, target_edge_index, target_edge_attr)\n",
    "            x_target = x_gnn[pos_edge_index.T].reshape(-1, 2 * args[\"channels\"])#.relu()\n",
    "            pos_edge_attr = target_edge_attr_mcm[:batch_size,:]\n",
    "            x_target = torch.cat((x_target, pos_edge_attr), 1)\n",
    "            num_pred, cat_pred = mcm_decoder(x_target)\n",
    "            print(f\"forward time: {time.time()-start}\")\n",
    "\n",
    "            start = time.time()\n",
    "            num_pred = num_pred.cpu()\n",
    "            cat_pred = [x.cpu() for x in cat_pred]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            link_loss = ssloss.lp_loss(pos_pred, neg_pred)\n",
    "            t_loss, loss_c, loss_n = ssloss.mcm_loss(cat_pred, num_pred, tf.y)\n",
    "            if moo == \"moco\":\n",
    "                moco_loss = mocoloss.loss([link_loss, t_loss])\n",
    "                loss_accum += ((link_loss.item()*moco_loss[0]+(t_loss.item()*moco_loss[1])) * len(tf.y))\n",
    "            else:\n",
    "                loss = link_loss + t_loss\n",
    "                loss.backward()\n",
    "                loss_accum += ((link_loss.item()+(t_loss.item())) * len(tf.y))\n",
    "            optimizer.step()\n",
    "            # scheduler.step()\n",
    "\n",
    "            total_count += len(tf.y)\n",
    "            t_c += loss_c[1]\n",
    "            t_n += loss_n[1]\n",
    "            loss_c_accum += loss_c[0].item()\n",
    "            loss_n_accum += loss_n[0].item()\n",
    "            loss_lp_accum += link_loss.item() * len(tf.y)\n",
    "            print(f\"backward time: {time.time()-start}\")\n",
    "            t.set_postfix(loss=f'{loss_accum/total_count:.4f}', loss_lp=f'{loss_lp_accum/total_count:.4f}', loss_c=f'{loss_c_accum/t_c:.4f}', loss_n=f'{loss_n_accum/t_n:.4f}')\n",
    "            # wandb.log({\"epoch\": epoc, \"train_loss\": loss_accum/total_count, \"train_loss_lp\": loss_lp_accum/total_count, \"train_loss_c\": loss_c_accum/t_c, \"train_loss_n\": loss_n_accum/t_n})\n",
    "            #wandb.log({\"lr\": scheduler.get_last_lr()[0]})\n",
    "            if count == 10:\n",
    "                # logger.info(f\"ave time: {ave/count}\")\n",
    "                break\n",
    "            count += 1\n",
    "    return {'loss': loss_accum / total_count} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca629e63e21848818eaa45c59adbddae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 0:   0%|          | 0/16245 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/takyildiz/dev/models-for-relational-multimodal-data/src/datasets/ibm_transactions_for_aml.py:142: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edges = torch.tensor(edges, dtype=torch.int)\n",
      "/home/takyildiz/dev/models-for-relational-multimodal-data/src/datasets/ibm_transactions_for_aml.py:146: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input = EdgeSamplerInput(None, torch.tensor(row, dtype=torch.long), torch.tensor(col, dtype=torch.long))\n",
      "/opt/miniconda3/envs/rel-mm/lib/python3.10/site-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "/opt/miniconda3/envs/rel-mm/lib/python3.10/site-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='min')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# loss = train_mcm(dataset, train_loader, 0, encoder, model, mcm_decoder, optimizer, scheduler)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_lp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlp_decoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# loss = train(dataset, train_loader, 0, encoder, model, lp_decoder, mcm_decoder, optimizer, scheduler, 'sum')\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[8], line 77\u001b[0m, in \u001b[0;36mtrain_lp\u001b[0;34m(dataset, loader, epoc, encoder, model, lp_decoder, optimizer, scheduler)\u001b[0m\n\u001b[1;32m     75\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     76\u001b[0m link_loss \u001b[38;5;241m=\u001b[39m ssloss\u001b[38;5;241m.\u001b[39mlp_loss(pos_pred, neg_pred)\n\u001b[0;32m---> 77\u001b[0m \u001b[43mlink_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     80\u001b[0m total_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(tf\u001b[38;5;241m.\u001b[39my)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/rel-mm/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/rel-mm/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# loss = train_mcm(dataset, train_loader, 0, encoder, model, mcm_decoder, optimizer, scheduler)\n",
    "loss = train_lp(dataset, train_loader, 0, encoder, model, lp_decoder, optimizer, scheduler)\n",
    "# loss = train(dataset, train_loader, 0, encoder, model, lp_decoder, mcm_decoder, optimizer, scheduler, 'sum')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rel-mm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
