{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from src.primitives import negative_sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-15 15:53:33 - DEBUG - Popen(['git', 'version'], cwd=/home/takyildiz/dev/models-for-relational-multimodal-data, stdin=None, shell=False, universal_newlines=False)\n",
      "2024-07-15 15:53:33 - DEBUG - Popen(['git', 'version'], cwd=/home/takyildiz/dev/models-for-relational-multimodal-data, stdin=None, shell=False, universal_newlines=False)\n",
      "2024-07-15 15:53:33 - DEBUG - Trying paths: ['/home/takyildiz/.docker/config.json', '/home/takyildiz/.dockercfg']\n",
      "2024-07-15 15:53:33 - DEBUG - No config file found\n",
      "2024-07-15 15:53:33 - DEBUG - [Tracing] Create new propagation context: {'trace_id': '5f83322478ba40208f6ae9b9169f4072', 'span_id': 'ab6754d815996251', 'parent_span_id': None, 'dynamic_sampling_context': None}\n",
      "2024-07-15 15:53:33 - ERROR - Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "2024-07-15 15:53:33 - DEBUG - Starting new HTTPS connection (1): api.wandb.ai:443\n",
      "2024-07-15 15:53:33 - DEBUG - https://api.wandb.ai:443 \"POST /graphql HTTP/1.1\" 200 1918\n",
      "2024-07-15 15:53:34 - DEBUG - https://api.wandb.ai:443 \"POST /graphql HTTP/1.1\" 200 390\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maakyildiz\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "2024-07-15 15:53:34 - DEBUG - no default config file found in config-defaults.yaml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x75697be30090>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from src.datasets.util.mask import PretrainType\n",
    "from torch_frame.data import DataLoader\n",
    "from torch_frame import stype\n",
    "from torch_frame.nn import (\n",
    "    EmbeddingEncoder,\n",
    "    LinearEncoder,\n",
    "    TimestampEncoder,\n",
    ")\n",
    "from torch_frame.nn.encoder.stypewise_encoder import StypeWiseFeatureEncoder\n",
    "from torch_frame import TensorFrame\n",
    "from torch_frame.data.stats import StatType\n",
    "\n",
    "from torch_geometric.utils import degree\n",
    "\n",
    "from src.datasets import IBMTransactionsAML\n",
    "from src.nn.models import TABGNN\n",
    "from src.nn.decoder import MCMHead\n",
    "from src.nn.gnn.decoder import LinkPredHead\n",
    "from src.utils.loss import SSLoss\n",
    "from src.utils.metric import SSMetric\n",
    "from src.nn.weighting.MoCo import MoCoLoss\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import wandb\n",
    "\n",
    "import logging\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,  # Set the logging level\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',  # Specify the log message format\n",
    "    datefmt='%Y-%m-%d %H:%M:%S',  # Specify the date format\n",
    "    handlers=[\n",
    "        #logging.FileHandler('app.log'),  # Log messages to a file\n",
    "        logging.StreamHandler()  # Also output log messages to the console\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create a logger\n",
    "logger = logging.getLogger(__name__)\n",
    "import sys\n",
    "import time\n",
    "\n",
    "torch.set_num_threads(4)\n",
    "\n",
    "seed = 42\n",
    "batch_size = 200\n",
    "lr = 2e-4\n",
    "eps = 1e-8\n",
    "weight_decay = 1e-3\n",
    "epochs = 50\n",
    "\n",
    "compile = False\n",
    "data_split = [0.6, 0.2, 0.2]\n",
    "split_type = 'temporal'\n",
    "\n",
    "khop_neighbors = [100, 100]\n",
    "pos_sample_prob = 1\n",
    "num_neg_samples = 64\n",
    "channels = 128\n",
    "num_layers = 3\n",
    "dropout = 0.5\n",
    "\n",
    "pretrain = {PretrainType.MASK, PretrainType.LINK_PRED}\n",
    "#pretrain = {PretrainType.LINK_PRED}\n",
    "\n",
    "testing = True\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "args = {\n",
    "    'testing': testing,\n",
    "    'batch_size': batch_size,\n",
    "    'seed': seed,\n",
    "    'device': device,\n",
    "    'lr': lr,\n",
    "    'eps': eps,\n",
    "    'epochs': epochs,\n",
    "    'compile': compile,\n",
    "    'data_split': data_split,\n",
    "    'pos_sample_prob': pos_sample_prob,\n",
    "    'channels': channels,\n",
    "    'split_type': split_type,\n",
    "    'num_neg_samples': num_neg_samples,\n",
    "    'pretrain': pretrain,\n",
    "    'khop_neighbors': khop_neighbors,\n",
    "    'num_layers': num_layers,\n",
    "    'dropout': dropout,\n",
    "    'weight_decay': weight_decay,\n",
    "}\n",
    "\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "# When running on the CuDNN backend, two further options must be set\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "# Set a fixed value for the hash seed\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "wandb.login()\n",
    "run = wandb.init(\n",
    "    dir=\"/mnt/data/\",\n",
    "    #dir=\"/scratch/takyildiz/\",\n",
    "    mode=\"disabled\" if args['testing'] else \"online\",\n",
    "    #mode=\"disabled\" if args['testing'] else \"disabled\",\n",
    "    project=f\"rel-mm-fix\", \n",
    "    name=f\"tabgnn,tab=3,noemb,lp\",\n",
    "    #group=f\"new,mcm\",\n",
    "    entity=\"cse3000\",\n",
    "    #name=f\"debug-fused\",\n",
    "    config=args\n",
    ")\n",
    "\n",
    "dataset = IBMTransactionsAML(\n",
    "    root='/mnt/data/ibm-transactions-for-anti-money-laundering-aml/HI-Small_Trans-c.csv',# if not testing else '/mnt/data/ibm-transactions-for-anti-money-laundering-aml/dummy-c.csv', \n",
    "    #root='/scratch/takyildiz/ibm-transactions-for-anti-money-laundering-aml/HI-Small_Trans-c.csv' if not testing else '/scratch/takyildiz/ibm-transactions-for-anti-money-laundering-aml/dummy-c.csv', \n",
    "    pretrain=pretrain,\n",
    "    mask_type=\"replace\",\n",
    "    split_type=split_type, \n",
    "    splits=data_split, \n",
    "    khop_neighbors=khop_neighbors\n",
    ")\n",
    "dataset.materialize()\n",
    "dataset.df.head(5)\n",
    "train_dataset, val_dataset, test_dataset = dataset.split()\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    \n",
    "g = torch.Generator()\n",
    "g.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-15 16:06:40 - INFO - num_numerical: 1\n",
      "2024-07-15 16:06:40 - INFO - num_categorical: 5\n",
      "2024-07-15 16:06:40 - INFO - num_columns: 7\n"
     ]
    }
   ],
   "source": [
    "batch_size = 200\n",
    "tensor_frame = dataset.tensor_frame\n",
    "train_loader = DataLoader(train_dataset.tensor_frame, batch_size=batch_size, shuffle=True, worker_init_fn=seed_worker, generator=g, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset.tensor_frame, batch_size=batch_size, shuffle=False, worker_init_fn=seed_worker, generator=g, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset.tensor_frame, batch_size=batch_size, shuffle=False, worker_init_fn=seed_worker, generator=g, num_workers=4)\n",
    "num_numerical = len(dataset.tensor_frame.col_names_dict[stype.numerical])\n",
    "num_categorical = len(dataset.tensor_frame.col_names_dict[stype.categorical])\n",
    "\n",
    "num_columns = num_numerical + num_categorical + 1\n",
    "logger.info(f\"num_numerical: {num_numerical}\")\n",
    "logger.info(f\"num_categorical: {num_categorical}\")\n",
    "logger.info(f\"num_columns: {num_columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-15 16:06:42 - INFO - model_params: 7451904\n",
      "2024-07-15 16:06:42 - INFO - encoder_params: 5936384\n",
      "2024-07-15 16:06:42 - INFO - lp_decoder_params: 52531\n",
      "2024-07-15 16:06:42 - INFO - mcm_decoder_params: 17837423\n",
      "2024-07-15 16:06:42 - INFO - learnable_params: 31278242\n"
     ]
    }
   ],
   "source": [
    "def train_mcm(dataset, loader, epoc: int, encoder, model, mcm_decoder, optimizer, scheduler) -> float:\n",
    "    model.train()\n",
    "    loss_accum = total_count = 0\n",
    "    loss_accum = loss_lp_accum = loss_c_accum = loss_n_accum = total_count = t_c = t_n = 0\n",
    "\n",
    "    with tqdm(loader, desc=f'Epoch {epoc}') as t:\n",
    "        for tf in t:\n",
    "            node_feats, edge_index, edge_attr, target_edge_index, target_edge_attr = mcm_inputs(tf, dataset)\n",
    "            tf = tf.to(device)\n",
    "            edge_attr, _ = encoder(edge_attr)\n",
    "            target_edge_attr, _ = encoder(target_edge_attr)\n",
    "            x, edge_attr, target_edge_attr = model(node_feats, edge_index, edge_attr, target_edge_attr)\n",
    "            x_target = x[target_edge_index.T].reshape(-1, 2 * channels)#.relu()\n",
    "            x_target = torch.cat((x_target, target_edge_attr), 1)\n",
    "            num_pred, cat_pred = mcm_decoder(x_target)\n",
    "            num_pred = num_pred.cpu()\n",
    "            cat_pred = [x.cpu() for x in cat_pred]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            t_loss, loss_c, loss_n = ssloss.mcm_loss(cat_pred, num_pred, tf.y)\n",
    "            t_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_accum += (t_loss.item() * len(tf.y))\n",
    "            total_count += len(tf.y)\n",
    "            t_c += loss_c[1]\n",
    "            t_n += loss_n[1]\n",
    "            loss_c_accum += loss_c[0].item()\n",
    "            loss_n_accum += loss_n[0].item()\n",
    "            t.set_postfix(loss=f'{loss_accum/total_count:.4f}', loss_c=f'{loss_c_accum/t_c:.4f}', loss_n=f'{loss_n_accum/t_n:.4f}')\n",
    "            wandb.log({\"train_loss\": loss_accum/total_count, \"train_loss_c\": loss_c_accum/t_c, \"train_loss_n\": loss_n_accum/t_n})\n",
    "    return {'loss': loss_accum / total_count}\n",
    "\n",
    "def train_lp(dataset, loader, epoc: int, encoder, model, lp_decoder, optimizer, scheduler) -> float:\n",
    "    encoder.train()\n",
    "    model.train()\n",
    "    lp_decoder.train()\n",
    "    total_count = 0\n",
    "    loss_lp_accum = 0\n",
    "\n",
    "    with tqdm(loader, desc=f'Epoch {epoc}') as t:\n",
    "        for tf in t:\n",
    "            batch_size = len(tf.y)\n",
    "            node_feats, edge_index, edge_attr, target_edge_index, target_edge_attr = lp_inputs(tf, dataset)\n",
    "            tf = tf.to(device)\n",
    "            edge_attr, _ = encoder(edge_attr)\n",
    "            target_edge_attr, _ = encoder(target_edge_attr)\n",
    "            x_gnn, edge_attr, target_edge_attr = model(node_feats, edge_index, edge_attr, target_edge_attr)\n",
    "            pos_edge_index = target_edge_index[:, :batch_size]\n",
    "            neg_edge_index = target_edge_index[:, batch_size:]\n",
    "            pos_edge_attr = target_edge_attr[:batch_size,:]\n",
    "            neg_edge_attr = target_edge_attr[batch_size:,:]\n",
    "            pos_pred, neg_pred = lp_decoder(x_gnn, pos_edge_index, pos_edge_attr, neg_edge_index, neg_edge_attr)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            link_loss = ssloss.lp_loss(pos_pred, neg_pred)\n",
    "            link_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_count += len(tf.y)\n",
    "            loss_lp_accum += link_loss.item() * len(tf.y)\n",
    "            t.set_postfix(loss_lp=f'{loss_lp_accum/total_count:.4f}')\n",
    "            # wandb.log({\"train_loss_lp\": loss_lp_accum/total_count})\n",
    "            #wandb.log({\"lr\": scheduler.get_last_lr()[0]})\n",
    "    return {'loss': loss_lp_accum / total_count} \n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_mcm(dataset, loader: DataLoader, encoder, model, mcm_decoder, dataset_name) -> float:\n",
    "    encoder.eval()\n",
    "    model.eval()\n",
    "    mcm_decoder.eval()\n",
    "    total_count = 0\n",
    "    accum_acc = accum_l2 = 0\n",
    "    loss_c_accum = loss_n_accum = total_count = t_c = t_n = 0\n",
    "    t_n = t_c = 0\n",
    "    with tqdm(loader, desc=f'Evaluating') as t:\n",
    "        for tf in t:\n",
    "            node_feats, edge_index, edge_attr, target_edge_index, target_edge_attr = mcm_inputs(tf, dataset)\n",
    "            tf = tf.to(device)\n",
    "            edge_attr, _ = encoder(edge_attr)\n",
    "            target_edge_attr, _ = encoder(target_edge_attr)\n",
    "            x, edge_attr, target_edge_attr = model(node_feats, edge_index, edge_attr, target_edge_attr)\n",
    "            x_target = x[target_edge_index.T].reshape(-1, 2 * channels)#.relu()\n",
    "            x_target = torch.cat((x_target, target_edge_attr), 1)\n",
    "            num_pred, cat_pred = mcm_decoder(x_target)\n",
    "            num_pred = num_pred.cpu()\n",
    "            cat_pred = [x.cpu() for x in cat_pred]\n",
    "            _, loss_c, loss_n = ssloss.mcm_loss(cat_pred, num_pred, tf.y)\n",
    "            t_c += loss_c[1]\n",
    "            t_n += loss_n[1] \n",
    "            loss_c_accum += loss_c[0].item()\n",
    "            loss_n_accum += loss_n[0].item()\n",
    "            total_count += len(num_pred)\n",
    "            for i, ans in enumerate(tf.y):\n",
    "                if ans[1] > (num_numerical-1):\n",
    "                    accum_acc += (cat_pred[int(ans[1])-num_numerical][i].argmax() == int(ans[0]))\n",
    "                else:\n",
    "                    accum_l2 += torch.square(ans[0] - num_pred[i][int(ans[1])]) #rmse\n",
    "            t.set_postfix(\n",
    "                accuracy=f'{accum_acc / t_c:.4f}',\n",
    "                rmse=f'{torch.sqrt(accum_l2 / t_n):.4f}', \n",
    "                loss_mcm=f'{(loss_c_accum/t_c) + (loss_n_accum/t_n):.4f}',\n",
    "                loss_c = f'{loss_c_accum/t_c:.4f}', \n",
    "                loss_n = f'{loss_n_accum/t_n:.4f}',\n",
    "            )\n",
    "            wandb.log({\n",
    "                f\"{dataset_name}_loss_mcm\": (loss_c_accum/t_c) + (loss_n_accum/t_n),\n",
    "                f\"{dataset_name}_loss_c\": loss_c_accum/t_c,\n",
    "                f\"{dataset_name}_loss_n\": loss_n_accum/t_n,\n",
    "            })\n",
    "        accuracy = accum_acc / t_c\n",
    "        rmse = torch.sqrt(accum_l2 / t_n)\n",
    "        wandb.log({\n",
    "            f\"{dataset_name}_accuracy\": accuracy,\n",
    "            f\"{dataset_name}_rmse\": rmse,\n",
    "        })\n",
    "        return {\"accuracy\": accuracy, \"rmse\": rmse}\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_lp(dataset, loader: DataLoader, encoder, model, lp_decoder, dataset_name) -> float:\n",
    "    encoder.eval()\n",
    "    model.eval()\n",
    "    lp_decoder.eval()\n",
    "    mrrs = []\n",
    "    hits1 = []\n",
    "    hits2 = []\n",
    "    hits5 = []\n",
    "    hits10 = []\n",
    "    loss_accum = 0\n",
    "    total_count = 0\n",
    "    loss_accum = loss_lp_accum = total_count = 0\n",
    "    with tqdm(loader, desc=f'Evaluating') as t:\n",
    "        for tf in t:\n",
    "            batch_size = len(tf.y)\n",
    "            node_feats, input_edge_index, input_edge_attr, target_edge_index, target_edge_attr = lp_inputs(tf, dataset)\n",
    "            tf = tf.to(device)\n",
    "            input_edge_attr, _ = encoder(input_edge_attr)\n",
    "            target_edge_attr, _ = encoder(target_edge_attr)\n",
    "            x_gnn, edge_attr, target_edge_attr = model(node_feats, input_edge_index, input_edge_attr, target_edge_attr)\n",
    "            pos_edge_index = target_edge_index[:, :batch_size]\n",
    "            neg_edge_index = target_edge_index[:, batch_size:]\n",
    "            pos_edge_attr = target_edge_attr[:batch_size,:]\n",
    "            neg_edge_attr = target_edge_attr[batch_size:,:]\n",
    "\n",
    "            pos_pred, neg_pred = lp_decoder(x_gnn, pos_edge_index, pos_edge_attr, neg_edge_index, neg_edge_attr)\n",
    "            loss = ssloss.lp_loss(pos_pred, neg_pred)\n",
    "            \n",
    "            loss_lp_accum += loss * len(pos_pred)\n",
    "            loss_accum += float(loss) * len(pos_pred)\n",
    "            total_count += len(pos_pred)\n",
    "            mrr_score, hits = ssmetric.mrr(pos_pred, neg_pred, [1,2,5,10], num_neg_samples)\n",
    "            mrrs.append(mrr_score)\n",
    "            hits1.append(hits['hits@1'])\n",
    "            hits2.append(hits['hits@2'])\n",
    "            hits5.append(hits['hits@5'])\n",
    "            hits10.append(hits['hits@10'])\n",
    "            t.set_postfix(\n",
    "                mrr=f'{np.mean(mrrs):.4f}',\n",
    "                hits1=f'{np.mean(hits1):.4f}',\n",
    "                hits2=f'{np.mean(hits2):.4f}',\n",
    "                hits5=f'{np.mean(hits5):.4f}',\n",
    "                hits10=f'{np.mean(hits10):.4f}',\n",
    "                loss_lp = f'{loss_lp_accum/total_count:.4f}',\n",
    "            )\n",
    "        wandb.log({\n",
    "            f\"{dataset_name}_loss_lp\": loss_lp_accum/total_count,\n",
    "        })\n",
    "        mrr_score = np.mean(mrrs)\n",
    "        hits1 = np.mean(hits1)\n",
    "        hits2 = np.mean(hits2)\n",
    "        hits5 = np.mean(hits5)\n",
    "        hits10 = np.mean(hits10)\n",
    "        wandb.log({\n",
    "            f\"{dataset_name}_mrr\": mrr_score,\n",
    "            f\"{dataset_name}_hits@1\": hits1,\n",
    "            f\"{dataset_name}_hits@2\": hits2,\n",
    "            f\"{dataset_name}_hits@5\": hits5,\n",
    "            f\"{dataset_name}_hits@10\": hits10,\n",
    "        })\n",
    "        return {\"mrr\": mrr_score, \"hits@1\": hits1, \"hits@2\": hits2, \"hits@5\": hits5, \"hits@10\": hits10}\n",
    "\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval(dataset, loader, encoder, model, lp_decoder, mcm_decoder, dataset_name):\n",
    "    encoder.eval()\n",
    "    model.eval()\n",
    "    lp_decoder.eval()\n",
    "    mcm_decoder.eval()\n",
    "    mrrs = []\n",
    "    hits1 = []\n",
    "    hits2 = []\n",
    "    hits5 = []\n",
    "    hits10 = []\n",
    "    loss_accum = 0\n",
    "    total_count = 0\n",
    "    loss_accum = loss_lp_accum = total_count = 0\n",
    "    accum_acc = accum_l2 = 0\n",
    "    loss_c_accum = loss_n_accum = t_c = t_n = 0\n",
    "    with tqdm(loader, desc=f'Evaluating') as t:\n",
    "        for tf in t:\n",
    "            batch_size = len(tf.y)\n",
    "            node_feats, input_edge_index, input_edge_attr, target_edge_index, target_edge_attr = lp_inputs(tf, dataset)\n",
    "            input_edge_attr, _ = encoder(input_edge_attr)\n",
    "            target_edge_attr, _ = encoder(target_edge_attr)\n",
    "            x_gnn, edge_attr, target_edge_attr = model(node_feats, input_edge_index, input_edge_attr, target_edge_attr)\n",
    "            pos_edge_index = target_edge_index[:, :batch_size]\n",
    "            neg_edge_index = target_edge_index[:, batch_size:]\n",
    "            pos_edge_attr = target_edge_attr[:batch_size,:]\n",
    "            neg_edge_attr = target_edge_attr[batch_size:,:]\n",
    "            pos_pred, neg_pred = lp_decoder(x_gnn, pos_edge_index, pos_edge_attr, neg_edge_index, neg_edge_attr)\n",
    "\n",
    "            x_target = x_gnn[pos_edge_index.T].reshape(-1, 2 * channels)#.relu()\n",
    "            x_target = torch.cat((x_target, pos_edge_attr), 1)\n",
    "            num_pred, cat_pred = mcm_decoder(x_target)\n",
    "            num_pred = num_pred.cpu()\n",
    "            cat_pred = [x.cpu() for x in cat_pred] \n",
    "\n",
    "            link_loss = ssloss.lp_loss(pos_pred, neg_pred)\n",
    "            t_loss, loss_c, loss_n = ssloss.mcm_loss(cat_pred, num_pred, tf.y)\n",
    "            moco_loss = mocoloss.loss([link_loss, t_loss])\n",
    "\n",
    "            loss_accum += ((link_loss.item()*moco_loss[0]+(t_loss.item()*moco_loss[1])) * len(tf.y))\n",
    "            total_count += len(tf.y)\n",
    "            t_c += loss_c[1]\n",
    "            t_n += loss_n[1]\n",
    "            loss_c_accum += loss_c[0].item()\n",
    "            loss_n_accum += loss_n[0].item()\n",
    "            loss_lp_accum += link_loss.item() * len(tf.y)\n",
    "            mrr_score, hits = ssmetric.mrr(pos_pred, neg_pred, [1,2,5,10], num_neg_samples)\n",
    "            mrrs.append(mrr_score)\n",
    "            hits1.append(hits['hits@1'])\n",
    "            hits2.append(hits['hits@2'])\n",
    "            hits5.append(hits['hits@5'])\n",
    "            hits10.append(hits['hits@10'])\n",
    "\n",
    "            for i, ans in enumerate(tf.y):\n",
    "                if ans[1] > (num_numerical-1):\n",
    "                    accum_acc += (cat_pred[int(ans[1])-num_numerical][i].argmax() == int(ans[0]))\n",
    "                else:\n",
    "                    accum_l2 += torch.square(ans[0] - num_pred[i][int(ans[1])]) #rmse\n",
    "\n",
    "            t.set_postfix(\n",
    "                mrr=f'{np.mean(mrrs):.4f}',\n",
    "                hits1=f'{np.mean(hits1):.4f}',\n",
    "                hits2=f'{np.mean(hits2):.4f}',\n",
    "                hits5=f'{np.mean(hits5):.4f}',\n",
    "                hits10=f'{np.mean(hits10):.4f}',\n",
    "                loss_lp = f'{loss_lp_accum/total_count:.4f}',\n",
    "                accuracy=f'{accum_acc / t_c:.4f}',\n",
    "                rmse=f'{torch.sqrt(accum_l2 / t_n):.4f}', \n",
    "                loss_mcm=f'{(loss_c_accum/t_c) + (loss_n_accum/t_n):.4f}',\n",
    "                loss_c = f'{loss_c_accum/t_c:.4f}', \n",
    "                loss_n = f'{loss_n_accum/t_n:.4f}'\n",
    "            )\n",
    "        mrr_score = np.mean(mrrs)\n",
    "        hits1 = np.mean(hits1)\n",
    "        hits2 = np.mean(hits2)\n",
    "        hits5 = np.mean(hits5)\n",
    "        hits10 = np.mean(hits10)\n",
    "        accuracy = accum_acc / t_c\n",
    "        rmse = torch.sqrt(accum_l2 / t_n)\n",
    "        wandb.log({\n",
    "            f\"{dataset_name}_loss_mcm\": (loss_c_accum/t_c) + (loss_n_accum/t_n),\n",
    "            f\"{dataset_name}_loss_c\": loss_c_accum/t_c,\n",
    "            f\"{dataset_name}_loss_n\": loss_n_accum/t_n,\n",
    "            f\"{dataset_name}_loss_lp\": loss_lp_accum/total_count,\n",
    "            f\"{dataset_name}_mrr\": mrr_score,\n",
    "            f\"{dataset_name}_hits@1\": hits1,\n",
    "            f\"{dataset_name}_hits@2\": hits2,\n",
    "            f\"{dataset_name}_hits@5\": hits5,\n",
    "            f\"{dataset_name}_hits@10\": hits10,\n",
    "            f\"{dataset_name}_accuracy\": accuracy,\n",
    "            f\"{dataset_name}_rmse\": rmse,\n",
    "        })\n",
    "        return {\"mrr\": mrr_score, \"hits@1\": hits1, \"hits@2\": hits2, \"hits@5\": hits5, \"hits@10\": hits10}, {\"accuracy\": accuracy, \"rmse\": rmse}\n",
    "\n",
    "num_categorical = [len(dataset.col_stats[col][StatType.COUNT][0]) for col in dataset.tensor_frame.col_names_dict[stype.categorical]] if stype.categorical in dataset.tensor_frame.col_names_dict else 0\n",
    "mcm_decoder = MCMHead(channels, num_numerical, num_categorical, w=3).to(device)\n",
    "lp_decoder = LinkPredHead(n_classes=1, n_hidden=channels, dropout=dropout).to(device)\n",
    "\n",
    "stype_encoder_dict = {\n",
    "    stype.categorical: EmbeddingEncoder(),\n",
    "    stype.numerical: LinearEncoder(),\n",
    "    stype.timestamp: TimestampEncoder(),\n",
    "}\n",
    "encoder = StypeWiseFeatureEncoder(\n",
    "            out_channels=channels,\n",
    "            col_stats=dataset.col_stats,\n",
    "            col_names_dict=dataset.tensor_frame.col_names_dict,\n",
    "            stype_encoder_dict=stype_encoder_dict,\n",
    ").to(device)\n",
    "\n",
    "edge_index = dataset.train_graph.edge_index\n",
    "num_nodes = dataset.train_graph.num_nodes\n",
    "in_degrees = degree(edge_index[1], num_nodes=num_nodes, dtype=torch.long)\n",
    "max_in_degree = int(in_degrees.max())\n",
    "in_degree_histogram = torch.zeros(max_in_degree + 1, dtype=torch.long)\n",
    "in_degree_histogram += torch.bincount(in_degrees, minlength=in_degree_histogram.numel())\n",
    "model = TABGNN(\n",
    "    encoder=encoder,\n",
    "    channels=channels,\n",
    "    edge_dim=channels*dataset.tensor_frame.num_cols,\n",
    "    num_layers=num_layers, \n",
    "    dropout=dropout,\n",
    "    deg=in_degree_histogram\n",
    ")\n",
    "model = torch.compile(model, dynamic=True) if compile else model\n",
    "model.to(device)\n",
    "\n",
    "model_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "encoder_params = sum(p.numel() for p in encoder.parameters() if p.requires_grad)\n",
    "lp_decoder_params = sum(p.numel() for p in lp_decoder.parameters() if p.requires_grad)\n",
    "mcm_decoder_params = sum(p.numel() for p in mcm_decoder.parameters() if p.requires_grad)\n",
    "logger.info(f\"model_params: {model_params}\")\n",
    "logger.info(f\"encoder_params: {encoder_params}\")\n",
    "logger.info(f\"lp_decoder_params: {lp_decoder_params}\")\n",
    "logger.info(f\"mcm_decoder_params: {mcm_decoder_params}\")\n",
    "learnable_params = model_params + encoder_params + lp_decoder_params + mcm_decoder_params\n",
    "logger.info(f\"learnable_params: {learnable_params}\")\n",
    "# wandb.log({\"learnable_params\": learnable_params})\n",
    "\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    # {'params': [p for n, p in encoder.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': weight_decay},\n",
    "    # {'params': [p for n, p in encoder.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': weight_decay},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n",
    "    {'params': [p for n, p in mcm_decoder.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': weight_decay},\n",
    "    {'params': [p for n, p in mcm_decoder.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n",
    "    {'params': [p for n, p in lp_decoder.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': weight_decay},\n",
    "    {'params': [p for n, p in lp_decoder.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=lr, eps=eps)\n",
    "scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=lr/3, max_lr=3*lr, step_size_up=2000, cycle_momentum=False)\n",
    "ssloss = SSLoss(device, num_numerical)\n",
    "ssmetric = SSMetric(device)\n",
    "mocoloss = MoCoLoss(model, 2, device, beta=0.999, beta_sigma=0.1, gamma=0.999, gamma_sigma=0.1, rho=0.05)\n",
    "\n",
    "save_dir = '/mnt/data/.cache/saved_models'\n",
    "#save_dir = '/scratch/takyildiz/.cache/saved_models'\n",
    "# run_id = wandb.run.id\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "best_lp = 0\n",
    "best_acc = 0\n",
    "best_rmse = 2\n",
    "\n",
    "\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mcm_inputs(tf: TensorFrame, dataset):\n",
    "    batch_size = len(tf.y)\n",
    "    edges = tf.y[:,-3:]\n",
    "    khop_source, khop_destination, idx = dataset.sample_neighbors(edges, 'train')\n",
    "\n",
    "    edge_attr = dataset.tensor_frame.__getitem__(idx)\n",
    "\n",
    "    nodes = torch.unique(torch.cat([khop_source, khop_destination]))\n",
    "    num_nodes = nodes.shape[0]\n",
    "    node_feats = torch.ones(num_nodes).view(-1,num_nodes).t()\n",
    "\n",
    "    n_id_map = {value.item(): index for index, value in enumerate(nodes)}\n",
    "    local_khop_source = torch.tensor([n_id_map[node.item()] for node in khop_source], dtype=torch.long)\n",
    "    local_khop_destination = torch.tensor([n_id_map[node.item()] for node in khop_destination], dtype=torch.long)\n",
    "    edge_index = torch.cat((local_khop_source.unsqueeze(0), local_khop_destination.unsqueeze(0)))\n",
    "\n",
    "    drop_edge_ind = torch.tensor([x for x in range(batch_size)])\n",
    "    mask = torch.zeros((edge_index.shape[1],)).long() #[E, ]\n",
    "    mask = mask.index_fill_(dim=0, index=drop_edge_ind, value=1).bool() #[E, ]\n",
    "    # input_edge_index = edge_index[:, ~mask]\n",
    "    # input_edge_attr  = edge_attr[~mask]\n",
    "    input_edge_index = edge_index\n",
    "    input_edge_attr  = edge_attr\n",
    "    target_edge_index = edge_index[:, mask]\n",
    "    target_edge_attr  = edge_attr[mask]\n",
    "    return node_feats.to(device), input_edge_index.to(device), input_edge_attr.to(device), target_edge_index.to(device), target_edge_attr.to(device)  \n",
    "\n",
    "def lp_inputs(tf: TensorFrame, dataset):\n",
    "    edges = tf.y[:,-3:]\n",
    "    batch_size = len(edges)\n",
    "    start = time.time()\n",
    "    khop_source, khop_destination, idx = dataset.sample_neighbors(edges, 'train')\n",
    "    # logger.info(f\"sample_neighbors time: {time.time()-start}\")\n",
    "    \n",
    "    edge_attr = dataset.tensor_frame.__getitem__(idx)\n",
    "\n",
    "    nodes = torch.unique(torch.cat([khop_source, khop_destination]))\n",
    "    num_nodes = nodes.shape[0]\n",
    "    node_feats = torch.ones(num_nodes).view(-1,num_nodes).t()\n",
    "\n",
    "    start = time.time()\n",
    "    import numpy as np\n",
    "\n",
    "    n_id_map = {value.item(): index for index, value in enumerate(nodes)}\n",
    "    vectorized_map = np.vectorize(lambda x: n_id_map[x])\n",
    "\n",
    "    khop_combined = torch.cat((khop_source, khop_destination))\n",
    "    local_khop_combined = torch.LongTensor(vectorized_map(khop_combined.numpy()))\n",
    "\n",
    "    local_khop_source, local_khop_destination = local_khop_combined.split(khop_source.size(0))\n",
    "    edge_index = torch.stack((local_khop_source, local_khop_destination))\n",
    "    # n_id_map = {value.item(): index for index, value in enumerate(nodes)}\n",
    "    # local_khop_source = torch.tensor([n_id_map[node.item()] for node in khop_source], dtype=torch.long)\n",
    "    # local_khop_destination = torch.tensor([n_id_map[node.item()] for node in khop_destination], dtype=torch.long)\n",
    "    # edge_index = torch.cat((local_khop_source.unsqueeze(0), local_khop_destination.unsqueeze(0)))\n",
    "    #edge_index = torch.stack((local_khop_source, local_khop_destination))\n",
    "    # logger.info(f\"new_edge_index time: {time.time()-start}\")\n",
    "    start = time.time()\n",
    "\n",
    "    # drop_edge_ind = torch.tensor([x for x in range(int(batch_size))])\n",
    "    # mask = torch.zeros((edge_index.shape[1],)).long() #[E, ]\n",
    "    # mask = mask.index_fill_(dim=0, index=drop_edge_ind, value=1).bool() #[E, ]\n",
    "    input_edge_index = edge_index[:, batch_size:]\n",
    "    input_edge_attr  = edge_attr[batch_size:]\n",
    "\n",
    "    pos_edge_index = edge_index[:, :batch_size]\n",
    "    pos_edge_attr  = edge_attr[:batch_size]\n",
    "    #logger.info(f\"sample_neighbors time: {time.time()-start}\")\n",
    "\n",
    "    #start = time.time()\n",
    "    # generate/sample negative edges\n",
    "    # neg_edges = []\n",
    "    target_dict = pos_edge_attr.feat_dict\n",
    "    for key, value in pos_edge_attr.feat_dict.items():\n",
    "        attr = []\n",
    "        # duplicate each row of the tensor by num_neg_samples times repeated values must be contiguous\n",
    "        for r in value:\n",
    "            if key == stype.timestamp:\n",
    "                attr.append(r.repeat(num_neg_samples, 1, 1))\n",
    "            else:\n",
    "                attr.append(r.repeat(num_neg_samples, 1))\n",
    "        target_dict[key] = torch.cat([target_dict[key], torch.cat(attr, dim=0)], dim=0)\n",
    "    target_edge_attr = TensorFrame(target_dict, pos_edge_attr.col_names_dict)\n",
    "    # logger.info(f\"target_edge_attr time: {time.time()-start}\")\n",
    "\n",
    "    start = time.time()\n",
    "    # nodeset = set(range(edge_index.max()+1))\n",
    "    # # logger.info(f\"edge_index: {edge_index}\")\n",
    "    # # logger.info(f\"pos_edge_index: {pos_edge_index}\")\n",
    "    # # # Write to a file\n",
    "    # # import json\n",
    "    # # with open('edge_index.json', 'w') as f:\n",
    "    # #     json.dump(edge_index.tolist(), f)\n",
    "    # # with open('pos_edge_index.json', 'w') as f:\n",
    "    # #     json.dump(pos_edge_index.tolist(), f)\n",
    "    # # sys.exit()\n",
    "    # for i, edge in enumerate(pos_edge_index.t()):\n",
    "    #     src, dst = edge[0], edge[1]\n",
    "\n",
    "    #     # # # Chose negative examples in a smart way\n",
    "    #     # unavail_mask = (edge_index == src).any(dim=0) | (edge_index == dst).any(dim=0)\n",
    "    #     # unavail_nodes = torch.unique(edge_index[:, unavail_mask])\n",
    "    #     # unavail_nodes = set(unavail_nodes.tolist())\n",
    "    #     # avail_nodes = nodeset - unavail_nodes\n",
    "    #     # avail_nodes = torch.tensor(list(avail_nodes))\n",
    "    #     # # Finally, emmulate np.random.choice() to chose randomly amongst available nodes\n",
    "    #     # indices = torch.randperm(len(avail_nodes))[:num_neg_samples]\n",
    "    #     # neg_nodes = avail_nodes[indices]\n",
    "        \n",
    "    #     # # Create a mask of unavailable nodes\n",
    "    #     # unavail_mask = torch.isin(edge_index.flatten(), torch.tensor([src, dst])).view(edge_index.shape).any(dim=0)\n",
    "        \n",
    "    #     # # Get unique unavailable nodes\n",
    "    #     # unavail_nodes = torch.unique(edge_index[:, unavail_mask])\n",
    "        \n",
    "    #     # # Convert to set for fast set operations\n",
    "    #     # unavail_nodes_set = set(unavail_nodes.tolist())\n",
    "        \n",
    "    #     # # Determine available nodes by set difference\n",
    "    #     # avail_nodes = list(nodeset - unavail_nodes_set)\n",
    "        \n",
    "    #     # # Convert available nodes back to tensor\n",
    "    #     # avail_nodes = torch.tensor(avail_nodes, dtype=torch.long)\n",
    "        \n",
    "    #     # # Randomly select negative samples from available nodes\n",
    "    #     # indices = torch.randperm(len(avail_nodes))[:num_neg_samples]\n",
    "    #     # neg_nodes = avail_nodes[indices]\n",
    "\n",
    "    #     # Create a mask of unavailable nodes\n",
    "    #     unavail_mask = torch.isin(edge_index.flatten(), torch.tensor([src, dst]))\n",
    "    #     unavail_nodes = edge_index.flatten()[unavail_mask].unique()\n",
    "\n",
    "    #     # Create a mask for all nodes\n",
    "    #     all_nodes = torch.arange(max(nodeset) + 1)\n",
    "    #     avail_mask = ~torch.isin(all_nodes, unavail_nodes)\n",
    "\n",
    "    #     # Get available nodes\n",
    "    #     avail_nodes = all_nodes[avail_mask]\n",
    "\n",
    "    #     # Randomly select negative samples from available nodes\n",
    "    #     neg_nodes = avail_nodes[torch.randint(high=len(avail_nodes), size=(num_neg_samples,))]\n",
    "\n",
    "    #     # Generate num_neg_samples/2 negative edges with the same source but different destinations\n",
    "    #     num_neg_samples_half = int(num_neg_samples/2)\n",
    "    #     neg_dsts = neg_nodes[:num_neg_samples_half]  # Selecting num_neg_samples/2 random destination nodes for the source\n",
    "    #     neg_edges_src = torch.stack([src.repeat(num_neg_samples_half), neg_dsts], dim=0)\n",
    "        \n",
    "    #     # Generate num_neg_samples/2 negative edges with the same destination but different sources\n",
    "    #     neg_srcs = neg_nodes[num_neg_samples_half:]  # Selecting num_neg_samples/2 random source nodes for the destination\n",
    "    #     neg_edges_dst = torch.stack([neg_srcs, dst.repeat(num_neg_samples_half)], dim=0)\n",
    "\n",
    "    #     # Add these negative edges to the list\n",
    "    #     neg_edges.append(neg_edges_src)\n",
    "    #     neg_edges.append(neg_edges_dst)\n",
    "    \n",
    "    input_edge_index = input_edge_index.to(device)\n",
    "    input_edge_attr = input_edge_attr.to(device)\n",
    "    #pos_edge_index = pos_edge_index.to(device)\n",
    "    #pos_edge_attr = pos_edge_attr.to(device)\n",
    "    node_feats = node_feats.to(device)\n",
    "    \n",
    "    # if len(neg_edges) > 0:\n",
    "    #     #neg_edge_index = torch.cat(neg_edges, dim=1).to(device)\n",
    "    #     neg_edge_index = torch.cat(neg_edges, dim=1)\n",
    "\n",
    "    neg_edge_index = negative_sampling.generate_negative_samples(edge_index.tolist(), pos_edge_index.tolist(), num_neg_samples)\n",
    "    neg_edge_index = torch.tensor(neg_edge_index, dtype=torch.long)\n",
    "\n",
    "    target_edge_index = torch.cat([pos_edge_index, neg_edge_index], dim=1).to(device)\n",
    "    target_edge_attr = target_edge_attr.to(device)\n",
    "    #logger.info(f\"negative edges: {time.time()-start}\")\n",
    "    #sys.exit()\n",
    "    return node_feats, input_edge_index, input_edge_attr, target_edge_index, target_edge_attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, loader, epoc: int, encoder, model, lp_decoder, mcm_decoder, optimizer, scheduler):\n",
    "    encoder.train()\n",
    "    model.train()\n",
    "    lp_decoder.train()\n",
    "    mcm_decoder.train()\n",
    "    loss_accum = total_count = 0\n",
    "    loss_lp_accum = loss_c_accum = loss_n_accum = total_count = t_c = t_n = 0\n",
    "    count = 0\n",
    "    ave = 0\n",
    "    with tqdm(loader, desc=f'Epoch {epoc}') as t:\n",
    "        for tf in t:\n",
    "            ss = time.time()\n",
    "            batch_size = len(tf.y)\n",
    "            start = time.time()\n",
    "            node_feats, input_edge_index, input_edge_attr, target_edge_index, target_edge_attr = lp_inputs(tf, dataset)\n",
    "            l_time = time.time()-start\n",
    "            logger.info(f\"lp_inputs time: {l_time}\")\n",
    "            start = time.time()\n",
    "            input_edge_attr, _ = encoder(input_edge_attr)\n",
    "            target_edge_attr, _ = encoder(target_edge_attr)\n",
    "            x_gnn, edge_attr, target_edge_attr = model(node_feats, input_edge_index, input_edge_attr, target_edge_attr)\n",
    "            pos_edge_index = target_edge_index[:, :batch_size]\n",
    "            neg_edge_index = target_edge_index[:, batch_size:]\n",
    "            pos_edge_attr = target_edge_attr[:batch_size,:]\n",
    "            neg_edge_attr = target_edge_attr[batch_size:,:]\n",
    "            pos_pred, neg_pred = lp_decoder(x_gnn, pos_edge_index, pos_edge_attr, neg_edge_index, neg_edge_attr)\n",
    "\n",
    "            x_target = x_gnn[pos_edge_index.T].reshape(-1, 2 * channels)#.relu()\n",
    "            x_target = torch.cat((x_target, pos_edge_attr), 1)\n",
    "            num_pred, cat_pred = mcm_decoder(x_target)\n",
    "            num_pred = num_pred.cpu()\n",
    "            cat_pred = [x.cpu() for x in cat_pred]\n",
    "            m_time = time.time()-start\n",
    "            logger.info(f\"model time: {m_time}\")\n",
    "            \n",
    "            start = time.time()\n",
    "            optimizer.zero_grad()\n",
    "            link_loss = ssloss.lp_loss(pos_pred, neg_pred)\n",
    "            t_loss, loss_c, loss_n = ssloss.mcm_loss(cat_pred, num_pred, tf.y)\n",
    "            loss = link_loss + t_loss\n",
    "            loss.backward()\n",
    "            #moco_loss = mocoloss.loss([link_loss, t_loss])\n",
    "            optimizer.step()\n",
    "            # scheduler.step()\n",
    "\n",
    "            #loss_accum += ((link_loss.item()*moco_loss[0]+(t_loss.item()*moco_loss[1])) * len(tf.y))\n",
    "            total_count += len(tf.y)\n",
    "            t_c += loss_c[1]\n",
    "            t_n += loss_n[1]\n",
    "            loss_c_accum += loss_c[0].item()\n",
    "            loss_n_accum += loss_n[0].item()\n",
    "            loss_lp_accum += link_loss.item() * len(tf.y)\n",
    "            b_time = time.time()-start\n",
    "            logger.info(f\"backward time: {b_time}\")\n",
    "            t.set_postfix(loss=f'{loss_accum/total_count:.4f}', loss_lp=f'{loss_lp_accum/total_count:.4f}', loss_c=f'{loss_c_accum/t_c:.4f}', loss_n=f'{loss_n_accum/t_n:.4f}')#, moco_loss=f'{moco_loss[0]:.4f},{moco_loss[1]:.4f}')\n",
    "            logger.info(f\"total time: {b_time+m_time+l_time}\")\n",
    "            ave += b_time+m_time+l_time\n",
    "            # logger.info('-----------------')\n",
    "            if count == 100:\n",
    "                logger.info(f\"ave time: {ave/count}\")\n",
    "                break\n",
    "            count += 1\n",
    "    return {'loss': loss_accum / total_count} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1332c571ce5d47a5b8245fb417b9e5a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 0:   0%|          | 0/8123 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-15 15:49:14 - INFO - lp_inputs time: 0.10088920593261719\n",
      "2024-07-15 15:49:14 - INFO - model time: 0.07967281341552734\n",
      "2024-07-15 15:49:14 - INFO - backward time: 0.06083202362060547\n",
      "2024-07-15 15:49:14 - INFO - total time: 0.24139404296875\n",
      "2024-07-15 15:49:14 - INFO - lp_inputs time: 0.03855013847351074\n",
      "2024-07-15 15:49:14 - INFO - model time: 0.06864619255065918\n",
      "2024-07-15 15:49:14 - INFO - backward time: 0.05961966514587402\n",
      "2024-07-15 15:49:14 - INFO - total time: 0.16681599617004395\n",
      "2024-07-15 15:49:14 - INFO - lp_inputs time: 0.03721189498901367\n",
      "2024-07-15 15:49:14 - INFO - model time: 0.0684654712677002\n",
      "2024-07-15 15:49:15 - INFO - backward time: 0.059671878814697266\n",
      "2024-07-15 15:49:15 - INFO - total time: 0.16534924507141113\n",
      "2024-07-15 15:49:15 - INFO - lp_inputs time: 0.03784775733947754\n",
      "2024-07-15 15:49:15 - INFO - model time: 0.06939411163330078\n",
      "2024-07-15 15:49:15 - INFO - backward time: 0.06009030342102051\n",
      "2024-07-15 15:49:15 - INFO - total time: 0.16733217239379883\n",
      "2024-07-15 15:49:15 - INFO - lp_inputs time: 0.03741264343261719\n",
      "2024-07-15 15:49:15 - INFO - model time: 0.06865572929382324\n",
      "2024-07-15 15:49:15 - INFO - backward time: 0.05949139595031738\n",
      "2024-07-15 15:49:15 - INFO - total time: 0.1655597686767578\n",
      "2024-07-15 15:49:15 - INFO - lp_inputs time: 0.03717398643493652\n",
      "2024-07-15 15:49:15 - INFO - model time: 0.06816434860229492\n",
      "2024-07-15 15:49:15 - INFO - backward time: 0.05870866775512695\n",
      "2024-07-15 15:49:15 - INFO - total time: 0.1640470027923584\n",
      "2024-07-15 15:49:15 - INFO - lp_inputs time: 0.03863859176635742\n",
      "2024-07-15 15:49:15 - INFO - model time: 0.06965947151184082\n",
      "2024-07-15 15:49:15 - INFO - backward time: 0.06035041809082031\n",
      "2024-07-15 15:49:15 - INFO - total time: 0.16864848136901855\n",
      "2024-07-15 15:49:15 - INFO - lp_inputs time: 0.03749895095825195\n",
      "2024-07-15 15:49:15 - INFO - model time: 0.06869244575500488\n",
      "2024-07-15 15:49:15 - INFO - backward time: 0.059108734130859375\n",
      "2024-07-15 15:49:15 - INFO - total time: 0.1653001308441162\n",
      "2024-07-15 15:49:15 - INFO - lp_inputs time: 0.03760099411010742\n",
      "2024-07-15 15:49:15 - INFO - model time: 0.06839227676391602\n",
      "2024-07-15 15:49:16 - INFO - backward time: 0.060151100158691406\n",
      "2024-07-15 15:49:16 - INFO - total time: 0.16614437103271484\n",
      "2024-07-15 15:49:16 - INFO - lp_inputs time: 0.037195682525634766\n",
      "2024-07-15 15:49:16 - INFO - model time: 0.06850886344909668\n",
      "2024-07-15 15:49:16 - INFO - backward time: 0.05923914909362793\n",
      "2024-07-15 15:49:16 - INFO - total time: 0.16494369506835938\n",
      "2024-07-15 15:49:16 - INFO - lp_inputs time: 0.03791451454162598\n",
      "2024-07-15 15:49:16 - INFO - model time: 0.0691385269165039\n",
      "2024-07-15 15:49:16 - INFO - backward time: 0.05978107452392578\n",
      "2024-07-15 15:49:16 - INFO - total time: 0.16683411598205566\n",
      "2024-07-15 15:49:16 - INFO - lp_inputs time: 0.03839516639709473\n",
      "2024-07-15 15:49:16 - INFO - model time: 0.06896591186523438\n",
      "2024-07-15 15:49:16 - INFO - backward time: 0.0597381591796875\n",
      "2024-07-15 15:49:16 - INFO - total time: 0.1670992374420166\n",
      "2024-07-15 15:49:16 - INFO - lp_inputs time: 0.037015676498413086\n",
      "2024-07-15 15:49:16 - INFO - model time: 0.06788754463195801\n",
      "2024-07-15 15:49:16 - INFO - backward time: 0.05855226516723633\n",
      "2024-07-15 15:49:16 - INFO - total time: 0.16345548629760742\n",
      "2024-07-15 15:49:16 - INFO - lp_inputs time: 0.03724813461303711\n",
      "2024-07-15 15:49:16 - INFO - model time: 0.06853175163269043\n",
      "2024-07-15 15:49:16 - INFO - backward time: 0.0594325065612793\n",
      "2024-07-15 15:49:16 - INFO - total time: 0.16521239280700684\n",
      "2024-07-15 15:49:16 - INFO - lp_inputs time: 0.03712892532348633\n",
      "2024-07-15 15:49:16 - INFO - model time: 0.06833171844482422\n",
      "2024-07-15 15:49:17 - INFO - backward time: 0.058817148208618164\n",
      "2024-07-15 15:49:17 - INFO - total time: 0.1642777919769287\n",
      "2024-07-15 15:49:17 - INFO - lp_inputs time: 0.03825545310974121\n",
      "2024-07-15 15:49:17 - INFO - model time: 0.06824827194213867\n",
      "2024-07-15 15:49:17 - INFO - backward time: 0.059273719787597656\n",
      "2024-07-15 15:49:17 - INFO - total time: 0.16577744483947754\n",
      "2024-07-15 15:49:17 - INFO - lp_inputs time: 0.037337541580200195\n",
      "2024-07-15 15:49:17 - INFO - model time: 0.06876444816589355\n",
      "2024-07-15 15:49:17 - INFO - backward time: 0.05925297737121582\n",
      "2024-07-15 15:49:17 - INFO - total time: 0.16535496711730957\n",
      "2024-07-15 15:49:17 - INFO - lp_inputs time: 0.037015438079833984\n",
      "2024-07-15 15:49:17 - INFO - model time: 0.06842613220214844\n",
      "2024-07-15 15:49:17 - INFO - backward time: 0.05942559242248535\n",
      "2024-07-15 15:49:17 - INFO - total time: 0.16486716270446777\n",
      "2024-07-15 15:49:17 - INFO - lp_inputs time: 0.03910422325134277\n",
      "2024-07-15 15:49:17 - INFO - model time: 0.07238459587097168\n",
      "2024-07-15 15:49:17 - INFO - backward time: 0.06124687194824219\n",
      "2024-07-15 15:49:17 - INFO - total time: 0.17273569107055664\n",
      "2024-07-15 15:49:17 - INFO - lp_inputs time: 0.0381622314453125\n",
      "2024-07-15 15:49:17 - INFO - model time: 0.0691537857055664\n",
      "2024-07-15 15:49:17 - INFO - backward time: 0.05988121032714844\n",
      "2024-07-15 15:49:17 - INFO - total time: 0.16719722747802734\n",
      "2024-07-15 15:49:17 - INFO - lp_inputs time: 0.038779258728027344\n",
      "2024-07-15 15:49:18 - INFO - model time: 0.06948351860046387\n",
      "2024-07-15 15:49:18 - INFO - backward time: 0.06054258346557617\n",
      "2024-07-15 15:49:18 - INFO - total time: 0.16880536079406738\n",
      "2024-07-15 15:49:18 - INFO - lp_inputs time: 0.0381011962890625\n",
      "2024-07-15 15:49:18 - INFO - model time: 0.0692591667175293\n",
      "2024-07-15 15:49:18 - INFO - backward time: 0.06055283546447754\n",
      "2024-07-15 15:49:18 - INFO - total time: 0.16791319847106934\n",
      "2024-07-15 15:49:18 - INFO - lp_inputs time: 0.03874993324279785\n",
      "2024-07-15 15:49:18 - INFO - model time: 0.06955981254577637\n",
      "2024-07-15 15:49:18 - INFO - backward time: 0.06023907661437988\n",
      "2024-07-15 15:49:18 - INFO - total time: 0.1685488224029541\n",
      "2024-07-15 15:49:18 - INFO - lp_inputs time: 0.03776097297668457\n",
      "2024-07-15 15:49:18 - INFO - model time: 0.0707705020904541\n",
      "2024-07-15 15:49:18 - INFO - backward time: 0.05950570106506348\n",
      "2024-07-15 15:49:18 - INFO - total time: 0.16803717613220215\n",
      "2024-07-15 15:49:18 - INFO - lp_inputs time: 0.03848075866699219\n",
      "2024-07-15 15:49:18 - INFO - model time: 0.06914496421813965\n",
      "2024-07-15 15:49:18 - INFO - backward time: 0.06014394760131836\n",
      "2024-07-15 15:49:18 - INFO - total time: 0.1677696704864502\n",
      "2024-07-15 15:49:18 - INFO - lp_inputs time: 0.03729701042175293\n",
      "2024-07-15 15:49:18 - INFO - model time: 0.06786489486694336\n",
      "2024-07-15 15:49:18 - INFO - backward time: 0.05840706825256348\n",
      "2024-07-15 15:49:18 - INFO - total time: 0.16356897354125977\n",
      "2024-07-15 15:49:18 - INFO - lp_inputs time: 0.0398101806640625\n",
      "2024-07-15 15:49:19 - INFO - model time: 0.07030367851257324\n",
      "2024-07-15 15:49:19 - INFO - backward time: 0.06086087226867676\n",
      "2024-07-15 15:49:19 - INFO - total time: 0.1709747314453125\n",
      "2024-07-15 15:49:19 - INFO - lp_inputs time: 0.03827214241027832\n",
      "2024-07-15 15:49:19 - INFO - model time: 0.06910538673400879\n",
      "2024-07-15 15:49:19 - INFO - backward time: 0.05989503860473633\n",
      "2024-07-15 15:49:19 - INFO - total time: 0.16727256774902344\n",
      "2024-07-15 15:49:19 - INFO - lp_inputs time: 0.04045295715332031\n",
      "2024-07-15 15:49:19 - INFO - model time: 0.07253146171569824\n",
      "2024-07-15 15:49:19 - INFO - backward time: 0.06104564666748047\n",
      "2024-07-15 15:49:19 - INFO - total time: 0.17403006553649902\n",
      "2024-07-15 15:49:19 - INFO - lp_inputs time: 0.03836488723754883\n",
      "2024-07-15 15:49:19 - INFO - model time: 0.06911420822143555\n",
      "2024-07-15 15:49:19 - INFO - backward time: 0.059600114822387695\n",
      "2024-07-15 15:49:19 - INFO - total time: 0.16707921028137207\n",
      "2024-07-15 15:49:19 - INFO - lp_inputs time: 0.0385899543762207\n",
      "2024-07-15 15:49:19 - INFO - model time: 0.06929683685302734\n",
      "2024-07-15 15:49:19 - INFO - backward time: 0.0600438117980957\n",
      "2024-07-15 15:49:19 - INFO - total time: 0.16793060302734375\n",
      "2024-07-15 15:49:19 - INFO - lp_inputs time: 0.03828787803649902\n",
      "2024-07-15 15:49:19 - INFO - model time: 0.06915903091430664\n",
      "2024-07-15 15:49:19 - INFO - backward time: 0.05994224548339844\n",
      "2024-07-15 15:49:19 - INFO - total time: 0.1673891544342041\n",
      "2024-07-15 15:49:19 - INFO - lp_inputs time: 0.037459611892700195\n",
      "2024-07-15 15:49:20 - INFO - model time: 0.0683891773223877\n",
      "2024-07-15 15:49:20 - INFO - backward time: 0.059679269790649414\n",
      "2024-07-15 15:49:20 - INFO - total time: 0.1655280590057373\n",
      "2024-07-15 15:49:20 - INFO - lp_inputs time: 0.040778398513793945\n",
      "2024-07-15 15:49:20 - INFO - model time: 0.06977653503417969\n",
      "2024-07-15 15:49:20 - INFO - backward time: 0.06050467491149902\n",
      "2024-07-15 15:49:20 - INFO - total time: 0.17105960845947266\n",
      "2024-07-15 15:49:20 - INFO - lp_inputs time: 0.03652024269104004\n",
      "2024-07-15 15:49:20 - INFO - model time: 0.06780648231506348\n",
      "2024-07-15 15:49:20 - INFO - backward time: 0.05831146240234375\n",
      "2024-07-15 15:49:20 - INFO - total time: 0.16263818740844727\n",
      "2024-07-15 15:49:20 - INFO - lp_inputs time: 0.0381467342376709\n",
      "2024-07-15 15:49:20 - INFO - model time: 0.0682532787322998\n",
      "2024-07-15 15:49:20 - INFO - backward time: 0.05866742134094238\n",
      "2024-07-15 15:49:20 - INFO - total time: 0.16506743431091309\n",
      "2024-07-15 15:49:20 - INFO - lp_inputs time: 0.039276123046875\n",
      "2024-07-15 15:49:20 - INFO - model time: 0.06957077980041504\n",
      "2024-07-15 15:49:20 - INFO - backward time: 0.060169219970703125\n",
      "2024-07-15 15:49:20 - INFO - total time: 0.16901612281799316\n",
      "2024-07-15 15:49:20 - INFO - lp_inputs time: 0.03823351860046387\n",
      "2024-07-15 15:49:20 - INFO - model time: 0.06833720207214355\n",
      "2024-07-15 15:49:20 - INFO - backward time: 0.0590817928314209\n",
      "2024-07-15 15:49:20 - INFO - total time: 0.16565251350402832\n",
      "2024-07-15 15:49:21 - INFO - lp_inputs time: 0.03741121292114258\n",
      "2024-07-15 15:49:21 - INFO - model time: 0.06841063499450684\n",
      "2024-07-15 15:49:21 - INFO - backward time: 0.0594632625579834\n",
      "2024-07-15 15:49:21 - INFO - total time: 0.1652851104736328\n",
      "2024-07-15 15:49:21 - INFO - lp_inputs time: 0.03852558135986328\n",
      "2024-07-15 15:49:21 - INFO - model time: 0.06922221183776855\n",
      "2024-07-15 15:49:21 - INFO - backward time: 0.05990910530090332\n",
      "2024-07-15 15:49:21 - INFO - total time: 0.16765689849853516\n",
      "2024-07-15 15:49:21 - INFO - lp_inputs time: 0.03975558280944824\n",
      "2024-07-15 15:49:21 - INFO - model time: 0.07038140296936035\n",
      "2024-07-15 15:49:21 - INFO - backward time: 0.06101512908935547\n",
      "2024-07-15 15:49:21 - INFO - total time: 0.17115211486816406\n",
      "2024-07-15 15:49:21 - INFO - lp_inputs time: 0.03776240348815918\n",
      "2024-07-15 15:49:21 - INFO - model time: 0.06827950477600098\n",
      "2024-07-15 15:49:21 - INFO - backward time: 0.059111833572387695\n",
      "2024-07-15 15:49:21 - INFO - total time: 0.16515374183654785\n",
      "2024-07-15 15:49:21 - INFO - lp_inputs time: 0.037776947021484375\n",
      "2024-07-15 15:49:21 - INFO - model time: 0.06849861145019531\n",
      "2024-07-15 15:49:21 - INFO - backward time: 0.05939984321594238\n",
      "2024-07-15 15:49:21 - INFO - total time: 0.16567540168762207\n",
      "2024-07-15 15:49:21 - INFO - lp_inputs time: 0.0379939079284668\n",
      "2024-07-15 15:49:21 - INFO - model time: 0.06890106201171875\n",
      "2024-07-15 15:49:21 - INFO - backward time: 0.05954265594482422\n",
      "2024-07-15 15:49:21 - INFO - total time: 0.16643762588500977\n",
      "2024-07-15 15:49:22 - INFO - lp_inputs time: 0.04077506065368652\n",
      "2024-07-15 15:49:22 - INFO - model time: 0.0711977481842041\n",
      "2024-07-15 15:49:22 - INFO - backward time: 0.062174081802368164\n",
      "2024-07-15 15:49:22 - INFO - total time: 0.1741468906402588\n",
      "2024-07-15 15:49:22 - INFO - lp_inputs time: 0.037616729736328125\n",
      "2024-07-15 15:49:22 - INFO - model time: 0.06725215911865234\n",
      "2024-07-15 15:49:22 - INFO - backward time: 0.05825161933898926\n",
      "2024-07-15 15:49:22 - INFO - total time: 0.16312050819396973\n",
      "2024-07-15 15:49:22 - INFO - lp_inputs time: 0.03780078887939453\n",
      "2024-07-15 15:49:22 - INFO - model time: 0.06832313537597656\n",
      "2024-07-15 15:49:22 - INFO - backward time: 0.058869123458862305\n",
      "2024-07-15 15:49:22 - INFO - total time: 0.1649930477142334\n",
      "2024-07-15 15:49:22 - INFO - lp_inputs time: 0.0381474494934082\n",
      "2024-07-15 15:49:22 - INFO - model time: 0.0706944465637207\n",
      "2024-07-15 15:49:22 - INFO - backward time: 0.05953335762023926\n",
      "2024-07-15 15:49:22 - INFO - total time: 0.16837525367736816\n",
      "2024-07-15 15:49:22 - INFO - lp_inputs time: 0.03700876235961914\n",
      "2024-07-15 15:49:22 - INFO - model time: 0.06770849227905273\n",
      "2024-07-15 15:49:22 - INFO - backward time: 0.05827188491821289\n",
      "2024-07-15 15:49:22 - INFO - total time: 0.16298913955688477\n",
      "2024-07-15 15:49:22 - INFO - lp_inputs time: 0.03900909423828125\n",
      "2024-07-15 15:49:22 - INFO - model time: 0.06966066360473633\n",
      "2024-07-15 15:49:23 - INFO - backward time: 0.060689449310302734\n",
      "2024-07-15 15:49:23 - INFO - total time: 0.1693592071533203\n",
      "2024-07-15 15:49:23 - INFO - lp_inputs time: 0.03673553466796875\n",
      "2024-07-15 15:49:23 - INFO - model time: 0.06791138648986816\n",
      "2024-07-15 15:49:23 - INFO - backward time: 0.0582890510559082\n",
      "2024-07-15 15:49:23 - INFO - total time: 0.16293597221374512\n",
      "2024-07-15 15:49:23 - INFO - lp_inputs time: 0.03821873664855957\n",
      "2024-07-15 15:49:23 - INFO - model time: 0.06862473487854004\n",
      "2024-07-15 15:49:23 - INFO - backward time: 0.05930304527282715\n",
      "2024-07-15 15:49:23 - INFO - total time: 0.16614651679992676\n",
      "2024-07-15 15:49:23 - INFO - lp_inputs time: 0.03873085975646973\n",
      "2024-07-15 15:49:23 - INFO - model time: 0.06909370422363281\n",
      "2024-07-15 15:49:23 - INFO - backward time: 0.0597081184387207\n",
      "2024-07-15 15:49:23 - INFO - total time: 0.16753268241882324\n",
      "2024-07-15 15:49:23 - INFO - lp_inputs time: 0.03789710998535156\n",
      "2024-07-15 15:49:23 - INFO - model time: 0.06817054748535156\n",
      "2024-07-15 15:49:23 - INFO - backward time: 0.05890321731567383\n",
      "2024-07-15 15:49:23 - INFO - total time: 0.16497087478637695\n",
      "2024-07-15 15:49:23 - INFO - lp_inputs time: 0.04037308692932129\n",
      "2024-07-15 15:49:23 - INFO - model time: 0.07264852523803711\n",
      "2024-07-15 15:49:23 - INFO - backward time: 0.06138300895690918\n",
      "2024-07-15 15:49:23 - INFO - total time: 0.17440462112426758\n",
      "2024-07-15 15:49:23 - INFO - lp_inputs time: 0.03855729103088379\n",
      "2024-07-15 15:49:23 - INFO - model time: 0.06892848014831543\n",
      "2024-07-15 15:49:24 - INFO - backward time: 0.05987906455993652\n",
      "2024-07-15 15:49:24 - INFO - total time: 0.16736483573913574\n",
      "2024-07-15 15:49:24 - INFO - lp_inputs time: 0.038993120193481445\n",
      "2024-07-15 15:49:24 - INFO - model time: 0.06952834129333496\n",
      "2024-07-15 15:49:24 - INFO - backward time: 0.06043744087219238\n",
      "2024-07-15 15:49:24 - INFO - total time: 0.1689589023590088\n",
      "2024-07-15 15:49:24 - INFO - lp_inputs time: 0.038452863693237305\n",
      "2024-07-15 15:49:24 - INFO - model time: 0.06889772415161133\n",
      "2024-07-15 15:49:24 - INFO - backward time: 0.0603177547454834\n",
      "2024-07-15 15:49:24 - INFO - total time: 0.16766834259033203\n",
      "2024-07-15 15:49:24 - INFO - lp_inputs time: 0.0381312370300293\n",
      "2024-07-15 15:49:24 - INFO - model time: 0.06812834739685059\n",
      "2024-07-15 15:49:24 - INFO - backward time: 0.058968305587768555\n",
      "2024-07-15 15:49:24 - INFO - total time: 0.16522789001464844\n",
      "2024-07-15 15:49:24 - INFO - lp_inputs time: 0.03698992729187012\n",
      "2024-07-15 15:49:24 - INFO - model time: 0.06807518005371094\n",
      "2024-07-15 15:49:24 - INFO - backward time: 0.05895280838012695\n",
      "2024-07-15 15:49:24 - INFO - total time: 0.164017915725708\n",
      "2024-07-15 15:49:24 - INFO - lp_inputs time: 0.03927803039550781\n",
      "2024-07-15 15:49:24 - INFO - model time: 0.06976890563964844\n",
      "2024-07-15 15:49:24 - INFO - backward time: 0.06104564666748047\n",
      "2024-07-15 15:49:24 - INFO - total time: 0.17009258270263672\n",
      "2024-07-15 15:49:24 - INFO - lp_inputs time: 0.03808474540710449\n",
      "2024-07-15 15:49:24 - INFO - model time: 0.06885981559753418\n",
      "2024-07-15 15:49:25 - INFO - backward time: 0.05949807167053223\n",
      "2024-07-15 15:49:25 - INFO - total time: 0.1664426326751709\n",
      "2024-07-15 15:49:25 - INFO - lp_inputs time: 0.03868842124938965\n",
      "2024-07-15 15:49:25 - INFO - model time: 0.06935501098632812\n",
      "2024-07-15 15:49:25 - INFO - backward time: 0.05993795394897461\n",
      "2024-07-15 15:49:25 - INFO - total time: 0.16798138618469238\n",
      "2024-07-15 15:49:25 - INFO - lp_inputs time: 0.03829669952392578\n",
      "2024-07-15 15:49:25 - INFO - model time: 0.06933307647705078\n",
      "2024-07-15 15:49:25 - INFO - backward time: 0.059594154357910156\n",
      "2024-07-15 15:49:25 - INFO - total time: 0.16722393035888672\n",
      "2024-07-15 15:49:25 - INFO - lp_inputs time: 0.03931570053100586\n",
      "2024-07-15 15:49:25 - INFO - model time: 0.06938338279724121\n",
      "2024-07-15 15:49:25 - INFO - backward time: 0.06010556221008301\n",
      "2024-07-15 15:49:25 - INFO - total time: 0.16880464553833008\n",
      "2024-07-15 15:49:25 - INFO - lp_inputs time: 0.03948044776916504\n",
      "2024-07-15 15:49:25 - INFO - model time: 0.07006454467773438\n",
      "2024-07-15 15:49:25 - INFO - backward time: 0.06089019775390625\n",
      "2024-07-15 15:49:25 - INFO - total time: 0.17043519020080566\n",
      "2024-07-15 15:49:25 - INFO - lp_inputs time: 0.03919863700866699\n",
      "2024-07-15 15:49:25 - INFO - model time: 0.07011175155639648\n",
      "2024-07-15 15:49:25 - INFO - backward time: 0.0607447624206543\n",
      "2024-07-15 15:49:25 - INFO - total time: 0.17005515098571777\n",
      "2024-07-15 15:49:25 - INFO - lp_inputs time: 0.03735828399658203\n",
      "2024-07-15 15:49:26 - INFO - model time: 0.06837105751037598\n",
      "2024-07-15 15:49:26 - INFO - backward time: 0.058904170989990234\n",
      "2024-07-15 15:49:26 - INFO - total time: 0.16463351249694824\n",
      "2024-07-15 15:49:26 - INFO - lp_inputs time: 0.03927016258239746\n",
      "2024-07-15 15:49:26 - INFO - model time: 0.06938028335571289\n",
      "2024-07-15 15:49:26 - INFO - backward time: 0.06040215492248535\n",
      "2024-07-15 15:49:26 - INFO - total time: 0.1690526008605957\n",
      "2024-07-15 15:49:26 - INFO - lp_inputs time: 0.0374903678894043\n",
      "2024-07-15 15:49:26 - INFO - model time: 0.06857633590698242\n",
      "2024-07-15 15:49:26 - INFO - backward time: 0.059580087661743164\n",
      "2024-07-15 15:49:26 - INFO - total time: 0.16564679145812988\n",
      "2024-07-15 15:49:26 - INFO - lp_inputs time: 0.037024497985839844\n",
      "2024-07-15 15:49:26 - INFO - model time: 0.06849074363708496\n",
      "2024-07-15 15:49:26 - INFO - backward time: 0.05954313278198242\n",
      "2024-07-15 15:49:26 - INFO - total time: 0.16505837440490723\n",
      "2024-07-15 15:49:26 - INFO - lp_inputs time: 0.037019968032836914\n",
      "2024-07-15 15:49:26 - INFO - model time: 0.0679483413696289\n",
      "2024-07-15 15:49:26 - INFO - backward time: 0.05885171890258789\n",
      "2024-07-15 15:49:26 - INFO - total time: 0.1638200283050537\n",
      "2024-07-15 15:49:26 - INFO - lp_inputs time: 0.03821706771850586\n",
      "2024-07-15 15:49:26 - INFO - model time: 0.06913590431213379\n",
      "2024-07-15 15:49:26 - INFO - backward time: 0.06017780303955078\n",
      "2024-07-15 15:49:26 - INFO - total time: 0.16753077507019043\n",
      "2024-07-15 15:49:26 - INFO - lp_inputs time: 0.03754544258117676\n",
      "2024-07-15 15:49:27 - INFO - model time: 0.06865191459655762\n",
      "2024-07-15 15:49:27 - INFO - backward time: 0.059541940689086914\n",
      "2024-07-15 15:49:27 - INFO - total time: 0.1657392978668213\n",
      "2024-07-15 15:49:27 - INFO - lp_inputs time: 0.03809165954589844\n",
      "2024-07-15 15:49:27 - INFO - model time: 0.0708627700805664\n",
      "2024-07-15 15:49:27 - INFO - backward time: 0.05960798263549805\n",
      "2024-07-15 15:49:27 - INFO - total time: 0.1685624122619629\n",
      "2024-07-15 15:49:27 - INFO - lp_inputs time: 0.03817939758300781\n",
      "2024-07-15 15:49:27 - INFO - model time: 0.06932950019836426\n",
      "2024-07-15 15:49:27 - INFO - backward time: 0.06013154983520508\n",
      "2024-07-15 15:49:27 - INFO - total time: 0.16764044761657715\n",
      "2024-07-15 15:49:27 - INFO - lp_inputs time: 0.03710055351257324\n",
      "2024-07-15 15:49:27 - INFO - model time: 0.06807756423950195\n",
      "2024-07-15 15:49:27 - INFO - backward time: 0.058746337890625\n",
      "2024-07-15 15:49:27 - INFO - total time: 0.1639244556427002\n",
      "2024-07-15 15:49:27 - INFO - lp_inputs time: 0.037661075592041016\n",
      "2024-07-15 15:49:27 - INFO - model time: 0.0687723159790039\n",
      "2024-07-15 15:49:27 - INFO - backward time: 0.05943465232849121\n",
      "2024-07-15 15:49:27 - INFO - total time: 0.16586804389953613\n",
      "2024-07-15 15:49:27 - INFO - lp_inputs time: 0.037639617919921875\n",
      "2024-07-15 15:49:27 - INFO - model time: 0.06796669960021973\n",
      "2024-07-15 15:49:27 - INFO - backward time: 0.05888080596923828\n",
      "2024-07-15 15:49:27 - INFO - total time: 0.16448712348937988\n",
      "2024-07-15 15:49:27 - INFO - lp_inputs time: 0.039521217346191406\n",
      "2024-07-15 15:49:28 - INFO - model time: 0.07174396514892578\n",
      "2024-07-15 15:49:28 - INFO - backward time: 0.06080770492553711\n",
      "2024-07-15 15:49:28 - INFO - total time: 0.1720728874206543\n",
      "2024-07-15 15:49:28 - INFO - lp_inputs time: 0.037740230560302734\n",
      "2024-07-15 15:49:28 - INFO - model time: 0.06865286827087402\n",
      "2024-07-15 15:49:28 - INFO - backward time: 0.059098243713378906\n",
      "2024-07-15 15:49:28 - INFO - total time: 0.16549134254455566\n",
      "2024-07-15 15:49:28 - INFO - lp_inputs time: 0.03782844543457031\n",
      "2024-07-15 15:49:28 - INFO - model time: 0.06882548332214355\n",
      "2024-07-15 15:49:28 - INFO - backward time: 0.05999946594238281\n",
      "2024-07-15 15:49:28 - INFO - total time: 0.16665339469909668\n",
      "2024-07-15 15:49:28 - INFO - lp_inputs time: 0.04004788398742676\n",
      "2024-07-15 15:49:28 - INFO - model time: 0.07003116607666016\n",
      "2024-07-15 15:49:28 - INFO - backward time: 0.06088876724243164\n",
      "2024-07-15 15:49:28 - INFO - total time: 0.17096781730651855\n",
      "2024-07-15 15:49:28 - INFO - lp_inputs time: 0.03753209114074707\n",
      "2024-07-15 15:49:28 - INFO - model time: 0.06785106658935547\n",
      "2024-07-15 15:49:28 - INFO - backward time: 0.058809518814086914\n",
      "2024-07-15 15:49:28 - INFO - total time: 0.16419267654418945\n",
      "2024-07-15 15:49:28 - INFO - lp_inputs time: 0.0384519100189209\n",
      "2024-07-15 15:49:28 - INFO - model time: 0.06932902336120605\n",
      "2024-07-15 15:49:28 - INFO - backward time: 0.060059547424316406\n",
      "2024-07-15 15:49:28 - INFO - total time: 0.16784048080444336\n",
      "2024-07-15 15:49:29 - INFO - lp_inputs time: 0.0390162467956543\n",
      "2024-07-15 15:49:29 - INFO - model time: 0.06983280181884766\n",
      "2024-07-15 15:49:29 - INFO - backward time: 0.06053304672241211\n",
      "2024-07-15 15:49:29 - INFO - total time: 0.16938209533691406\n",
      "2024-07-15 15:49:29 - INFO - lp_inputs time: 0.03944587707519531\n",
      "2024-07-15 15:49:29 - INFO - model time: 0.07037472724914551\n",
      "2024-07-15 15:49:29 - INFO - backward time: 0.061147451400756836\n",
      "2024-07-15 15:49:29 - INFO - total time: 0.17096805572509766\n",
      "2024-07-15 15:49:29 - INFO - lp_inputs time: 0.03880786895751953\n",
      "2024-07-15 15:49:29 - INFO - model time: 0.06919574737548828\n",
      "2024-07-15 15:49:29 - INFO - backward time: 0.06005716323852539\n",
      "2024-07-15 15:49:29 - INFO - total time: 0.1680607795715332\n",
      "2024-07-15 15:49:29 - INFO - lp_inputs time: 0.03856515884399414\n",
      "2024-07-15 15:49:29 - INFO - model time: 0.06878137588500977\n",
      "2024-07-15 15:49:29 - INFO - backward time: 0.059461116790771484\n",
      "2024-07-15 15:49:29 - INFO - total time: 0.1668076515197754\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7c9bdabe4790>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/miniconda3/envs/rel-mm/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/miniconda3/envs/rel-mm/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1443, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/opt/miniconda3/envs/rel-mm/lib/python3.10/multiprocessing/process.py\", line 149, in join\n",
      "    res = self._popen.wait(timeout)\n",
      "  File \"/opt/miniconda3/envs/rel-mm/lib/python3.10/multiprocessing/popen_fork.py\", line 40, in wait\n",
      "    if not wait([self.sentinel], timeout):\n",
      "  File \"/opt/miniconda3/envs/rel-mm/lib/python3.10/multiprocessing/connection.py\", line 931, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/opt/miniconda3/envs/rel-mm/lib/python3.10/selectors.py\", line 416, in select\n",
      "    fd_event_list = self._selector.poll(timeout)\n",
      "KeyboardInterrupt: \n"
     ]
    }
   ],
   "source": [
    "loss = train(dataset, train_loader, 0, encoder, model, lp_decoder, mcm_decoder, optimizer, scheduler)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rel-mm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
