{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "from torch_frame.data import DataLoader\n",
    "from torch_geometric.data import Data\n",
    "from transformers import get_inverse_sqrt_schedule\n",
    "\n",
    "from src.datasets import IBMTransactionsAML\n",
    "from src.nn.gnn.model import GINe\n",
    "from src.utils.loss import lp_loss\n",
    "from src.utils.metric import mrr\n",
    "\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "from icecream import ic\n",
    "import sys\n",
    "\n",
    "torch.set_float32_matmul_precision('high')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "batch_size = 1024\n",
    "lr = 5e-4\n",
    "eps = 1e-8\n",
    "epochs = 5\n",
    "\n",
    "compile = False\n",
    "data_split = [0.6, 0.2, 0.2]\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "args = {\n",
    "    'testing': True,\n",
    "    'batch_size': batch_size,\n",
    "    'seed': seed,\n",
    "    'device': device,\n",
    "    'lr': lr,\n",
    "    'eps': eps,\n",
    "    'epochs': epochs,\n",
    "    'compile': compile,\n",
    "    'data_split': data_split\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login()\n",
    "run = wandb.init(\n",
    "    mode=\"disabled\" if args['testing'] else \"online\",\n",
    "    project=f\"rel-mm\", \n",
    "    name=\"model=GINe,dataset=IBM-AML_Hi_Sm,objective=lp\", \n",
    "    config=args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "# When running on the CuDNN backend, two further options must be set\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "# Set a fixed value for the hash seed\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| dataset: IBMTransactionsAML()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>From Bank</th>\n",
       "      <th>From ID</th>\n",
       "      <th>To Bank</th>\n",
       "      <th>To ID</th>\n",
       "      <th>Amount Received</th>\n",
       "      <th>Receiving Currency</th>\n",
       "      <th>Amount Paid</th>\n",
       "      <th>Payment Currency</th>\n",
       "      <th>Payment Format</th>\n",
       "      <th>Is Laundering</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1200</td>\n",
       "      <td>B_10</td>\n",
       "      <td>8000EBD30</td>\n",
       "      <td>B_10</td>\n",
       "      <td>8000EBD30</td>\n",
       "      <td>3.533720e-09</td>\n",
       "      <td>US Dollar</td>\n",
       "      <td>3.533720e-09</td>\n",
       "      <td>US Dollar</td>\n",
       "      <td>Reinvestment</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1200</td>\n",
       "      <td>B_3208</td>\n",
       "      <td>8000F4580</td>\n",
       "      <td>B_1</td>\n",
       "      <td>8000F5340</td>\n",
       "      <td>9.556511e-15</td>\n",
       "      <td>US Dollar</td>\n",
       "      <td>9.556511e-15</td>\n",
       "      <td>US Dollar</td>\n",
       "      <td>Cheque</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>B_3209</td>\n",
       "      <td>8000F4670</td>\n",
       "      <td>B_3209</td>\n",
       "      <td>8000F4670</td>\n",
       "      <td>1.402613e-08</td>\n",
       "      <td>US Dollar</td>\n",
       "      <td>1.402613e-08</td>\n",
       "      <td>US Dollar</td>\n",
       "      <td>Reinvestment</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>120</td>\n",
       "      <td>B_12</td>\n",
       "      <td>8000F5030</td>\n",
       "      <td>B_12</td>\n",
       "      <td>8000F5030</td>\n",
       "      <td>2.682752e-09</td>\n",
       "      <td>US Dollar</td>\n",
       "      <td>2.682752e-09</td>\n",
       "      <td>US Dollar</td>\n",
       "      <td>Reinvestment</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>360</td>\n",
       "      <td>B_10</td>\n",
       "      <td>8000F5200</td>\n",
       "      <td>B_10</td>\n",
       "      <td>8000F5200</td>\n",
       "      <td>3.505963e-08</td>\n",
       "      <td>US Dollar</td>\n",
       "      <td>3.505963e-08</td>\n",
       "      <td>US Dollar</td>\n",
       "      <td>Reinvestment</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Timestamp From Bank    From ID To Bank      To ID  Amount Received  \\\n",
       "0       1200      B_10  8000EBD30    B_10  8000EBD30     3.533720e-09   \n",
       "1       1200    B_3208  8000F4580     B_1  8000F5340     9.556511e-15   \n",
       "2          0    B_3209  8000F4670  B_3209  8000F4670     1.402613e-08   \n",
       "3        120      B_12  8000F5030    B_12  8000F5030     2.682752e-09   \n",
       "4        360      B_10  8000F5200    B_10  8000F5200     3.505963e-08   \n",
       "\n",
       "  Receiving Currency   Amount Paid Payment Currency Payment Format  \\\n",
       "0          US Dollar  3.533720e-09        US Dollar   Reinvestment   \n",
       "1          US Dollar  9.556511e-15        US Dollar         Cheque   \n",
       "2          US Dollar  1.402613e-08        US Dollar   Reinvestment   \n",
       "3          US Dollar  2.682752e-09        US Dollar   Reinvestment   \n",
       "4          US Dollar  3.505963e-08        US Dollar   Reinvestment   \n",
       "\n",
       "  Is Laundering  split  \n",
       "0             0      0  \n",
       "1             0      0  \n",
       "2             0      0  \n",
       "3             0      0  \n",
       "4             0      0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = IBMTransactionsAML(root='/mnt/data/ibm-transactions-for-anti-money-laundering-aml/dummy.csv')\n",
    "#dataset = IBMTransactionsAML(root='/mnt/data/ibm-transactions-for-anti-money-laundering-aml/HI-Small_Trans-cleaned.csv', pretrain=pretrain, split_type='temporal', splits=data_split)\n",
    "ic(dataset)\n",
    "dataset.materialize()\n",
    "dataset.df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, val_dataset, test_dataset = dataset.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tensor_frame = train_dataset.tensor_frame\n",
    "train_loader = DataLoader(train_tensor_frame, batch_size=batch_size, shuffle=True)\n",
    "val_tensor_frame = val_dataset.tensor_frame\n",
    "val_loader = DataLoader(val_tensor_frame, batch_size=batch_size, shuffle=True)\n",
    "test_tensor_frame = test_dataset.tensor_frame\n",
    "test_loader = DataLoader(test_tensor_frame, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| num_nodes: 298015\n",
      "ic| edge_index.shape: torch.Size([2, 499843])\n"
     ]
    }
   ],
   "source": [
    "# TODO: generalize the trainable columns\n",
    "source = train_tensor_frame.get_col_feat('From ID')\n",
    "destination = train_tensor_frame.get_col_feat('To ID')\n",
    "\n",
    "#create dummy node features\n",
    "num_nodes = np.unique(np.concatenate([source, destination])).shape[0]\n",
    "ic(num_nodes)\n",
    "node_feat = torch.ones(num_nodes)\n",
    "\n",
    "edge_index = torch.cat([source, destination], dim=1).t()\n",
    "ic(edge_index.shape)\n",
    "g = Data(node_feat, edge_index=edge_index, edge_attr=train_tensor_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| learnable_params: 125177\n"
     ]
    }
   ],
   "source": [
    "model = GINe(num_features=1, num_gnn_layers=3, edge_dim=train_dataset.tensor_frame.num_cols-3)\n",
    "model = torch.compile(model, dynamic=True) if compile else model\n",
    "model.to(args['device'])\n",
    "learnable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "ic(learnable_params)\n",
    "wandb.log({\"learnable_params\": learnable_params})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gnn_inputs(batch):\n",
    "    source = batch.get_col_feat('From ID')\n",
    "    destination = batch.get_col_feat('To ID')\n",
    "    #ic(source, destination)\n",
    "    feat_cols = train_dataset.feat_cols\n",
    "\n",
    "    # TODO: generalize the trainable columns\n",
    "    feat_cols.remove('Timestamp')\n",
    "    feat_cols.remove('From ID')\n",
    "    feat_cols.remove('To ID')\n",
    "\n",
    "    # TODO: fix, a very crude approach\n",
    "    feats = [batch.get_col_feat(col_name) for col_name in feat_cols]\n",
    "    edge_attr = torch.cat(feats, dim=1)\n",
    "    nodes = torch.unique(torch.cat([source, destination]))\n",
    "    num_nodes = nodes.shape[0]\n",
    "\n",
    "    n_id_map = {value.item(): index for index, value in enumerate(nodes)}\n",
    "    local_source = torch.tensor([n_id_map[node.item()] for node in source], dtype=torch.long)\n",
    "    local_destination = torch.tensor([n_id_map[node.item()] for node in destination], dtype=torch.long)\n",
    "    edge_index = torch.cat((local_source.unsqueeze(0), local_destination.unsqueeze(0)))\n",
    "    node_feats = torch.ones(num_nodes).view(-1,num_nodes).t()\n",
    "\n",
    "    # TODO: could choose false negatives, the entire graph is not used\n",
    "    neg_edges = []\n",
    "    neg_edge_attr = []\n",
    "    nodeset = set(range(edge_index.max()+1))\n",
    "    for i, edge in enumerate(edge_index.t()):\n",
    "        src, dst = edge[0], edge[1]\n",
    "\n",
    "        # Chose negative examples in a smart way\n",
    "        unavail_mask = (edge_index == src).any(dim=0) | (edge_index == dst).any(dim=0)\n",
    "        unavail_nodes = torch.unique(edge_index[:, unavail_mask])\n",
    "        unavail_nodes = set(unavail_nodes.tolist())\n",
    "        avail_nodes = nodeset - unavail_nodes\n",
    "        avail_nodes = torch.tensor(list(avail_nodes))\n",
    "        # Finally, emmulate np.random.choice() to chose randomly amongst available nodes\n",
    "        indices = torch.randperm(len(avail_nodes))[:64]\n",
    "        neg_nodes = avail_nodes[indices]\n",
    "        \n",
    "        # Generate 32 negative edges with the same source but different destinations\n",
    "        neg_dsts = neg_nodes[:32]  # Selecting 32 random destination nodes for the source\n",
    "        neg_edges_src = torch.stack([src.repeat(32), neg_dsts], dim=0)\n",
    "        \n",
    "        # Generate 32 negative edges with the same destination but different sources\n",
    "        neg_srcs = neg_nodes[32:]  # Selecting 32 random source nodes for the destination\n",
    "        neg_edges_dst = torch.stack([neg_srcs, dst.repeat(32)], dim=0)\n",
    "\n",
    "        # Add these negative edges to the list\n",
    "        neg_edges.append(neg_edges_src)\n",
    "        neg_edges.append(neg_edges_dst)\n",
    "        # Replicate the positive edge attribute for each of the negative edges generated from this edge\n",
    "        pos_attr = edge_attr[i].unsqueeze(0)  # Get the attribute of the current positive edge\n",
    "        replicated_attr = pos_attr.repeat(64, 1)  # Replicate it 64 times (for each negative edge)\n",
    "        neg_edge_attr.append(replicated_attr)\n",
    "    \n",
    "    edge_index = edge_index.to(device)\n",
    "    edge_attr = edge_attr.to(device)\n",
    "    node_feats = node_feats.to(device)\n",
    "    neg_edge_index = torch.cat(neg_edges, dim=1).to(device)\n",
    "    neg_edge_attr = torch.cat(neg_edge_attr, dim=0).to(device)\n",
    "    return edge_index, edge_attr, node_feats, neg_edge_index, neg_edge_attr\n",
    "# batch = next(iter(train_loader))\n",
    "# edge_index, edge_attr, node_feats, neg_edge_index, neg_edge_attr = get_gnn_inputs(batch)\n",
    "# ic(edge_index, edge_attr, node_feats, neg_edge_index, neg_edge_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoc: int, model, optimizer) -> float:\n",
    "    model.train()\n",
    "    loss_accum = total_count = 0\n",
    "\n",
    "    with tqdm(train_loader, desc=f'Epoch {epoc}') as t:\n",
    "        for tf in t:\n",
    "            tf = tf.to(device)\n",
    "            edge_index, edge_attr, node_feats, neg_edge_index, neg_edge_attr = get_gnn_inputs(tf)\n",
    "            pred = model(node_feats, edge_index, edge_attr)\n",
    "            neg_pred = model(node_feats, neg_edge_index, neg_edge_attr)\n",
    "            loss = lp_loss(pred, neg_pred)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_accum += float(loss) * len(pred)\n",
    "            total_count += len(pred)\n",
    "            t.set_postfix(loss=f'{loss_accum/total_count:.4f}')\n",
    "            del pred\n",
    "            del tf\n",
    "        wandb.log({\"train_loss\": loss_accum/total_count})\n",
    "    return loss_accum / total_count\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(loader: DataLoader, model, dataset_name) -> float:\n",
    "    model.eval()\n",
    "    accum_acc = 0\n",
    "    mrrs = []\n",
    "    hits1 = []\n",
    "    hits2 = []\n",
    "    hits5 = []\n",
    "    hits10 = []\n",
    "    loss_accum = 0\n",
    "    total_count = 0\n",
    "    with tqdm(loader, desc=f'Evaluating') as t:\n",
    "        for tf in t:\n",
    "            tf = tf.to(device)\n",
    "            edge_index, edge_attr, node_feats, neg_edge_index, neg_edge_attr = get_gnn_inputs(tf)\n",
    "            pred = model(node_feats, edge_index, edge_attr)\n",
    "            neg_pred = model(node_feats, neg_edge_index, neg_edge_attr)\n",
    "            mrr_score, hits = mrr(pred, neg_pred, [1,2,5,10])\n",
    "            mrrs.append(mrr_score)\n",
    "            hits1.append(hits['hits@1'])\n",
    "            hits2.append(hits['hits@2'])\n",
    "            hits5.append(hits['hits@5'])\n",
    "            hits10.append(hits['hits@10'])\n",
    "            loss = lp_loss(pred, neg_pred)\n",
    "            loss_accum += float(loss) * len(pred)\n",
    "            accum_acc += pred.sum().item()\n",
    "            accum_acc += len(neg_pred) - neg_pred.sum().item()\n",
    "            total_count += len(pred) + len(neg_pred)\n",
    "            t.set_postfix(\n",
    "                accuracy=f'{accum_acc/total_count:.4f}',\n",
    "                loss=f'{loss_accum/total_count:.4f}',\n",
    "                mrr=f'{mrr_score:.4f}',\n",
    "                hits1=f'{hits[\"hits@1\"]:.4f}',\n",
    "                hits2=f'{hits[\"hits@2\"]:.4f}',\n",
    "                hits5=f'{hits[\"hits@5\"]:.4f}',\n",
    "                hits10=f'{hits[\"hits@10\"]:.4f}'\n",
    "            )\n",
    "        mrr_score = np.mean(mrrs)\n",
    "        hits1 = np.mean(hits1)\n",
    "        hits2 = np.mean(hits2)\n",
    "        hits5 = np.mean(hits5)\n",
    "        hits10 = np.mean(hits10)\n",
    "        wandb.log({\n",
    "            f\"{dataset_name}_accuracy\": accum_acc/total_count, \n",
    "            f\"{dataset_name}_loss\": loss_accum/total_count,\n",
    "            f\"{dataset_name}_mrr\": mrr,\n",
    "            f\"{dataset_name}_hits@1\": hits1,\n",
    "            f\"{dataset_name}_hits@2\": hits2,\n",
    "            f\"{dataset_name}_hits@5\": hits5,\n",
    "            f\"{dataset_name}_hits@10\": hits10\n",
    "        })\n",
    "        del tf\n",
    "        del pred\n",
    "        accuracy = accum_acc / total_count\n",
    "        return accuracy, mrr_score, hits1, hits2, hits5, hits10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| learnable_params: 125151\n",
      "Evaluating:   0%|                                                                                                                                                                                                       | 0/489 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 489/489 [01:53<00:00,  4.29it/s, accuracy=0.0387, hits1=0.0000, hits10=0.0992, hits2=0.0000, hits5=0.0000, loss=0.3702, mrr=0.0267]\n",
      "Evaluating: 100%|█████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 127.12it/s, accuracy=0.0220, hits1=0.0000, hits10=0.0492, hits2=0.0000, hits5=0.0000, loss=0.4090, mrr=0.0218]\n",
      "Evaluating: 100%|██████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 91.07it/s, accuracy=0.0232, hits1=0.0000, hits10=0.0632, hits2=0.0000, hits5=0.0000, loss=0.4177, mrr=0.0238]\n",
      "Epoch 1: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 489/489 [01:52<00:00,  4.35it/s, loss=1.4193]\n",
      "Evaluating: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 489/489 [01:54<00:00,  4.27it/s, accuracy=0.6576, hits1=0.0458, hits10=0.3206, hits2=0.0458, hits5=0.0611, loss=0.0438, mrr=0.1040]\n",
      "Evaluating: 100%|█████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 127.96it/s, accuracy=0.6176, hits1=0.0656, hits10=0.2787, hits2=0.0656, hits5=0.0656, loss=0.0292, mrr=0.1190]\n",
      "Evaluating: 100%|██████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 91.77it/s, accuracy=0.7188, hits1=0.1789, hits10=0.4947, hits2=0.2000, hits5=0.3053, loss=0.0270, mrr=0.2614]\n",
      "ic| train_loss: 1.4192870558223416\n",
      "    train_metric: (0.6575925453119981,\n",
      "                   0.10529010214634354,\n",
      "                   0.04342587863336924,\n",
      "                   0.0516497730607721,\n",
      "                   0.07961387707523532,\n",
      "                   0.2611142505102718)\n",
      "    val_metric: (0.6176294430207184,\n",
      "                 0.11902657526966086,\n",
      "                 0.06557377049180328,\n",
      "                 0.06557377049180328,\n",
      "                 0.06557377049180328,\n",
      "                 0.2786885245901639)\n",
      "    test_metric: (0.7188231729978491,\n",
      "                  0.2613523760205327,\n",
      "                  0.17894736842105263,\n",
      "                  0.2,\n",
      "                  0.30526315789473685,\n",
      "                  0.49473684210526314)\n",
      "Epoch 2: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 489/489 [01:51<00:00,  4.38it/s, loss=0.8511]\n",
      "Evaluating: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 489/489 [01:52<00:00,  4.34it/s, accuracy=0.9281, hits1=0.1908, hits10=0.6336, hits2=0.2137, hits5=0.4122, loss=0.1320, mrr=0.3048]\n",
      "Evaluating: 100%|█████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 130.52it/s, accuracy=0.9518, hits1=0.2951, hits10=0.6230, hits2=0.2951, hits5=0.3934, loss=0.0969, mrr=0.3748]\n",
      "Evaluating: 100%|██████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 92.60it/s, accuracy=0.9410, hits1=0.2526, hits10=0.8000, hits2=0.4000, hits5=0.5684, loss=0.0968, mrr=0.4175]\n",
      "ic| train_loss: 0.8510926832285952\n",
      "    train_metric: (0.9281113695920753,\n",
      "                   0.1994098285617755,\n",
      "                   0.0820440708224059,\n",
      "                   0.12191428263397805,\n",
      "                   0.28230665833938245,\n",
      "                   0.5224765023747638)\n",
      "    val_metric: (0.9518392073401587,\n",
      "                 0.3747639425432147,\n",
      "                 0.29508196721311475,\n",
      "                 0.29508196721311475,\n",
      "                 0.39344262295081966,\n",
      "                 0.6229508196721312)\n",
      "    test_metric: (0.9409887760177797,\n",
      "                  0.41745007647339394,\n",
      "                  0.25263157894736843,\n",
      "                  0.4,\n",
      "                  0.5684210526315789,\n",
      "                  0.8)\n",
      "Epoch 3:  43%|█████████████████████████████████████████████████████████████████████████████▏                                                                                                     | 211/489 [00:48<01:03,  4.41it/s, loss=0.5002]"
     ]
    }
   ],
   "source": [
    "model = GINe(num_features=1, num_gnn_layers=3, edge_dim=train_dataset.tensor_frame.num_cols-3, n_classes=1)\n",
    "model = torch.compile(model, dynamic=True) if compile else model\n",
    "model.to(device)\n",
    "learnable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "ic(learnable_params)\n",
    "wandb.log({\"learnable_params\": learnable_params})\n",
    "\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ]\n",
    "optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=lr, eps=eps)\n",
    "scheduler = get_inverse_sqrt_schedule(optimizer, num_warmup_steps=0, timescale=1000)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "train_metric = test(train_loader, model, \"train\")\n",
    "val_metric = test(val_loader, model, \"val\")\n",
    "test_metric = test(test_loader, model, \"test\")\n",
    "ic(\n",
    "        train_metric, \n",
    "        val_metric, \n",
    "        test_metric\n",
    ")\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train_loss = train(epoch, model, optimizer)\n",
    "    train_metric = test(train_loader, model, \"train\")\n",
    "    val_metric = test(val_loader, model, \"val\")\n",
    "    test_metric = test(test_loader, model, \"test\")\n",
    "    ic(\n",
    "        train_loss, \n",
    "        train_metric, \n",
    "        val_metric, \n",
    "        test_metric\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
