{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from torch_frame.data import DataLoader\n",
    "from torch_frame import TensorFrame\n",
    "from torch_frame.nn.encoder.stypewise_encoder import StypeWiseFeatureEncoder\n",
    "from torch_frame import stype\n",
    "from torch_frame.nn import (\n",
    "    EmbeddingEncoder,\n",
    "    LinearEncoder,\n",
    "    TimestampEncoder,\n",
    ")\n",
    "from torch_geometric.data import Data\n",
    "from transformers import get_inverse_sqrt_schedule\n",
    "\n",
    "from src.datasets import IBMTransactionsAML\n",
    "from src.nn.gnn.model import GINe\n",
    "from src.utils.loss import lp_loss\n",
    "from src.utils.metric import mrr\n",
    "\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "from icecream import ic\n",
    "import sys\n",
    "\n",
    "torch.set_float32_matmul_precision('high')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "batch_size = 200\n",
    "lr = 5e-4\n",
    "eps = 1e-8\n",
    "epochs = 3\n",
    "\n",
    "compile = False\n",
    "data_split = [0.6, 0.2, 0.2]\n",
    "split_type = 'temporal'\n",
    "\n",
    "khop_neighbors = [100, 100]\n",
    "pos_sample_prob = 0.15\n",
    "num_neg_samples = 64\n",
    "channels = 32\n",
    "\n",
    "pretrain = 'lp'\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "args = {\n",
    "    'testing': True,\n",
    "    'batch_size': batch_size,\n",
    "    'seed': seed,\n",
    "    'device': device,\n",
    "    'lr': lr,\n",
    "    'eps': eps,\n",
    "    'epochs': epochs,\n",
    "    'compile': compile,\n",
    "    'data_split': data_split,\n",
    "    'pos_sample_prob': pos_sample_prob,\n",
    "    'channels': channels,\n",
    "    'split_type': split_type,\n",
    "    'num_neg_samples': num_neg_samples,\n",
    "    'pretrain': pretrain,\n",
    "    'khop_neighbors': khop_neighbors,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maakyildiz\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "wandb.login()\n",
    "run = wandb.init(\n",
    "    mode=\"disabled\" if args['testing'] else \"online\",\n",
    "    project=f\"rel-mm\", \n",
    "    name=\"model=GINe,dataset=IBM-AML_Hi_Sm,objective=lp,khop_neighs=[100,100],channels=32,epoch=3\",\n",
    "    #name=\"debug-temporal-LOL-channels256-LOL\",\n",
    "    config=args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "# When running on the CuDNN backend, two further options must be set\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "# Set a fixed value for the hash seed\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| dataset: IBMTransactionsAML()\n",
      "ic| len(train_dataset): 3248921\n",
      "    len(val_dataset): 965524\n",
      "    len(test_dataset): 863900\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3248921, 965524, 863900)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dataset = IBMTransactionsAML(root='/mnt/data/ibm-transactions-for-anti-money-laundering-aml/dummy-c.csv', pretrain='lp', split_type=split_type, splits=data_split)\n",
    "dataset = IBMTransactionsAML(\n",
    "    root='/mnt/data/ibm-transactions-for-anti-money-laundering-aml/HI-Small_Trans-c.csv', \n",
    "    pretrain=pretrain, \n",
    "    split_type=split_type, \n",
    "    splits=data_split, \n",
    "    khop_neighbors=khop_neighbors\n",
    ")\n",
    "ic(dataset)\n",
    "dataset.materialize()\n",
    "dataset.df.head(5)\n",
    "train_dataset, val_dataset, test_dataset = dataset.split()\n",
    "ic(len(train_dataset), len(val_dataset), len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| node_feats: tensor([[1.],\n",
      "                        [1.],\n",
      "                        [1.],\n",
      "                        ...,\n",
      "                        [1.],\n",
      "                        [1.],\n",
      "                        [1.]], device='cuda:0')\n",
      "    edge_index: tensor([[  653, 11811,   183,  ...,  2772, 11815, 10185],\n",
      "                        [ 1208,  7767, 10709,  ...,  2772, 12251,  4130]])\n",
      "    edge_attr: tensor([[ 7.3913e-04, -2.4598e-03, -2.0262e-04,  ..., -2.4377e-02,\n",
      "                         3.1424e-02,  3.0117e-02],\n",
      "                       [ 8.3739e-04, -2.7868e-03, -2.2956e-04,  ..., -2.4377e-02,\n",
      "                         3.1424e-02,  3.0117e-02],\n",
      "                       [ 1.2223e-03, -4.0679e-03, -3.3508e-04,  ..., -2.4377e-02,\n",
      "                         3.1424e-02,  3.0117e-02],\n",
      "                       ...,\n",
      "                       [ 3.1000e-03, -1.0317e-02, -8.4981e-04,  ..., -2.4377e-02,\n",
      "                         3.1424e-02,  3.0117e-02],\n",
      "                       [ 1.2191e-03, -4.0570e-03, -3.3419e-04,  ..., -2.4377e-02,\n",
      "                         3.1424e-02,  3.0117e-02],\n",
      "                       [ 6.5155e-05, -2.1683e-04, -1.7861e-05,  ..., -2.4377e-02,\n",
      "                         3.1424e-02,  3.0117e-02]], grad_fn=<ViewBackward0>)\n",
      "    input_edge_index: tensor([[  653, 11811,   183,  ...,  2772, 11815, 10185],\n",
      "                              [ 1208,  7767, 10709,  ...,  2772, 12251,  4130]], device='cuda:0')\n",
      "    input_edge_attr: tensor([[ 7.3913e-04, -2.4598e-03, -2.0262e-04,  ..., -2.4377e-02,\n",
      "                               3.1424e-02,  3.0117e-02],\n",
      "                             [ 8.3739e-04, -2.7868e-03, -2.2956e-04,  ..., -2.4377e-02,\n",
      "                               3.1424e-02,  3.0117e-02],\n",
      "                             [ 1.2223e-03, -4.0679e-03, -3.3508e-04,  ..., -2.4377e-02,\n",
      "                               3.1424e-02,  3.0117e-02],\n",
      "                             ...,\n",
      "                             [ 3.1000e-03, -1.0317e-02, -8.4981e-04,  ..., -2.4377e-02,\n",
      "                               3.1424e-02,  3.0117e-02],\n",
      "                             [ 1.2191e-03, -4.0570e-03, -3.3419e-04,  ..., -2.4377e-02,\n",
      "                               3.1424e-02,  3.0117e-02],\n",
      "                             [ 6.5155e-05, -2.1683e-04, -1.7861e-05,  ..., -2.4377e-02,\n",
      "                               3.1424e-02,  3.0117e-02]], device='cuda:0',\n",
      "                            grad_fn=<ToCopyBackward0>)\n",
      "    pos_edge_index: tensor([[    3,  4512,  9178,     7,  7220,  1048,  4372,  7348, 10365, 11673,\n",
      "                              4995,  7201,  8151,  1678,  3863,  4352,  6394,     7,  9560,  9295,\n",
      "                              8069,  2271,  6790,  8461,   305,     7,  2479,  6960,   871,  5418],\n",
      "                            [10754,  4524,  9766, 10762,  9009,  1892, 11858,  7348, 10040, 10666,\n",
      "                             10121,  3922, 12221,  1894,  3863, 10346,  6394,  1638,  5525, 12220,\n",
      "                              3391, 12008,  6790,  5076,  9809, 11037,  7448,  6960,  1228,  9043]],\n",
      "                           device='cuda:0')\n",
      "    pos_edge_attr: tensor([[ 5.6324e-04, -1.8744e-03, -1.5440e-04,  ..., -2.4377e-02,\n",
      "                             3.1424e-02,  3.0117e-02],\n",
      "                           [-4.3284e-03,  1.4405e-02,  1.1865e-03,  ..., -2.4377e-02,\n",
      "                             3.1424e-02,  3.0117e-02],\n",
      "                           [ 1.1600e-03, -3.8603e-03, -3.1798e-04,  ..., -2.4377e-02,\n",
      "                             3.1424e-02,  3.0117e-02],\n",
      "                           ...,\n",
      "                           [ 3.8043e-03, -1.2660e-02, -1.0429e-03,  ..., -2.4377e-02,\n",
      "                             3.1424e-02,  3.0117e-02],\n",
      "                           [ 2.4027e-04, -7.9960e-04, -6.5866e-05,  ..., -2.4377e-02,\n",
      "                             3.1424e-02,  3.0117e-02],\n",
      "                           [-5.6824e-04,  1.8911e-03,  1.5577e-04,  ..., -2.4377e-02,\n",
      "                             3.1424e-02,  3.0117e-02]], device='cuda:0',\n",
      "                          grad_fn=<ToCopyBackward0>)\n",
      "    neg_edge_index: tensor([[   3,    3,    3,  ..., 6587, 8215, 8495],\n",
      "                            [6894, 1040,   46,  ..., 9043, 9043, 9043]], device='cuda:0')\n",
      "    neg_edge_attr: tensor([[ 0.0006, -0.0019, -0.0002,  ..., -0.0244,  0.0314,  0.0301],\n",
      "                           [ 0.0006, -0.0019, -0.0002,  ..., -0.0244,  0.0314,  0.0301],\n",
      "                           [ 0.0006, -0.0019, -0.0002,  ..., -0.0244,  0.0314,  0.0301],\n",
      "                           ...,\n",
      "                           [-0.0006,  0.0019,  0.0002,  ..., -0.0244,  0.0314,  0.0301],\n",
      "                           [-0.0006,  0.0019,  0.0002,  ..., -0.0244,  0.0314,  0.0301],\n",
      "                           [-0.0006,  0.0019,  0.0002,  ..., -0.0244,  0.0314,  0.0301]],\n",
      "                          device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "ic| node_feats.shape: torch.Size([12353, 1])\n",
      "    edge_index.shape: torch.Size([2, 10108])\n",
      "    edge_attr.shape: torch.Size([10108, 256])\n",
      "    input_edge_index.shape: torch.Size([2, 10078])\n",
      "    input_edge_attr.shape: torch.Size([10078, 256])\n",
      "    pos_edge_index.shape: torch.Size([2, 30])\n",
      "    pos_edge_attr.shape: torch.Size([30, 256])\n",
      "    neg_edge_index.shape: torch.Size([2, 1920])\n",
      "    neg_edge_attr.shape: torch.Size([1920, 256])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([12353, 1]),\n",
       " torch.Size([2, 10108]),\n",
       " torch.Size([10108, 256]),\n",
       " torch.Size([2, 10078]),\n",
       " torch.Size([10078, 256]),\n",
       " torch.Size([2, 30]),\n",
       " torch.Size([30, 256]),\n",
       " torch.Size([2, 1920]),\n",
       " torch.Size([1920, 256]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "g = torch.Generator()\n",
    "g.manual_seed(seed)\n",
    "tensor_frame = dataset.tensor_frame\n",
    "train_tensor_frame = train_dataset.tensor_frame\n",
    "train_loader = DataLoader(train_tensor_frame, batch_size=batch_size, shuffle=True, worker_init_fn=seed_worker, generator=g)\n",
    "val_tensor_frame = val_dataset.tensor_frame\n",
    "val_loader = DataLoader(val_tensor_frame, batch_size=batch_size, shuffle=False, worker_init_fn=seed_worker, generator=g)\n",
    "test_tensor_frame = test_dataset.tensor_frame\n",
    "test_loader = DataLoader(test_tensor_frame, batch_size=batch_size, shuffle=False, worker_init_fn=seed_worker, generator=g)\n",
    "\n",
    "stype_encoder_dict = {\n",
    "    stype.categorical: EmbeddingEncoder(),\n",
    "    stype.numerical: LinearEncoder(),\n",
    "    stype.timestamp: TimestampEncoder(),\n",
    "}\n",
    "encoder = StypeWiseFeatureEncoder(\n",
    "            out_channels=channels,\n",
    "            col_stats=dataset.col_stats,\n",
    "            col_names_dict=train_tensor_frame.col_names_dict,\n",
    "            stype_encoder_dict=stype_encoder_dict,\n",
    ")\n",
    "def lp_inputs(tf: TensorFrame, pos_sample_prob=0.15, train=True):\n",
    "    \n",
    "    edges = tf.y\n",
    "    batch_size = len(edges)\n",
    "    khop_source, khop_destination, idx = dataset.sample_neighbors(edges, train)\n",
    "\n",
    "    edge_data = tensor_frame.__getitem__(idx)\n",
    "    edge_attr, col_names = encoder(edge_data)\n",
    "    edge_attr = edge_attr.view(-1, len(col_names) * channels)\n",
    "\n",
    "    nodes = torch.unique(torch.cat([khop_source, khop_destination]))\n",
    "    num_nodes = nodes.shape[0]\n",
    "    node_feats = torch.ones(num_nodes).view(-1,num_nodes).t()\n",
    "\n",
    "    n_id_map = {value.item(): index for index, value in enumerate(nodes)}\n",
    "    local_khop_source = torch.tensor([n_id_map[node.item()] for node in khop_source], dtype=torch.long)\n",
    "    local_khop_destination = torch.tensor([n_id_map[node.item()] for node in khop_destination], dtype=torch.long)\n",
    "    edge_index = torch.cat((local_khop_source.unsqueeze(0), local_khop_destination.unsqueeze(0)))\n",
    "\n",
    "    # sample positive edges\n",
    "    positions = torch.arange(batch_size)\n",
    "    num_samples = int(len(positions) * pos_sample_prob)\n",
    "    if len(positions) > 0 and num_samples > 0:\n",
    "        drop_idxs = torch.multinomial(torch.full((len(positions),), 1.0), num_samples, replacement=False)\n",
    "    else:\n",
    "        drop_idxs = torch.tensor([]).long()\n",
    "    drop_edge_ind = positions[drop_idxs]\n",
    "\n",
    "    mask = torch.zeros((edge_index.shape[1],)).long() #[E, ]\n",
    "    mask = mask.index_fill_(dim=0, index=drop_edge_ind, value=1).bool() #[E, ]\n",
    "    input_edge_index = edge_index[:, ~mask]\n",
    "    input_edge_attr  = edge_attr[~mask]\n",
    "\n",
    "    pos_edge_index = edge_index[:, mask]\n",
    "    pos_edge_attr  = edge_attr[mask]\n",
    "\n",
    "    # generate/sample negative edges\n",
    "    neg_edges = []\n",
    "    neg_edge_attr = []\n",
    "    nodeset = set(range(edge_index.max()+1))\n",
    "    for i, edge in enumerate(pos_edge_index.t()):\n",
    "        src, dst = edge[0], edge[1]\n",
    "\n",
    "        # Chose negative examples in a smart way\n",
    "        unavail_mask = (edge_index == src).any(dim=0) | (edge_index == dst).any(dim=0)\n",
    "        unavail_nodes = torch.unique(edge_index[:, unavail_mask])\n",
    "        unavail_nodes = set(unavail_nodes.tolist())\n",
    "        avail_nodes = nodeset - unavail_nodes\n",
    "        avail_nodes = torch.tensor(list(avail_nodes))\n",
    "        # Finally, emmulate np.random.choice() to chose randomly amongst available nodes\n",
    "        indices = torch.randperm(len(avail_nodes))[:num_neg_samples]\n",
    "        neg_nodes = avail_nodes[indices]\n",
    "        \n",
    "        # Generate num_neg_samples/2 negative edges with the same source but different destinations\n",
    "        num_neg_samples_half = int(num_neg_samples/2)\n",
    "        neg_dsts = neg_nodes[:num_neg_samples_half]  # Selecting num_neg_samples/2 random destination nodes for the source\n",
    "        neg_edges_src = torch.stack([src.repeat(num_neg_samples_half), neg_dsts], dim=0)\n",
    "        \n",
    "        # Generate num_neg_samples/2 negative edges with the same destination but different sources\n",
    "        neg_srcs = neg_nodes[num_neg_samples_half:]  # Selecting num_neg_samples/2 random source nodes for the destination\n",
    "        neg_edges_dst = torch.stack([neg_srcs, dst.repeat(num_neg_samples_half)], dim=0)\n",
    "\n",
    "        # Add these negative edges to the list\n",
    "        neg_edges.append(neg_edges_src)\n",
    "        neg_edges.append(neg_edges_dst)\n",
    "        # Replicate the positive edge attribute for each of the negative edges generated from this edge\n",
    "        pos_attr = pos_edge_attr[i].unsqueeze(0)  # Get the attribute of the current positive edge\n",
    "        \n",
    "        replicated_attr = pos_attr.repeat(num_neg_samples, 1)  # Replicate it num_neg_samples times (for each negative edge)\n",
    "        neg_edge_attr.append(replicated_attr)\n",
    "    \n",
    "    input_edge_index = input_edge_index.to(device)\n",
    "    input_edge_attr = input_edge_attr.to(device)\n",
    "    pos_edge_index = pos_edge_index.to(device)\n",
    "    pos_edge_attr = pos_edge_attr.to(device)\n",
    "    node_feats = node_feats.to(device)\n",
    "    neg_edge_index = torch.cat(neg_edges, dim=1).to(device)\n",
    "    neg_edge_attr = torch.cat(neg_edge_attr, dim=0).to(device)\n",
    "    return node_feats, edge_index, edge_attr, input_edge_index, input_edge_attr, pos_edge_index, pos_edge_attr, neg_edge_index, neg_edge_attr\n",
    "batch = next(iter(train_loader))\n",
    "node_feats, edge_index, edge_attr, input_edge_index, input_edge_attr, pos_edge_index, pos_edge_attr, neg_edge_index, neg_edge_attr = lp_inputs(batch)\n",
    "ic( node_feats, edge_index, edge_attr, input_edge_index, input_edge_attr, pos_edge_index, pos_edge_attr, neg_edge_index, neg_edge_attr)\n",
    "ic( node_feats.shape, edge_index.shape, edge_attr.shape, input_edge_index.shape, input_edge_attr.shape, pos_edge_index.shape, pos_edge_attr.shape, neg_edge_index.shape, neg_edge_attr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoc: int, model, optimizer) -> float:\n",
    "    model.train()\n",
    "    loss_accum = total_count = 0\n",
    "\n",
    "    with tqdm(train_loader, desc=f'Epoch {epoc}') as t:\n",
    "        for tf in t:\n",
    "            node_feats, _, _, input_edge_index, input_edge_attr, pos_edge_index, pos_edge_attr, neg_edge_index, neg_edge_attr = lp_inputs(tf)\n",
    "            pred, neg_pred = model(node_feats, input_edge_index, input_edge_attr, pos_edge_index, pos_edge_attr, neg_edge_index, neg_edge_attr)\n",
    "            loss = lp_loss(pred, neg_pred)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_accum += float(loss) * len(pred)\n",
    "            total_count += len(pred)\n",
    "            t.set_postfix(loss=f'{loss_accum/total_count:.4f}')\n",
    "            del pred\n",
    "            del tf\n",
    "        wandb.log({\"train_loss\": loss_accum/total_count})\n",
    "    return {'loss': loss_accum / total_count}\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(loader: DataLoader, model, dataset_name) -> float:\n",
    "    model.eval()\n",
    "    mrrs = []\n",
    "    hits1 = []\n",
    "    hits2 = []\n",
    "    hits5 = []\n",
    "    hits10 = []\n",
    "    loss_accum = 0\n",
    "    total_count = 0\n",
    "    with tqdm(loader, desc=f'Evaluating') as t:\n",
    "        for tf in t:\n",
    "            node_feats, _, _, input_edge_index, input_edge_attr, pos_edge_index, pos_edge_attr, neg_edge_index, neg_edge_attr = lp_inputs(tf, train=False)\n",
    "            pred, neg_pred = model(node_feats, input_edge_index, input_edge_attr, pos_edge_index, pos_edge_attr, neg_edge_index, neg_edge_attr)\n",
    "            loss = lp_loss(pred, neg_pred)\n",
    "            loss_accum += float(loss) * len(pred)\n",
    "            total_count += len(pred)\n",
    "            mrr_score, hits = mrr(pred, neg_pred, [1,2,5,10], num_neg_samples)\n",
    "            mrrs.append(mrr_score)\n",
    "            hits1.append(hits['hits@1'])\n",
    "            hits2.append(hits['hits@2'])\n",
    "            hits5.append(hits['hits@5'])\n",
    "            hits10.append(hits['hits@10'])\n",
    "            t.set_postfix(\n",
    "                loss=f'{loss_accum/total_count:.4f}',\n",
    "                mrr=f'{np.mean(mrrs):.4f}',\n",
    "                hits1=f'{np.mean(hits1):.4f}',\n",
    "                hits2=f'{np.mean(hits2):.4f}',\n",
    "                hits5=f'{np.mean(hits5):.4f}',\n",
    "                hits10=f'{np.mean(hits10):.4f}'\n",
    "            )\n",
    "        mrr_score = np.mean(mrrs)\n",
    "        hits1 = np.mean(hits1)\n",
    "        hits2 = np.mean(hits2)\n",
    "        hits5 = np.mean(hits5)\n",
    "        hits10 = np.mean(hits10)\n",
    "        wandb.log({\n",
    "            f\"{dataset_name}_loss\": loss_accum/total_count,\n",
    "            f\"{dataset_name}_mrr\": mrr_score,\n",
    "            f\"{dataset_name}_hits@1\": hits1,\n",
    "            f\"{dataset_name}_hits@2\": hits2,\n",
    "            f\"{dataset_name}_hits@5\": hits5,\n",
    "            f\"{dataset_name}_hits@10\": hits10\n",
    "        })\n",
    "        del tf\n",
    "        del pred\n",
    "        return {\"mrr\": mrr_score, \"hits@1\": hits1, \"hits@2\": hits2, \"hits@5\": hits5, \"hits@10\": hits10}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch.autograd' has no attribute 'set_detect_anamoly'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_detect_anamoly\u001b[49m(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m GINe(num_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, num_gnn_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, edge_dim\u001b[38;5;241m=\u001b[39mtrain_dataset\u001b[38;5;241m.\u001b[39mtensor_frame\u001b[38;5;241m.\u001b[39mnum_cols\u001b[38;5;241m*\u001b[39mchannels, n_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcompile(model, dynamic\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcompile\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m model\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torch.autograd' has no attribute 'set_detect_anamoly'"
     ]
    }
   ],
   "source": [
    "torch.autograd.set_detect_anomaly(True)\n",
    "model = GINe(num_features=1, num_gnn_layers=3, edge_dim=train_dataset.tensor_frame.num_cols*channels, n_classes=1)\n",
    "model = torch.compile(model, dynamic=True) if compile else model\n",
    "model.to(device)\n",
    "learnable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "ic(learnable_params)\n",
    "wandb.log({\"learnable_params\": learnable_params})\n",
    "\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ]\n",
    "optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=lr, eps=eps)\n",
    "scheduler = get_inverse_sqrt_schedule(optimizer, num_warmup_steps=0, timescale=1000)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "train_metric = test(train_loader, model, \"train\")\n",
    "val_metric = test(val_loader, model, \"val\")\n",
    "test_metric = test(test_loader, model, \"test\")\n",
    "ic(\n",
    "        train_metric, \n",
    "        val_metric, \n",
    "        test_metric\n",
    ")\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train_loss = train(epoch, model, optimizer)\n",
    "    train_metric = test(train_loader, model, \"train\")\n",
    "    val_metric = test(val_loader, model, \"val\")\n",
    "    test_metric = test(test_loader, model, \"test\")\n",
    "    ic(\n",
    "        train_loss, \n",
    "        train_metric, \n",
    "        val_metric, \n",
    "        test_metric\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.004 MB of 0.005 MB uploaded\\r'), FloatProgress(value=0.7691676916769168, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>learnable_params</td><td>▁</td></tr><tr><td>test_hits@1</td><td>▁█</td></tr><tr><td>test_hits@10</td><td>▁█</td></tr><tr><td>test_hits@2</td><td>▁█</td></tr><tr><td>test_hits@5</td><td>▁█</td></tr><tr><td>test_loss</td><td>█▁</td></tr><tr><td>test_mrr</td><td>▁█</td></tr><tr><td>train_hits@1</td><td>▁█</td></tr><tr><td>train_hits@10</td><td>▁█</td></tr><tr><td>train_hits@2</td><td>▁█</td></tr><tr><td>train_hits@5</td><td>▁█</td></tr><tr><td>train_loss</td><td>█▂▁</td></tr><tr><td>train_mrr</td><td>▁█</td></tr><tr><td>val_hits@1</td><td>▁█</td></tr><tr><td>val_hits@10</td><td>▁█</td></tr><tr><td>val_hits@2</td><td>▁█</td></tr><tr><td>val_hits@5</td><td>▁█</td></tr><tr><td>val_loss</td><td>█▁</td></tr><tr><td>val_mrr</td><td>▁█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>learnable_params</td><td>464435</td></tr><tr><td>test_hits@1</td><td>0.37035</td></tr><tr><td>test_hits@10</td><td>0.84794</td></tr><tr><td>test_hits@2</td><td>0.5547</td></tr><tr><td>test_hits@5</td><td>0.76336</td></tr><tr><td>test_loss</td><td>0.77909</td></tr><tr><td>test_mrr</td><td>0.54056</td></tr><tr><td>train_hits@1</td><td>0.58056</td></tr><tr><td>train_hits@10</td><td>0.81762</td></tr><tr><td>train_hits@2</td><td>0.71823</td></tr><tr><td>train_hits@5</td><td>0.80224</td></tr><tr><td>train_loss</td><td>0.57013</td></tr><tr><td>train_mrr</td><td>0.68554</td></tr><tr><td>val_hits@1</td><td>0.40069</td></tr><tr><td>val_hits@10</td><td>0.89261</td></tr><tr><td>val_hits@2</td><td>0.60478</td></tr><tr><td>val_hits@5</td><td>0.82792</td></tr><tr><td>val_loss</td><td>0.71713</td></tr><tr><td>val_mrr</td><td>0.58062</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">model=GINe,dataset=IBM-AML_Hi_Sm,objective=lp,khop_neighs=[100,100],channels=256</strong> at: <a href='https://wandb.ai/aakyildiz/rel-mm/runs/f7uzxly2' target=\"_blank\">https://wandb.ai/aakyildiz/rel-mm/runs/f7uzxly2</a><br/> View project at: <a href='https://wandb.ai/aakyildiz/rel-mm' target=\"_blank\">https://wandb.ai/aakyildiz/rel-mm</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240423_223842-f7uzxly2/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
