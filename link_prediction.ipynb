{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "from torch_frame.data import DataLoader\n",
    "from torch_frame import TensorFrame\n",
    "from torch_geometric.data import Data\n",
    "from transformers import get_inverse_sqrt_schedule\n",
    "\n",
    "from src.datasets import IBMTransactionsAML\n",
    "from src.nn.gnn.model import GINe\n",
    "from src.utils.loss import lp_loss\n",
    "from src.utils.metric import mrr\n",
    "\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "from icecream import ic\n",
    "import sys\n",
    "\n",
    "torch.set_float32_matmul_precision('high')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "batch_size = 1024\n",
    "lr = 5e-4\n",
    "eps = 1e-8\n",
    "epochs = 5\n",
    "\n",
    "compile = True\n",
    "data_split = [0.6, 0.2, 0.2]\n",
    "split_type = 'temporal'\n",
    "\n",
    "pos_sample_prob = 0.15\n",
    "num_neg_samples = 64\n",
    "channels = 256\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "args = {\n",
    "    'testing': False,\n",
    "    'batch_size': batch_size,\n",
    "    'seed': seed,\n",
    "    'device': device,\n",
    "    'lr': lr,\n",
    "    'eps': eps,\n",
    "    'epochs': epochs,\n",
    "    'compile': compile,\n",
    "    'data_split': data_split,\n",
    "    'pos_sample_prob': pos_sample_prob,\n",
    "    'channels': channels,\n",
    "    'split_type': split_type,\n",
    "    'num_neg_samples': num_neg_samples\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    }
   ],
   "source": [
    "wandb.login()\n",
    "run = wandb.init(\n",
    "    mode=\"disabled\" if args['testing'] else \"online\",\n",
    "    project=f\"rel-mm\", \n",
    "    #name=\"model=GINe,dataset=IBM-AML_Hi_Sm,objective=lp\",\n",
    "    name=\"debug-temporal-LOL-channels256-LOL\",\n",
    "    config=args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "# When running on the CuDNN backend, two further options must be set\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "# Set a fixed value for the hash seed\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| dataset: IBMTransactionsAML()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>From Bank</th>\n",
       "      <th>From ID</th>\n",
       "      <th>To Bank</th>\n",
       "      <th>To ID</th>\n",
       "      <th>Amount Received</th>\n",
       "      <th>Receiving Currency</th>\n",
       "      <th>Amount Paid</th>\n",
       "      <th>Payment Currency</th>\n",
       "      <th>Payment Format</th>\n",
       "      <th>Is Laundering</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1200</td>\n",
       "      <td>B_10</td>\n",
       "      <td>8000EBD30</td>\n",
       "      <td>B_10</td>\n",
       "      <td>8000EBD30</td>\n",
       "      <td>0.296848</td>\n",
       "      <td>US Dollar</td>\n",
       "      <td>0.296848</td>\n",
       "      <td>US Dollar</td>\n",
       "      <td>Reinvestment</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1200</td>\n",
       "      <td>B_3208</td>\n",
       "      <td>8000F4580</td>\n",
       "      <td>B_1</td>\n",
       "      <td>8000F5340</td>\n",
       "      <td>0.000359</td>\n",
       "      <td>US Dollar</td>\n",
       "      <td>0.000359</td>\n",
       "      <td>US Dollar</td>\n",
       "      <td>Cheque</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>B_3209</td>\n",
       "      <td>8000F4670</td>\n",
       "      <td>B_3209</td>\n",
       "      <td>8000F4670</td>\n",
       "      <td>0.346651</td>\n",
       "      <td>US Dollar</td>\n",
       "      <td>0.346651</td>\n",
       "      <td>US Dollar</td>\n",
       "      <td>Reinvestment</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>120</td>\n",
       "      <td>B_12</td>\n",
       "      <td>8000F5030</td>\n",
       "      <td>B_12</td>\n",
       "      <td>8000F5030</td>\n",
       "      <td>0.286896</td>\n",
       "      <td>US Dollar</td>\n",
       "      <td>0.286896</td>\n",
       "      <td>US Dollar</td>\n",
       "      <td>Reinvestment</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>360</td>\n",
       "      <td>B_10</td>\n",
       "      <td>8000F5200</td>\n",
       "      <td>B_10</td>\n",
       "      <td>8000F5200</td>\n",
       "      <td>0.379751</td>\n",
       "      <td>US Dollar</td>\n",
       "      <td>0.379751</td>\n",
       "      <td>US Dollar</td>\n",
       "      <td>Reinvestment</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Timestamp From Bank    From ID To Bank      To ID  Amount Received  \\\n",
       "0       1200      B_10  8000EBD30    B_10  8000EBD30         0.296848   \n",
       "1       1200    B_3208  8000F4580     B_1  8000F5340         0.000359   \n",
       "2          0    B_3209  8000F4670  B_3209  8000F4670         0.346651   \n",
       "3        120      B_12  8000F5030    B_12  8000F5030         0.286896   \n",
       "4        360      B_10  8000F5200    B_10  8000F5200         0.379751   \n",
       "\n",
       "  Receiving Currency  Amount Paid Payment Currency Payment Format  \\\n",
       "0          US Dollar     0.296848        US Dollar   Reinvestment   \n",
       "1          US Dollar     0.000359        US Dollar         Cheque   \n",
       "2          US Dollar     0.346651        US Dollar   Reinvestment   \n",
       "3          US Dollar     0.286896        US Dollar   Reinvestment   \n",
       "4          US Dollar     0.379751        US Dollar   Reinvestment   \n",
       "\n",
       "  Is Laundering  split  \n",
       "0             0      0  \n",
       "1             0      0  \n",
       "2             0      0  \n",
       "3             0      0  \n",
       "4             0      0  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dataset = IBMTransactionsAML(root='/mnt/data/ibm-transactions-for-anti-money-laundering-aml/dummy.csv')\n",
    "dataset = IBMTransactionsAML(root='/mnt/data/ibm-transactions-for-anti-money-laundering-aml/HI-Small_Trans-cleaned.csv', split_type=split_type, splits=data_split)\n",
    "ic(dataset)\n",
    "dataset.materialize()\n",
    "dataset.df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, val_dataset, test_dataset = dataset.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tensor_frame = train_dataset.tensor_frame\n",
    "train_loader = DataLoader(train_tensor_frame, batch_size=batch_size, shuffle=True)\n",
    "val_tensor_frame = val_dataset.tensor_frame\n",
    "val_loader = DataLoader(val_tensor_frame, batch_size=batch_size, shuffle=True)\n",
    "test_tensor_frame = test_dataset.tensor_frame\n",
    "test_loader = DataLoader(test_tensor_frame, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TODO: generalize the trainable columns\n",
    "# source = train_tensor_frame.get_col_feat('From ID')\n",
    "# destination = train_tensor_frame.get_col_feat('To ID')\n",
    "\n",
    "# #create dummy node features\n",
    "# num_nodes = np.unique(np.concatenate([source, destination])).shape[0]\n",
    "# ic(num_nodes)\n",
    "# node_feat = torch.ones(num_nodes)\n",
    "\n",
    "# edge_index = torch.cat([source, destination], dim=1).t()\n",
    "# ic(edge_index.shape)\n",
    "# g = Data(node_feat, edge_index=edge_index, edge_attr=train_tensor_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| node_feats: tensor([[1.],\n",
      "                        [1.],\n",
      "                        [1.],\n",
      "                        ...,\n",
      "                        [1.],\n",
      "                        [1.],\n",
      "                        [1.]], device='cuda:0')\n",
      "    edge_index: tensor([[1697,  710, 1900,  ...,  140,  209, 1084],\n",
      "                        [1743,  735, 1897,  ...,  534,  428,  903]])\n",
      "    edge_attr: tensor([[-0.0009,  0.0013,  0.0005,  ..., -0.0527, -0.0719,  0.0330],\n",
      "                       [-0.0027,  0.0039,  0.0015,  ..., -0.0527, -0.0719,  0.0330],\n",
      "                       [ 0.0038, -0.0054, -0.0022,  ..., -0.0527, -0.0719,  0.0330],\n",
      "                       ...,\n",
      "                       [-0.0020,  0.0029,  0.0011,  ..., -0.0527, -0.0719,  0.0330],\n",
      "                       [ 0.0055, -0.0078, -0.0031,  ..., -0.0527, -0.0719,  0.0330],\n",
      "                       [ 0.0029, -0.0041, -0.0016,  ..., -0.0527, -0.0719,  0.0330]],\n",
      "                      grad_fn=<ViewBackward0>)\n",
      "    input_edge_index: tensor([[1697,  710, 1900,  ...,  140,  209, 1084],\n",
      "                              [1743,  735, 1897,  ...,  534,  428,  903]], device='cuda:0')\n",
      "    input_edge_attr: tensor([[-0.0009,  0.0013,  0.0005,  ..., -0.0527, -0.0719,  0.0330],\n",
      "                             [-0.0027,  0.0039,  0.0015,  ..., -0.0527, -0.0719,  0.0330],\n",
      "                             [ 0.0038, -0.0054, -0.0022,  ..., -0.0527, -0.0719,  0.0330],\n",
      "                             ...,\n",
      "                             [-0.0020,  0.0029,  0.0011,  ..., -0.0527, -0.0719,  0.0330],\n",
      "                             [ 0.0055, -0.0078, -0.0031,  ..., -0.0527, -0.0719,  0.0330],\n",
      "                             [ 0.0029, -0.0041, -0.0016,  ..., -0.0527, -0.0719,  0.0330]],\n",
      "                            device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "    pos_edge_index: tensor([[1735, 1284,  509,  674,  870, 1120,    6,  818, 1225, 1649, 1804,  576,\n",
      "                              161,  626, 1299,    4,  405,    4, 1581, 1251, 1226, 1810,  737,  680,\n",
      "                              346,  258,  376,    2, 1927,  214, 1926,    0, 1710,  239, 1848, 1076,\n",
      "                                4,  599, 1040,  639, 1045,    1, 1192,  855,  565,    0,  386,    0,\n",
      "                             1830,  902,  310, 1922,  339,   27,    0,  447,  762, 1886,  504,    0,\n",
      "                              133,  844, 1749,  645, 1077,  518,  976,  300,  529,  127,  269, 1692,\n",
      "                             1415,  953, 1935, 1421, 1806,    4,  668, 1429,  780,    1,  220, 1109,\n",
      "                                2,  323, 1774,  151,  431,  132, 1108,  757, 1554, 1396,   87,    0,\n",
      "                             1695,  207,  501,  488,  404,  227,  708, 1702, 1878,   33,  839,   14,\n",
      "                             1346,  858,  337,  415, 1158, 1451, 1837,  183,  123, 1424,   52,  270,\n",
      "                             1359, 1128,  842, 1894,  965, 1167,    1,  891,   25, 1833, 1224,   93,\n",
      "                              945, 1856,  130,  966,  490, 1375,  497,  446, 1934, 1432,    0,  594,\n",
      "                               20,  896, 1054, 1221,  807,  690,  887,  111,  973],\n",
      "                            [   8, 1643, 1308,  647,  172, 1184, 1528,  815, 1543, 1492, 1718,  952,\n",
      "                              556, 1004, 1449,  586,  727, 1631, 1757,  926,  554, 1862,  311, 1030,\n",
      "                              745, 1392,  888, 1571, 1911,  572, 1923, 1499,  679,   89, 1796, 1874,\n",
      "                             1634, 1408,  883,  356,  464,  372, 1794, 1172,  776, 1069, 1813, 1536,\n",
      "                             1504,  898, 1319, 1912, 1140, 1043,  122, 1203,  538, 1792, 1559,  584,\n",
      "                              219,  777, 1750,  272, 1668,  199, 1195, 1439,  794,  798, 1409, 1517,\n",
      "                              845, 1256, 1932, 1712,  958, 1338,  249, 1699, 1708, 1688,  756, 1336,\n",
      "                             1509,   36,  400,  115,  548,  614,  969,  193,  121,  413, 1686, 1638,\n",
      "                             1847,  213,  156,  202,  392,  802,   85, 1662, 1789,  506,  535,  863,\n",
      "                             1143,  658, 1454,  136,  657, 1719, 1834, 1320,  660, 1258,  401,  245,\n",
      "                             1075, 1447, 1574, 1876,  996,  621, 1406, 1096,  524, 1737,  811, 1260,\n",
      "                              892, 1770,  683,  931, 1391, 1550,  275, 1582, 1776, 1621, 1145,  232,\n",
      "                             1283,  231, 1029, 1732,  695, 1689, 1236, 1799, 1349]],\n",
      "                           device='cuda:0')\n",
      "    pos_edge_attr: tensor([[-0.0003,  0.0005,  0.0002,  ..., -0.0527, -0.0719,  0.0330],\n",
      "                           [ 0.0054, -0.0077, -0.0031,  ..., -0.0527, -0.0719,  0.0330],\n",
      "                           [-0.0019,  0.0027,  0.0011,  ..., -0.0527, -0.0719,  0.0330],\n",
      "                           ...,\n",
      "                           [ 0.0019, -0.0026, -0.0010,  ..., -0.0527, -0.0719,  0.0330],\n",
      "                           [ 0.0028, -0.0040, -0.0016,  ..., -0.0527, -0.0719,  0.0330],\n",
      "                           [ 0.0009, -0.0013, -0.0005,  ..., -0.0527, -0.0719,  0.0330]],\n",
      "                          device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "    neg_edge_index: tensor([[1735, 1735, 1735,  ...,  521,  549, 1476],\n",
      "                            [1094,  150,  238,  ..., 1349, 1349, 1349]], device='cuda:0')\n",
      "    neg_edge_attr: tensor([[-0.0003,  0.0005,  0.0002,  ..., -0.0527, -0.0719,  0.0330],\n",
      "                           [-0.0003,  0.0005,  0.0002,  ..., -0.0527, -0.0719,  0.0330],\n",
      "                           [-0.0003,  0.0005,  0.0002,  ..., -0.0527, -0.0719,  0.0330],\n",
      "                           ...,\n",
      "                           [ 0.0009, -0.0013, -0.0005,  ..., -0.0527, -0.0719,  0.0330],\n",
      "                           [ 0.0009, -0.0013, -0.0005,  ..., -0.0527, -0.0719,  0.0330],\n",
      "                           [ 0.0009, -0.0013, -0.0005,  ..., -0.0527, -0.0719,  0.0330]],\n",
      "                          device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
      "ic| node_feats.shape: torch.Size([1962, 1])\n",
      "    edge_index.shape: torch.Size([2, 1024])\n",
      "    edge_attr.shape: torch.Size([1024, 320])\n",
      "    input_edge_index.shape: torch.Size([2, 871])\n",
      "    input_edge_attr.shape: torch.Size([871, 320])\n",
      "    pos_edge_index.shape: torch.Size([2, 153])\n",
      "    pos_edge_attr.shape: torch.Size([153, 320])\n",
      "    neg_edge_index.shape: torch.Size([2, 9792])\n",
      "    neg_edge_attr.shape: torch.Size([9792, 320])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([1962, 1]),\n",
       " torch.Size([2, 1024]),\n",
       " torch.Size([1024, 320]),\n",
       " torch.Size([2, 871]),\n",
       " torch.Size([871, 320]),\n",
       " torch.Size([2, 153]),\n",
       " torch.Size([153, 320]),\n",
       " torch.Size([2, 9792]),\n",
       " torch.Size([9792, 320]))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch_frame.nn.encoder.stypewise_encoder import StypeWiseFeatureEncoder\n",
    "from torch_frame import stype\n",
    "from torch_frame.nn import (\n",
    "    EmbeddingEncoder,\n",
    "    LinearEncoder,\n",
    "    TimestampEncoder,\n",
    ")\n",
    "stype_encoder_dict = {\n",
    "    stype.categorical: EmbeddingEncoder(),\n",
    "    stype.numerical: LinearEncoder(),\n",
    "    stype.timestamp: TimestampEncoder(),\n",
    "}\n",
    "encoder = StypeWiseFeatureEncoder(\n",
    "            out_channels=channels,\n",
    "            col_stats=dataset.col_stats,\n",
    "            col_names_dict=train_tensor_frame.col_names_dict,\n",
    "            stype_encoder_dict=stype_encoder_dict,\n",
    ")\n",
    "def get_gnn_inputs(tf: TensorFrame, pos_sample_prob=0.15):\n",
    "    source = tf.get_col_feat('From ID')\n",
    "    destination = tf.get_col_feat('To ID')\n",
    "\n",
    "    edge_attr, col_names = encoder(tf)\n",
    "    edge_attr = edge_attr.view(-1, len(col_names) * channels)\n",
    "\n",
    "    nodes = torch.unique(torch.cat([source, destination]))\n",
    "    num_nodes = nodes.shape[0]\n",
    "    node_feats = torch.ones(num_nodes).view(-1,num_nodes).t()\n",
    "\n",
    "    n_id_map = {value.item(): index for index, value in enumerate(nodes)}\n",
    "    local_source = torch.tensor([n_id_map[node.item()] for node in source], dtype=torch.long)\n",
    "    local_destination = torch.tensor([n_id_map[node.item()] for node in destination], dtype=torch.long)\n",
    "    edge_index = torch.cat((local_source.unsqueeze(0), local_destination.unsqueeze(0)))\n",
    "\n",
    "    # sample positive edges\n",
    "    E = edge_index.shape[1]\n",
    "    positions = torch.arange(E)\n",
    "    num_samples = int(len(positions) * pos_sample_prob)\n",
    "    if len(positions) > 0 and num_samples > 0:\n",
    "        drop_idxs = torch.multinomial(torch.full((len(positions),), 1.0), num_samples, replacement=False)\n",
    "    else:\n",
    "        drop_idxs = torch.tensor([]).long()\n",
    "    drop_edge_ind = positions[drop_idxs]\n",
    "\n",
    "    mask = torch.zeros((E,)).long() #[E, ]\n",
    "    mask = mask.index_fill_(dim=0, index=drop_edge_ind, value=1).bool() #[E, ]\n",
    "    input_edge_index = edge_index[:, ~mask]\n",
    "    input_edge_attr  = edge_attr[~mask]\n",
    "\n",
    "    pos_edge_index = edge_index[:, mask]\n",
    "    pos_edge_attr  = edge_attr[mask]\n",
    "\n",
    "    # generate/sample negative edges\n",
    "    # could choose false negatives, the entire graph is not used!\n",
    "    neg_edges = []\n",
    "    neg_edge_attr = []\n",
    "    nodeset = set(range(edge_index.max()+1))\n",
    "    for i, edge in enumerate(pos_edge_index.t()):\n",
    "        src, dst = edge[0], edge[1]\n",
    "\n",
    "        # Chose negative examples in a smart way\n",
    "        unavail_mask = (edge_index == src).any(dim=0) | (edge_index == dst).any(dim=0)\n",
    "        unavail_nodes = torch.unique(edge_index[:, unavail_mask])\n",
    "        unavail_nodes = set(unavail_nodes.tolist())\n",
    "        avail_nodes = nodeset - unavail_nodes\n",
    "        avail_nodes = torch.tensor(list(avail_nodes))\n",
    "        # Finally, emmulate np.random.choice() to chose randomly amongst available nodes\n",
    "        indices = torch.randperm(len(avail_nodes))[:64]\n",
    "        neg_nodes = avail_nodes[indices]\n",
    "        \n",
    "        # Generate 32 negative edges with the same source but different destinations\n",
    "        neg_dsts = neg_nodes[:32]  # Selecting num_neg_samples/2 random destination nodes for the source\n",
    "        neg_edges_src = torch.stack([src.repeat(int(num_neg_samples/2)), neg_dsts], dim=0)\n",
    "        \n",
    "        # Generate 32 negative edges with the same destination but different sources\n",
    "        neg_srcs = neg_nodes[32:]  # Selecting num_neg_samples/2 random source nodes for the destination\n",
    "        neg_edges_dst = torch.stack([neg_srcs, dst.repeat(int(num_neg_samples/2))], dim=0)\n",
    "\n",
    "        # Add these negative edges to the list\n",
    "        neg_edges.append(neg_edges_src)\n",
    "        neg_edges.append(neg_edges_dst)\n",
    "        # Replicate the positive edge attribute for each of the negative edges generated from this edge\n",
    "        pos_attr = pos_edge_attr[i].unsqueeze(0)  # Get the attribute of the current positive edge\n",
    "        replicated_attr = pos_attr.repeat(64, 1)  # Replicate it 64 times (for each negative edge)\n",
    "        neg_edge_attr.append(replicated_attr)\n",
    "    \n",
    "    input_edge_index = input_edge_index.to(device)\n",
    "    input_edge_attr = input_edge_attr.to(device)\n",
    "    pos_edge_index = pos_edge_index.to(device)\n",
    "    pos_edge_attr = pos_edge_attr.to(device)\n",
    "    node_feats = node_feats.to(device)\n",
    "    neg_edge_index = torch.cat(neg_edges, dim=1).to(device)\n",
    "    neg_edge_attr = torch.cat(neg_edge_attr, dim=0).to(device)\n",
    "    return node_feats, edge_index, edge_attr, input_edge_index, input_edge_attr, pos_edge_index, pos_edge_attr, neg_edge_index, neg_edge_attr\n",
    "batch = next(iter(train_loader))\n",
    "node_feats, edge_index, edge_attr, input_edge_index, input_edge_attr, pos_edge_index, pos_edge_attr, neg_edge_index, neg_edge_attr = get_gnn_inputs(batch)\n",
    "ic( node_feats, edge_index, edge_attr, input_edge_index, input_edge_attr, pos_edge_index, pos_edge_attr, neg_edge_index, neg_edge_attr)\n",
    "ic( node_feats.shape, edge_index.shape, edge_attr.shape, input_edge_index.shape, input_edge_attr.shape, pos_edge_index.shape, pos_edge_attr.shape, neg_edge_index.shape, neg_edge_attr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoc: int, model, optimizer) -> float:\n",
    "    model.train()\n",
    "    loss_accum = total_count = 0\n",
    "\n",
    "    with tqdm(train_loader, desc=f'Epoch {epoc}') as t:\n",
    "        for tf in t:\n",
    "            node_feats, _, _, input_edge_index, input_edge_attr, pos_edge_index, pos_edge_attr, neg_edge_index, neg_edge_attr = get_gnn_inputs(tf)\n",
    "            pred, neg_pred = model(node_feats, input_edge_index, input_edge_attr, pos_edge_index, pos_edge_attr, neg_edge_index, neg_edge_attr)\n",
    "            loss = lp_loss(pred, neg_pred)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_accum += float(loss) * len(pred)\n",
    "            total_count += len(pred)\n",
    "            t.set_postfix(loss=f'{loss_accum/total_count:.4f}')\n",
    "            del pred\n",
    "            del tf\n",
    "        wandb.log({\"train_loss\": loss_accum/total_count})\n",
    "    return {'loss': loss_accum / total_count}\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(loader: DataLoader, model, dataset_name) -> float:\n",
    "    model.eval()\n",
    "    mrrs = []\n",
    "    hits1 = []\n",
    "    hits2 = []\n",
    "    hits5 = []\n",
    "    hits10 = []\n",
    "    loss_accum = 0\n",
    "    total_count = 0\n",
    "    with tqdm(loader, desc=f'Evaluating') as t:\n",
    "        for tf in t:\n",
    "            node_feats, _, _, input_edge_index, input_edge_attr, pos_edge_index, pos_edge_attr, neg_edge_index, neg_edge_attr = get_gnn_inputs(tf)\n",
    "            pred, neg_pred = model(node_feats, input_edge_index, input_edge_attr, pos_edge_index, pos_edge_attr, neg_edge_index, neg_edge_attr)\n",
    "            loss = lp_loss(pred, neg_pred)\n",
    "            loss_accum += float(loss) * len(pred)\n",
    "            total_count += len(pred)\n",
    "            mrr_score, hits = mrr(pred, neg_pred, [1,2,5,10], num_neg_samples)\n",
    "            # ic(hits)\n",
    "            # sys.exit()\n",
    "            mrrs.append(mrr_score)\n",
    "            hits1.append(hits['hits@1'])\n",
    "            hits2.append(hits['hits@2'])\n",
    "            hits5.append(hits['hits@5'])\n",
    "            hits10.append(hits['hits@10'])\n",
    "            t.set_postfix(\n",
    "                loss=f'{loss_accum/total_count:.4f}',\n",
    "                mrr=f'{mrr_score:.4f}',\n",
    "                hits1=f'{hits[\"hits@1\"]:.4f}',\n",
    "                hits2=f'{hits[\"hits@2\"]:.4f}',\n",
    "                hits5=f'{hits[\"hits@5\"]:.4f}',\n",
    "                hits10=f'{hits[\"hits@10\"]:.4f}'\n",
    "            )\n",
    "        mrr_score = np.mean(mrrs)\n",
    "        # ic(mrr_score)\n",
    "        # ic(hits1)\n",
    "        # ic(hits2)\n",
    "        # ic(hits5)\n",
    "        # ic(hits10)\n",
    "        # sys.exit()\n",
    "        hits1 = np.sum(hits1) / total_count\n",
    "        hits2 = np.sum(hits2) / total_count\n",
    "        hits5 = np.sum(hits5) / total_count\n",
    "        hits10 = np.sum(hits10) / total_count\n",
    "        wandb.log({\n",
    "            f\"{dataset_name}_loss\": loss_accum/total_count,\n",
    "            f\"{dataset_name}_mrr\": mrr_score,\n",
    "            f\"{dataset_name}_hits@1\": hits1,\n",
    "            f\"{dataset_name}_hits@2\": hits2,\n",
    "            f\"{dataset_name}_hits@5\": hits5,\n",
    "            f\"{dataset_name}_hits@10\": hits10\n",
    "        })\n",
    "        del tf\n",
    "        del pred\n",
    "        return {\"mrr\": mrr_score, \"hits@1\": hits1, \"hits@2\": hits2, \"hits@5\": hits5, \"hits@10\": hits10}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| learnable_params: 243251\n",
      "Evaluating:   0%|                                                                                | 0/3173 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|█| 3173/3173 [01:55<00:00, 27.53it/s, hits1=0.0000, hits10=7.0000, hits2=0.0000, hits5=2.0000, loss=1.396\n",
      "Evaluating: 100%|█| 943/943 [00:34<00:00, 27.53it/s, hits1=0.0000, hits10=12.0000, hits2=0.0000, hits5=2.0000, loss=1.3962\n",
      "Evaluating: 100%|█| 844/844 [00:30<00:00, 27.56it/s, hits1=2.0000, hits10=11.0000, hits2=3.0000, hits5=3.0000, loss=1.3962\n",
      "ic| train_metric: {'hits@1': 0.004665927808929741,\n",
      "                   'hits@10': 0.09710485874495813,\n",
      "                   'hits@2': 0.007949587379540782,\n",
      "                   'hits@5': 0.020663983157339617,\n",
      "                   'mrr': 0.06022523790879262}\n",
      "    val_metric: {'hits@1': 0.005053270762426956,\n",
      "                 'hits@10': 0.09622009801543015,\n",
      "                 'hits@2': 0.008158710133575484,\n",
      "                 'hits@5': 0.020677512598517984,\n",
      "                 'mrr': 0.060144905547873995}\n",
      "    test_metric: {'hits@1': 0.00471804088968771,\n",
      "                  'hits@10': 0.09677794219044152,\n",
      "                  'hits@2': 0.007832412708496347,\n",
      "                  'hits@5': 0.020839950727848836,\n",
      "                  'mrr': 0.060006722057882475}\n",
      "Epoch 1: 100%|███████████████████████████████████████████████████████████| 3173/3173 [03:38<00:00, 14.52it/s, loss=1.0669]\n",
      "Evaluating: 100%|█| 3173/3173 [02:02<00:00, 26.00it/s, hits1=15.0000, hits10=16.0000, hits2=15.0000, hits5=15.0000, loss=1\n",
      "Evaluating: 100%|█| 943/943 [00:35<00:00, 26.33it/s, hits1=24.0000, hits10=25.0000, hits2=24.0000, hits5=25.0000, loss=1.0\n",
      "Evaluating: 100%|█| 844/844 [00:31<00:00, 26.95it/s, hits1=10.0000, hits10=17.0000, hits2=10.0000, hits5=11.0000, loss=1.0\n",
      "ic| train_loss: {'loss': 1.0669191800047113}\n",
      "    train_metric: {'hits@1': 0.1300732952368396,\n",
      "                   'hits@10': 0.14891416752843847,\n",
      "                   'hits@2': 0.13084785985324474,\n",
      "                   'hits@5': 0.1318696259429706,\n",
      "                   'mrr': 0.1652390096654403}\n",
      "    val_metric: {'hits@1': 0.12553461386495499,\n",
      "                 'hits@10': 0.14187282948503774,\n",
      "                 'hits@2': 0.12572870382565177,\n",
      "                 'hits@5': 0.12620699694308313,\n",
      "                 'mrr': 0.1604693376171193}\n",
      "    test_metric: {'hits@1': 0.12461360872024109,\n",
      "                  'hits@10': 0.15301482038131686,\n",
      "                  'hits@2': 0.12698424995545363,\n",
      "                  'hits@5': 0.12916121135118802,\n",
      "                  'mrr': 0.16177676515531728}\n",
      "Epoch 2: 100%|███████████████████████████████████████████████████████████| 3173/3173 [03:41<00:00, 14.33it/s, loss=1.0629]\n",
      "Evaluating:  60%|▌| 1892/3173 [01:09<00:47, 26.78it/s, hits1=13.0000, hits10=14.0000, hits2=13.0000, hits5=13.0000, loss=1"
     ]
    }
   ],
   "source": [
    "model = GINe(num_features=1, num_gnn_layers=3, edge_dim=train_dataset.tensor_frame.num_cols*channels, n_classes=1)\n",
    "model = torch.compile(model, dynamic=True) if compile else model\n",
    "model.to(device)\n",
    "learnable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "ic(learnable_params)\n",
    "wandb.log({\"learnable_params\": learnable_params})\n",
    "\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ]\n",
    "optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=lr, eps=eps)\n",
    "scheduler = get_inverse_sqrt_schedule(optimizer, num_warmup_steps=0, timescale=1000)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "train_metric = test(train_loader, model, \"train\")\n",
    "val_metric = test(val_loader, model, \"val\")\n",
    "test_metric = test(test_loader, model, \"test\")\n",
    "ic(\n",
    "        train_metric, \n",
    "        val_metric, \n",
    "        test_metric\n",
    ")\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train_loss = train(epoch, model, optimizer)\n",
    "    train_metric = test(train_loader, model, \"train\")\n",
    "    val_metric = test(val_loader, model, \"val\")\n",
    "    test_metric = test(test_loader, model, \"test\")\n",
    "    ic(\n",
    "        train_loss, \n",
    "        train_metric, \n",
    "        val_metric, \n",
    "        test_metric\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "feaecd9ce678485eb83184bc74abd877",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.004 MB of 0.004 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>learnable_params</td><td>▁█</td></tr><tr><td>test_hits@1</td><td>▁█▇▇▆▆</td></tr><tr><td>test_hits@10</td><td>▁█▅▆▃▃</td></tr><tr><td>test_hits@2</td><td>▁█▇▇▆▆</td></tr><tr><td>test_hits@5</td><td>▁█▆▇▆▅</td></tr><tr><td>test_loss</td><td>█▁▁▁▁▁</td></tr><tr><td>test_mrr</td><td>▁█▆▇▆▆</td></tr><tr><td>train_hits@1</td><td>▁█▆▇▆▆</td></tr><tr><td>train_hits@10</td><td>▁█▅▆▄▃</td></tr><tr><td>train_hits@2</td><td>▁█▆▇▆▆</td></tr><tr><td>train_hits@5</td><td>▁█▆▇▅▅</td></tr><tr><td>train_loss</td><td>█▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_mrr</td><td>▁█▆▇▆▆</td></tr><tr><td>val_hits@1</td><td>▁█▆▇▆▆</td></tr><tr><td>val_hits@10</td><td>▁█▄▅▃▃</td></tr><tr><td>val_hits@2</td><td>▁█▆▇▆▆</td></tr><tr><td>val_hits@5</td><td>▁█▆▇▅▅</td></tr><tr><td>val_loss</td><td>█▁▁▁▁▁</td></tr><tr><td>val_mrr</td><td>▁█▆▇▆▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>learnable_params</td><td>284211</td></tr><tr><td>test_hits@1</td><td>0.09626</td></tr><tr><td>test_hits@10</td><td>0.119</td></tr><tr><td>test_hits@2</td><td>0.09721</td></tr><tr><td>test_hits@5</td><td>0.09888</td></tr><tr><td>test_loss</td><td>1.05289</td></tr><tr><td>test_mrr</td><td>0.1333</td></tr><tr><td>train_hits@1</td><td>0.09975</td></tr><tr><td>train_hits@10</td><td>0.11772</td></tr><tr><td>train_hits@2</td><td>0.10013</td></tr><tr><td>train_hits@5</td><td>0.10107</td></tr><tr><td>train_loss</td><td>1.05312</td></tr><tr><td>train_mrr</td><td>0.13554</td></tr><tr><td>val_hits@1</td><td>0.09545</td></tr><tr><td>val_hits@10</td><td>0.11267</td></tr><tr><td>val_hits@2</td><td>0.09557</td></tr><tr><td>val_hits@5</td><td>0.0963</td></tr><tr><td>val_loss</td><td>1.06235</td></tr><tr><td>val_mrr</td><td>0.13133</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">debug-temporal-LOL-channels64-LOL</strong> at: <a href='https://wandb.ai/aakyildiz/rel-mm/runs/mwzqozvk' target=\"_blank\">https://wandb.ai/aakyildiz/rel-mm/runs/mwzqozvk</a><br/> View project at: <a href='https://wandb.ai/aakyildiz/rel-mm' target=\"_blank\">https://wandb.ai/aakyildiz/rel-mm</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240418_213203-mwzqozvk/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
